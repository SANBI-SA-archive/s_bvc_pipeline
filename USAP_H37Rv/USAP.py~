'''
Copyright 2015 Ruben van der Merwe
This program is distributed under the terms of the GNA General Public Licence (GPL).
    
This file is part of USAP.

USAP is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

USAP is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with USAP. If not, see <hgttp://www.gnu.org/licenses/>.
'''    

import os
#import sys
import subprocess
from subprocess import Popen, PIPE
import time
try:
    import readline
except:
    print "import error, no library called realine"
try:
    import glob
except:
    print "import error, no library called glob"


try:
    import re
except:
    print "import error, no library called re"

###############################################################################################################################################
def complete(text,state):
    return (glob.glob(text+"*")+[None])[state]

def interface():
    disclaimer = '''
    USAP V1.0
    Copyright 2015 Ruben van der Merwe
    This program is distributed under the terms of the GNA General Public Licence (GPL).
    
    This file is part of USAP.

    USAP is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    USAP is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with USAP. If not, see <http://www.gnu.org/licenses/>.'''
    print disclaimer
    print
    print "_____________________________________________________________________________________________"
    print "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_"
    print "USAP - Universal sequence analysis pipeline for whole genome sequence data                   "
    print "V1.0, Created by Ruben G. van der Merwe, 20 November 2015, email: rvdm@sun.ac.za"
    print "Copyright Ruben van der Merwe                                                                "
    print "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_"
    print "_____________________________________________________________________________________________"
    print ""
    
    ######################################################################
    #For Debugging:
    ######################################################################
    #debugMode = False
    debugMode = True #False #True 
    skipSetup = False 
    controlPoint = 1
    if debugMode:
        controlPoint = int(raw_input("controlPoints are 1 = index, 2 = fastqc, 3 = trimming, 4 = alignment, 5 = genomecoverage, 6 = compres gen-cov files, 7 = annotation, 8 = filter, 9 = pheno predict, 10 = lineage determination, 11 = summary table."))
        ans = 1 
    else:
        controlPoint = 1
        while True:
            ans = raw_input("Enter 1 to run the NGS pipeline, 2 to enter tool menu or Q to exit : ")
            if ans == "1" or ans == "2" or ans.lower() =="q":
                break
            else:
                print "Please select input: 1,2 or Q."
        if ans.upper() == "Q":
            exit()
        if ans == '2':
            return 0, 0, 0, 0, 0, 0, 0, 0
    #######################################################################
    
    
    mappers = [["BWA",True],["NOVOAlign",True],["SMALT",True]]
    variantTools = [["GATK",True],["SAMTOOLS",True]]    

    while True:
        ans_auto = raw_input("Enter 1 to run USAP in fully automated mode, enter '2' to customise parameters, enter 'Q' to exit: ") 
        if ans_auto == "1" or ans_auto == "2" or ans_auto.lower() =="q":
            break
        else:
            print "Please select input: 1,2 or Q."
    if ans_auto.upper() == "Q":
        exit()
        
    if ans_auto == "1":
        auto = True
        skipSetup = False
        spaceSavingMode = True
        
        return "1", skipSetup, controlPoint, mappers, variantTools, spaceSavingMode, debugMode, auto
    else:
        auto = False
        
    
    flag = True 
    
    while flag:
        print "Please toggle the mappers to use by entering the corresponding number (enter 'Y' to accept current settings)"
        print "Mapping algorithms:"
        pos = 0
        for x in mappers:
            pos += 1
            print str(pos)+"\t"+x[0],":",x[1]
        print "Enter the corresponding digit to toggle the use of specific mappers and variant callers, press 'Y' to accept current settings"
        ans = raw_input("Selection: ")
        if ans == "Y" or ans == "y":
            numMappers = 0
            for mapper in mappers:
                if mapper[1] == True:
                    numMappers +=1
            if numMappers > 0:
                flag = False
                print numMappers, "mappers selected."
                print
            else:
                print "error, must select at least one mapper"
            continue
        if ans not in ["0","1","2","3","4","5","6","7","8","9"]:
            continue
        if int(ans) > len(mappers):
            print "Error,invalid selection"
            continue
        mappers[int(ans)-1][1] = not(mappers[int(ans)-1][1])
    
    flag = True 
    while flag:
        print "Please toggle the variant callers to use by entering the corresponding number (enter 'Y' to accept current settings)"
        print "Variant detection tools:"
        pos = 0
        for x in variantTools:
            pos += 1
            print str(pos)+"\t"+x[0],":",x[1]   
        #for x in filterSettings:
        #    print x[0],":",x[1]
        print "Enter the corresponding digit to toggle the use of specific mappers and variant callers, press 'Y' to accept current settings"
        ans = raw_input("Selection: ")
        if ans == "Y" or ans == "y":
            numMappers = 0
            for mapper in mappers:
                if mapper[1] == True:
                    numMappers +=1
            if numMappers > 0:
                flag = False
                print numMappers, "mappers selected."
                print
            else:
                print "error, must select at least one mapper"
            continue
        if ans not in ["0","1","2","3","4","5","6","7","8","9"]:
            continue
        if int(ans) > len(variantTools):
            print "Error,invalid selection"
            continue
        variantTools[int(ans)-1][1] = not(variantTools[int(ans)-1][1])
    
    spaceSavingMode = False
    flagX = False
    while flagX == False:
        ans = raw_input("Would you like run this program in space saving mode (will remove certain redundant files) ? <Y/N>")
        if ans.upper() not in ["Y","N"]:
            continue
        if ans.upper() == "Y":
            spaceSavingMode = True
            break
        elif ans.upper() == "N":
            spaceSavingMode = False
            break
                
    return "1", skipSetup, controlPoint, mappers, variantTools, spaceSavingMode, debugMode, auto

###############################################################################################################################################
############################################################################################################################################################
def phyloAll(dirX, genCovDir, output, useCovData):
    '''
    snp based phylogeny:
    use whole genome SNPS, use WT not mutated, but use special char for deletion if there is no coverage
    If the read is present and WT- assign WT
    If the read is absent - assign deletion "-"
    The coverage MUST be above thereshold value
    This tool takes as input the VCF file and the zero-coverage file from USAP
    '''
    def isThisBPDeleted(dirX,vcf_name,snpPos):
        '''Assumtion: sorted lists'''
        if "_" in vcf_name:
            temp_vcf_name = vcf_name.split("_")[0]
        else:
            temp_vcf_name=vcf_name.split(".")[0]+"_genomecov=0_collapsed.txt"
        
        for covFile in os.listdir(dirX):
            if covFile.split("_")[0] == temp_vcf_name and "0_collapsed.txt" in covFile:
                break
        fileX = covFile
        os.chdir(dirX)
        #print dirX
        #raw_input()
        f = open(fileX,'r')
        #f.readline()
        iterations = 0
        for line in f:
            iterations += 1
            del_range = line.split()
            #print "the snp pos is", snpPos
            #print "the deletion range is now"
            #print del_range
            #raw_input()
            if len(del_range) == 1:
                if snpPos == int(del_range[0]):
                    #print "iterations:",iterations
                    return True
                if snpPos < int(del_range[0]):
                    #print "iterations:",iterations
                    return False
            elif len(del_range) == 2:
                if snpPos >= int(del_range[0]) and snpPos <= int(del_range[1]):
                    #print "iterations:",iterations
                    return True
                if snpPos < int(del_range[1]):
                    #print "iterations:",iterations
                    #print "returning false"
                    return False
        #print "returning false"
        return False
        f.close()
        
    def loadAllRefereceSNPS (main):
        '''
        this creates a dictionary of all the bp position snp info...
        '''
        genome = {} # {100:"A",200:"G"}
        totalLoaded = 0
        totalSNPS = 0
        #fileArray = []
        os.chdir(main)
        #print main        
        for fileX in os.listdir(main):
            if fileX.endswith(".vcf"):
                totalLoaded +=1
                print "reading file: ",fileX
                inFile = open(fileX, 'r')
                header = inFile.readline().split("\t")
                pos = -1
                novoPos = None
                bwaPos = None
                smaltPos = None
                positionList = []
                for element in header:
                    pos += 1
                    if element == "BWA_mut":
                        bwaPos = pos
                    elif element == "NOVO_mut":
                        novoPos = pos
                    elif element == "SMALT_mut":
                        smaltPos = pos
                for x in [novoPos,bwaPos,smaltPos]:
                    if x <> None:
                        positionList.append(x)
                #print positionList
                #raw_input()
                for x in inFile:           
                    line = x.split("\t")
                    pos = line[0] 
                    #take best from novo bwa smalt
                    #only use snps!
                    
                    if pos not in genome:
                        bestVal = ""
                        flag = False
                        for tempPos in positionList:
                            tempRef = line[tempPos-1]
                            tempMut = line[tempPos]
                            if tempRef.lower() in ["a","t","g","c","u"] and tempMut.lower() in ["a","t","g","c","u"]: #both are snps, not indels
                                flag = True
                                break
                            if flag:
                                break
                        if not flag:
                            continue
                            
                        #print pos, tempRef, tempMut
                        #raw_input("Does this match the vcf??")
                                                
                        genome[pos] = tempRef  #[tempRef,tempMut]
                inFile.close()
                totalSNPS += len(genome)
        print "A total of ",totalLoaded,"files were loaded which contains a total of",totalSNPS,"SNPS."
        return genome
    
    def convertDictToSortedList (dictX): 
        listX = []
        for x in dictX:
            listX.append([x,dictX[x]])
        #listX.sort() #sorts by first element
        listX = sorted(listX, key = lambda i : int(i[0]))
        #for x in listX:
        #    print x
        #    raw_input()
        return listX
    
    def loadVariantsFromAnnotatedVCF(fileX,dirX):
        #returns all the variants for one file only
        genome = {}
        totalLoaded = 0
        os.chdir(dirX)
        if fileX.endswith(".vcf"):
            totalLoaded +=1
    ##        print "getting variants from: ",fileX
            inFile = open(fileX, 'r')
            header = inFile.readline().split("\t")
            pos = -1
            novoPos = None
            bwaPos = None
            smaltPos = None
            positionList = []
            for element in header:
                pos += 1
                if element == "BWA_mut":
                    bwaPos = pos
                elif element == "NOVO_mut":
                    novoPos = pos
                elif element == "SMALT_mut":
                    smaltPos = pos
            for x in [novoPos,bwaPos,smaltPos]:
                if x <> None:
                    positionList.append(x)
            #print positionList
            #raw_input()
            for x in inFile:           
                line = x.split("\t")
                pos = line[0] 
                #take best from novo bwa smalt
                #only use snps!
                
                if pos not in genome:
                    bestVal = ""
                    flag = False
                    for tempPos in positionList:
                        tempRef = line[tempPos-1]
                        tempMut = line[tempPos]
                        if tempRef.lower() in ["a","t","g","c","u"] and tempMut.lower() in ["a","t","g","c","u"]: #both are snps, not indels
                            flag = True
                            break
                        if flag:
                            break
                    if not flag:
                        continue
                        
                    #print pos, tempRef, tempMut
                    #raw_input("Does this match the vcf??")
                                            
                    genome[pos] = tempMut  #[tempRef,tempMut]
            inFile.close()
        #totalSNPS = len(genome)
    ##    print "A total of ",totalLoaded,"files were loaded which contains a total of",totalSNPS,"SNPS."
        return genome
    
    
    def overlayRefAndCurrentFileSNPs_with_genome_cov_lookup(vcf_name,dirX,refGenomeDict,currGenomeDict,genCovDir, useCovData): #takes 1 SORTED list one and dict
        # output the overlayed
        '''
        open the corresponding 0-cov file - load it into memmory.
        then for each snp in the ref genome, if its not present in the curr genome
            then check if it has coverage in the 0-cov file
                if it has cov - write the WT pos
                if not - write a "-" corresponding to a deletion.
        '''
        resultList = []
        deletionList = [] #store the positions of deletions
        for snp in refGenomeDict: # this is not sorted - its a dictionary
            if snp[0] in currGenomeDict: #Then it has coverage and it is in the ref AND this sample
                if str(currGenomeDict[snp[0]]) in ["a","t","g","c","A","T","G","C"]:
                    resultList.append([snp[0],currGenomeDict[snp[0]]])
                else:
                    print "error 1 reading file", currGenomeDict[snp[0]]
                    raw_input()
                    
            else:
                if str(snp[1][0][0]) not in ["a","t","g","c","A","T","G","C"]:
                    print "error 2 reading file", snp[1][0][0]
                    print snp[1]
                    print snp[1][0]
                    print snp[1][0][0]
                    raw_input()
                if not useCovData: #then assume read is present, use the reference NT
                    resultList.append([snp[0],snp[1]])                
                elif isThisBPDeleted(genCovDir,vcf_name,int(snp[0])):
                    #print snp
                    #print "IT DOES NOT HAVE COVERAGE"
                    #raw_input()
                    resultList.append([snp[0],"-"])
                    deletionList.append([snp[0],vcf_name])
                else:
                    resultList.append([snp[0],snp[1]])
        resultList = sorted(resultList, key = lambda i : int(i[0]))
        
        #test if it is sorted
        previousPos = -1
    
    
        f= open("SNPs_with_no_mapping.txt",'a')
        for x in deletionList:
            for y in x:
                f.write(str(y)+"\t")
            f.write("\n")
        f.close()
                                
        for x in resultList:
    ##        print "this has to be sorted", x
    ##        raw_input()
            if int(x[0]) < int(previousPos):
                print "the result list is not sorted"
                print "the previous pos was",previousPos,"the new pos",x[0]
                raw_input()
            if len(x[1]) > 1:
                print "multiple base pair found, this should not happen2"
                print x[1]
                raw_input()
            previousPos = x[0]
    
        finalSeq = ''
        for x in resultList:
            finalSeq += x[1]    
        return finalSeq
    ##############################################################################################
    def overlayRefAndCurrentFileSNPs(refGenomeDict,currGenomeDict): #takes 1 SORTED list one and dict
        # output the overlayed
        resultList = []
        for snp in refGenomeDict: # this is not sorted - its a dictionary
            if snp[0] in currGenomeDict:
                if str(currGenomeDict[snp[0]]) in ["a","t","g","c","A","T","G","C"]:
                    resultList.append([snp[0],currGenomeDict[snp[0]]])
                else:
                    print "error 1 reading file", currGenomeDict[snp[0]]
                    raw_input()
                    
            else:
                if str(snp[1][0][0])  in ["a","t","g","c","A","T","G","C"]:
                    resultList.append([snp[0],snp[1]])
                else:
                    print "error 2 reading file", snp[1][0][0]
                    print snp[1]
                    print snp[1][0]
                    print snp[1][0][0]
                    raw_input()
                    
        resultList = sorted(resultList, key = lambda i : int(i[0]))
        
        #test if it is sorted
        previousPos = -1
        
        for x in resultList:
    ##        print "this has to be sorted", x
    ##        raw_input()
            if int(x[0]) < int(previousPos):
                print "the result list is not sorted"
                print "the previous pos was",previousPos,"the new pos",x[0]
                raw_input()
            if len(x[1]) > 1:
                print "multiple base pair found, this should not happen1"
                print x[1]
                raw_input()
            previousPos = x[0]
    
        finalSeq = ''
        for x in resultList:
            finalSeq += x[1]    
        return finalSeq   
        ######################################################################################################  
    #allSNPData = []
    totalLoaded = 0
    #totalSNPS = 0
    #fileArray = []
    #snpFileData = ""
    
    os.chdir(output)
    f= open("SNPs_with_no_mapping.txt",'w')
    f.close()

    #First make a list of all snps storing snp pos and reference bp
 
    allRefSNPs = loadAllRefereceSNPS(dirX) #all snps - for use as a reference SNP data  - dictionary of all snps which exist -
    print "Processing variants"
    refGenomeList = convertDictToSortedList(allRefSNPs) #convert dict to sorted LIST
    #print type(refGenomeList)
    #print len(refGenomeList)
    #print "total snps according to len of refgenomelist = ",len(refGenomeList)
    #print "ok"

    os.chdir(output) 
    snpFile = open('allSNPForPhylo.FASTA', 'w')  #This clears the file
    snpFile.close()                              #This clears the file
    fileCount = 0
    for fileX in os.listdir(dirX):
        if fileX.endswith(".vcf"):
            fileCount += 1
    
    for fileX in os.listdir(dirX):

        if not fileX.endswith(".vcf"):
            print "skipping file:",fileX
            continue
        print "Reading file: ",fileX
        totalLoaded += 1
        print "progress:",str(totalLoaded)," / ",str(fileCount),"--> ",str(int(round((float(totalLoaded)/fileCount),2)*100)),"% complete..."
        currGenome = loadVariantsFromAnnotatedVCF(fileX,dirX) # a dictionary           
        resultSeq = overlayRefAndCurrentFileSNPs_with_genome_cov_lookup(fileX,dirX,refGenomeList,currGenome, genCovDir, useCovData)    
##            print "testing overlayed list"
##            print len(overlayedList)
##            raw_input()
        os.chdir(output) 
        snpFile = open('allSNPForPhylo.FASTA', 'a+')
        snpFile.write(">"+str(fileX)+"\n")
        snpFile.write(resultSeq+"\n")
    print totalLoaded,"files processed"
    print "Whole genome SNP multi-fasta generated as allSNPForPhylo.FASTA in folder",output
    snpFile.close()
############################################################################################################################################################    


class paramaters(object):
    def __init__(self, binDir, globalDir, userPrefcpu, userPrefmem, inputDir, outputDir, readsType, userRef, trimMethod, BQSRPossible, emblFile):
##        if globalDir[-1] <> "/":
##            globalDir+= "/"
##        if outputDir[-1] <> "/":
##            outputDir+="/"
        self.emblFile = emblFile
        self.multiMode = True
        self.binDir = binDir
        self.globalDir = globalDir
        self.reference = os.path.join(self.globalDir+"/Reference/"+userRef+"/FASTA/")
        self.EMBL = os.path.join(self.globalDir+"/Reference/"+userRef+"/EMBL/")
        self.dbSNP = os.path.join(self.globalDir+"/Reference/"+userRef+"/dbSNP/")
        self.pheno = os.path.join(self.globalDir+"/Reference/"+userRef+"/PhenotypeDB/")
        self.lineage = os.path.join(self.globalDir+"/Reference/"+userRef+"/LineageMarkers/")
        self.exclusionList = os.path.join(self.globalDir+"/Reference/"+userRef+"/ExclusionList/")
        self.reads = "mixed"
        self.cores = userPrefcpu
        self.mem = str(userPrefmem)
        self.fastQ = inputDir
        self.outputDir = outputDir
        self.readsType = readsType
        self.trimMethod = trimMethod
        self.fastaList = []
        self.coreSplit = [1,1,1]
        self.BQSRPossible = BQSRPossible
        #############################################
        self.tools = self.globalDir+"/Tools/"
        if outputDir[-1] == "/":
             self.main = outputDir #output dir for all results
        else:
            self.main = outputDir+"/"   #output dir for all results

        self.mapperOut = self.main+"Results/" 
        self.BWAAligned = self.mapperOut+"BWA/"
        self.NOVOAligned = self.mapperOut+"NOVO/"
        self.SMALTAligned = self.mapperOut+"SMALT/"

        self.BWAAligned_aln = self.BWAAligned+"Alignment_Files/"
        self.NOVOAligned_aln = self.NOVOAligned+"Alignment_Files/"
        self.SMALTAligned_aln = self.SMALTAligned+"Alignment_Files/"
        ###################################
        if self.trimMethod == "No_Trim":
            self.trimmedFastQ = self.fastQ
        else:
            self.trimmedFastQ = os.path.join(outputDir+"FastQ_"+self.trimMethod+"/")
        self.fastQCStatsDir = os.path.join(outputDir+"fastQCStats/")

        #tool settings and paths
        self.insertmin = 0
        self.insertmax = 500
        self.spolpred = self.tools+"spolpred/spolpred.run"        
        self.smaltBinary = self.tools+"smalt-0.7.5/src/smalt"
        self.bwa = self.tools+"bwa-0.6.2/bwa"
        self.novoalign = self.tools+"novocraftV3-02-13/novoalign" 
        self.novoIndex = self.tools+"novocraftV3-02-13/novoindex" 
        self.SortSamDir = self.tools+"picard-tools-1.107/SortSam.jar"
        self.picardDirOnPc = self.tools+"picard-tools-1.107/ValidateSamFile.jar"
        self.picardAddReadGroup = self.tools+"picard-tools-1.107/AddOrReplaceReadGroups.jar"
        #self.gatkHomeDir = self.tools+"GenomeAnalysisTK-3.4-46/GenomeAnalysisTK.jar" 
        self.gatkHomeDir = self.tools+"GATK3.5/GenomeAnalysisTK.jar" 
        self.scripts_trimming = self.globalDir+"/Scripts/Trimming/"
        self.scripts_BWA = self.globalDir+"/Scripts/BWA/"
        self.scripts_NOVO = self.globalDir+"/Scripts/NOVO/"
        self.scripts_SMALT = self.globalDir+"/Scripts/SMALT/"
        self.scripts_StrainIdentification = self.globalDir+"/Scripts/StrainIdentification/"
        self.markDuplicates = self.tools+"picard-tools-1.107/MarkDuplicates.jar"
        self.fastQCPath = self.tools+"FastQC/fastqc"
        self.java7 = self.tools+"jre1.7.0_51/bin/java"
        self.picardCreateSequenceDictionary = self.tools+"picard-tools-1.107/CreateSequenceDictionary.jar"
        self.bedtools = self.tools+"bedtools2-2.25/bin/"
        self.fastXTrimmer = self.tools+"fastx_toolkit/fastx_trimmer"
        self.fastXClipper = self.tools+"fastx_toolkit/fastx_clipper"
        self.trimOMatic = self.tools+"Trimmomatic-0.32/trimmomatic-0.32.jar"
        self.trimOMaticParams = "2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:20 MINLEN:36" 
        self.markDuplicatesDir = self.tools+"picard-tools-1.107/MarkDuplicates.jar"
        self.qualimap = self.tools+"qualimap_v2.0/qualimap"
        self.SAIDIR = self.BWAAligned #to save space, allows deleting initial fastq files)
        self.msort = os.path.join(self.tools+"msort/msort")
        self.shuffle =  self.binDir+"/shuffleSequences_fastq.pl"
        self.bcftools = self.tools+"samtools-0.1.19/bcftools/bcftools"
        self.samtools = self.tools+"samtools-0.1.8/samtools"
        #self.samtools = self.tools+"samtools-1.3/samtools"


#########################################################################################
#########################################################################################
#########################################################################################
#########################################################################################
#########################################################################################
#########################################################################################
#########################################################################################
                    #Automated annotation of variants
#########################################################################################
#########################################################################################
#########################################################################################
#########################################################################################
#########################################################################################
#########################################################################################
#########################################################################################
'''
05 October 2015
Being update to make use of an embl file format
this will hopefully allow the support of many more organisms than just m. tb
'''
#Step 1, iterate over all mapper folders, obtain a list of BWA,N,S
#For some the user might not want to have run, so we allow missing data, also
#the mapper could be incompatable with the fastq file for some reason
'''
BWAVCFS = [F1,F3]
NOVOVCFS = [F2,F3]
SMALTVCFS = [F1]
AllFiles = unique entries of the sum of all mappers
for each vcf in AllFiles
try to open matching file in all mappers
f1,f2,f3 = open filename
load vcf contents into mem for all 3
'''
#BEGINNING OF PHENOTYPE MATHCING
###############################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
'''
This includes VCF compression of same AA codon postions SNP to correctly match to DB.
'''
'''
For indels only using position and ref/mut data since the AA codon and AABP info is not correctly calculated
'''

'''
this program primarily makes use of gene coordinate data
if absence of gene coordinate data it will make use of genomic posisiton
failing this it will make use of AA codon postion
'''
'''
input database format:
Col1: position
Col2: ref    for example A or AA or AAA....
Col3: mut    for exmplae - or C or CC or CCC....
Col4...N: Information cols to be added to output (phenotype, reference etc)

ref:mut A -> - means delete bp A
ref:mut A -> C means snp change A to C
ref:mut ATC --> AC means delete T
ref:mut ATC --> ATGGC means insert GG

  
Makes use of SNPs, indel and large Deletions for report
Thus filtered SNP, filtered indels, Novoalign Gen cov

overview:
for each filename in snp vcf folder:
    fileData = []
    store all dr positions and dr gene ranges
    for each snp file:
        match to database
    for each indel file:
        match 
    for each large del file 
        match
    store fileData results (snp, indel, largedel)
store master summary

'''

import os


def translate(seq):
    seq = seq.upper()
    gencode = {
    'ATA':'I', 'ATC':'I', 'ATT':'I', 'ATG':'M',
    'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',
    'AAC':'N', 'AAT':'N', 'AAA':'K', 'AAG':'K',
    'AGC':'S', 'AGT':'S', 'AGA':'R', 'AGG':'R',
    'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',
    'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',
    'CAC':'H', 'CAT':'H', 'CAA':'Q', 'CAG':'Q',
    'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',
    'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',
    'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',
    'GAC':'D', 'GAT':'D', 'GAA':'E', 'GAG':'E',
    'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',
    'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',
    'TTC':'F', 'TTT':'F', 'TTA':'L', 'TTG':'L',
    'TAC':'Y', 'TAT':'Y', 'TAA':'*', 'TAG':'*',
    'TGC':'C', 'TGT':'C', 'TGA':'*', 'TGG':'W',
    }
 
    #print seq
    protSeq = ''
    for x in range(0,len(seq),3):
        if gencode.has_key(seq[x:x+3]) == True:
            protSeq += gencode[seq[x:x+3]]
        else:
       #     print "Warning, sequence is not a multiple of 3...truncating sequence..."
       #     raw_input()
            return protSeq
 
    return protSeq
    
def loadAllKnownVariants(variantListName):
    #Store all known phenotypes

##    os.chdir(variantListDir)
    f= open(variantListName,'r')
    f.readline()
    allVariantsByPos = {}
    allKnownGenes = {}
    PHENO_ORDER = []
    for line in f:
        temp = line.split("\t")
        tempPheno = temp[9]
        if tempPheno not in PHENO_ORDER:
            PHENO_ORDER.append(tempPheno)
        pos = temp[1] #may have more than one type of muation at same pos
        if "/" in pos:
            pos = pos.split("/")[0]
        gene = temp[3]
        if pos == "" or pos == "-": #then add to gene dictionary instead
            if gene == "" or gene == "-":
                print temp
                raw_input("This data point is problematic, no pos or gene data!")
            if gene not in allKnownGenes:
                allKnownGenes[gene] =  [temp]
            else:
                allKnownGenes[gene].append(temp)
        elif pos not in allVariantsByPos:
            allVariantsByPos[pos] = [temp] #create a list of lists
        else:
            allVariantsByPos[pos].append(temp) #add to list
    f.close()
    PHENO_ORDER.sort()
    return allVariantsByPos, allKnownGenes, PHENO_ORDER

def extractVariantsFromVCF(fileX,FILTERED_VARIANTS):
    '''
    Here snps that fall into the same codon are grouped together. This allows
    codon vs database-codon comparison.
    so snp at same codon goes from snp1: G:a, snp2:G:t: snp3: G:C --> snp1: GGG:ATC -->at snp pos 1
    '''
    def storeSingleLineData(temp):
        lastPos = temp[0]
        #Establish the reference base complete
        #################################
        lastRef = temp[8] #Prefer NOVOAlign data
        if lastRef == "":
            lastRef = temp[2] #BWA ref base
        if lastRef == "":
            lastRef = temp[14] #Smalt ref base
        #Establish the mutation
        #################################   
        lastMut = temp[9] #prefer NOVO data
        if lastMut == "":
            lastMut = temp[3] #2nd pref is BWA
        if lastMut == "":
            lastMut = temp[15] #3rd pref is SMALT
        #################################
        if temp[23] == "intergenic" or len(lastRef) > 1 or len(lastMut) > 1: #This is an intergenic region OR an indel and the calculated data is not accurate ans should not be used for pheno predict
            lastAALetterChange = "" 
            lastCodon = "" 
            lastRefCodon = "" 
            lastMutCodon = "" 
            lastLocus = ""
        else:
            #print temp
            #print len(temp)
            #print len(lastRef), len(lastMut)
            #raw_input()
            lastAALetterChange = temp[43]#convertAALetterChange(temp[17])
            lastCodon = temp[44]
            lastRefCodon = temp[45]
            lastMutCodon = temp[46]
            if lastAALetterChange == "":
                lastAALetterChange = temp[38]#convertAALetterChange(temp[17])
                lastCodon = temp[39]
                lastRefCodon = temp[40]
                lastMutCodon = temp[41]
            if lastAALetterChange == "":
                lastAALetterChange = temp[49]#convertAALetterChange(temp[17])
                lastCodon = temp[50]
                lastRefCodon = temp[51]
                lastMutCodon = temp[52]
            lastLocus = temp[31]
        return lastPos, lastRef, lastMut, lastRefCodon, lastAALetterChange, lastCodon, lastRefCodon, lastMutCodon, lastLocus
        
    def convertAALetterChange(s):
        #To convert A:I,I,I to A/I
        s = s.replace('"','')
        if ":" not in s:
            return ""
        ref = s[0]
        data = s[2:]
        data = data.split(",")
        mut = data[2]
        if mut == "-":
            mut = data[1]
        if mut == "-":
            mut = data[2]  
        return ref+"/"+mut #may sometimes return A/Syn
    
    def merge(lastRefCodon,currentRefCodon,lastMutCodon,currentMutCodon):
        #print [lastRefCodon,currentRefCodon,lastMutCodon,currentMutCodon]
        if lastRefCodon <> currentRefCodon:
            print "Error matching during codon merge step:"
            print lastRefCodon, currentRefCodon
            raw_input()
        result = ""
        for pos in range(3):
            print pos
            print lastMutCodon
            print lastRefCodon
            if lastMutCodon[pos] <> lastRefCodon[pos]:
                result += lastMutCodon[pos]
            elif currentMutCodon[pos] <> lastRefCodon[pos]:
                result += currentMutCodon[pos]
            else: 
                result += lastRefCodon[pos]
        return result
    ##############################################################
    variantData = []
    os.chdir(FILTERED_VARIANTS)
    f = open(fileX,'r')
    f.readline()
    currentCodon = None
    lastCodon = None
    firstRun = True
    print "Loading Variant data from file: ",f.name
    line = ""
    for line in f:
        if firstRun:
            ##### LOAD THE FIRST LINE OF DATA #######
            temp = line.split("\t") 
            lastPos, lastRef, lastMut, lastRefCodon, lastAALetterChange, lastCodon, lastRefCodon, lastMutCodon, lastLocus = storeSingleLineData(temp)
            firstRun = False
            continue 
            #########################################           
        #-----------------load a new line to be evaluated#
        temp = line.split("\t")
        currentPos, currentRef, currentMut,currentCodon,currentAALetterChange,currentCodon, currentRefCodon, currentMutCodon,currentLocus = storeSingleLineData(temp)
        #------------------------compare to last line data---------------------#
        if currentCodon == lastCodon and lastCodon <> "":
            #These belong to same AA codon, will now merge the lines
            lastRef += currentRef #Add together
            lastMut += currentMut #add together  
            lastCodon = currentCodon 
            lastRefCodon = currentRefCodon  
            try:      
                lastMutCodon = merge(lastRefCodon,currentRefCodon,lastMutCodon,currentMutCodon) #Debug point 98765
            except:
                print "Error calculating the lastMutCodon from:",lastRefCodon,currentRefCodon,lastMutCodon,currentMutCodon
                raw_input()
            #Recalculate the AA change            
            tempRefAALetter = translate(lastRefCodon) # the reference AA letter
            newAALetterChange = translate(lastMutCodon) #the new mutated aa letter
            if tempRefAALetter <> newAALetterChange:
                lastAALetterChange = tempRefAALetter+"/"+newAALetterChange
            else:
                lastAALetterChange = currentAALetterChange #there is no change its a synonomous change, will keep original style data syn,syn,syn
            #will have to recalculate the syn and nonsyn change also...
            continue                
        else: 
            #store [Postion,REF,MUT,AACodon,REfcodon,mutcodon,GeneName]
            variantData.append([lastPos,lastRef,lastMut,lastCodon,lastRefCodon,lastMutCodon,lastLocus,lastAALetterChange]) 
            #save previous line data
            lastPos = currentPos
            lastRef = currentRef #Add together
            lastMut = currentMut #add together  
            lastCodon = currentCodon 
            lastRefCodon = currentRefCodon        
            lastMutCodon = currentMutCodon
            lastLocus = currentLocus
            lastAALetterChange = currentAALetterChange
            #will now either load a new line to compare to current or if end of file will add this data to the list as singleton.

    try:
        variantData.append([lastPos,lastRef,lastMut,lastCodon,lastRefCodon,lastMutCodon,lastLocus,lastAALetterChange]) 
    except:
        print "No variants found in filtered file:", f.name, "in folder", FILTERED_VARIANTS
        return [] 
    f.close()
    return variantData

def findPhenoMarkersSNP_indel(fileX,FILTERED_VARIANTS,variantPositionDict, allGenesDict,MTB):
    '''
    OVERVIEW AND ORDER OF OPERATION:
        
    FIRST CHECK IF THE POSITION MATHCES
        SECOND CHECK IF THE CODONPOS MATCH
        THRID CHECK IF THE CODON BP INFO MATCH   
        IF NO CODON BP INFO 
            CHECK IF THE REF/MUT DATA MATCHES <--- important
    CHECK IF GENE MATCHES
        CHECK IF CODON POS MATCH
        CHECK IF CODON BP INFO MATCH
        IF NO CODON BP INFO
            CHECK IF REF/MUT DATA MATCHES <--- important
            
    at this point...need to check the vcf data and what data to use to match first
    it goes:
        position, codonnum, codonbp, if no match check if syn/nonsyn - if nonsyn report new mutation at known codon --> possible DR
        if no pos match (then could be due to gaps in the db now check on gene level)
        gene, codonnum if not match codon num check of syn or nonsyn if nonsyn report new mut at knon gene --> possible DR
        if match codonnum check if match codonbp - not if not check if s/nonsyn if s report new mut at knwon codon --> possible DR 
        '''
        
    #find all phenotype associated markers in SNP and INDEL files for this sample
    PHENO_data = []
    tempFileName = fileX #fileX.split("_")[0]+"_gatk_snps_FILTERED.vcf"
    #useAllData = True
    totalNGSData = extractVariantsFromVCF(tempFileName,FILTERED_VARIANTS) 
    #useAllData = False #Dont use AA codon position or AA ref/mut bp changes - not correctly calcualted in indels
    #totalNGSData = snpDataList + indelDataList
    if MTB:
        #Special check for M.tb PZA resistance, will be skipped if not M.tb used as reference.
        pzaDict = {}
        pzaDict[35] = ["L/R"]
        pzaDict[37] = ["E/V"]
        pzaDict[65] = ["S/S"]
        pzaDict[96] = ["K/K"]
        pzaDict[110] = ["D/G"]
        pzaDict[114] = ["T/M"]
        pzaDict[130] = ["V/A"]
        pzaDict[163] = ["V/A"]
        pzaDict[170] = ["A/V"]
        pzaDict[180] = ["V/I"]
    else:
        pzaDict = {}
  
    for variantData in totalNGSData: #Both snps and indel data
        #print fileX
        #print variantData
        #raw_input("debug point 654321")
        temp_PHENO_data = []
        if MTB:
            if (("Rv2043c" in variantData) and (variantData[-1] <> "")) or (("Rv2043c" in variantData) and (len(variantData[1]) <> len(variantData[2]))):
                #print "found possible pncA mutation", variantData
                if variantData[3] in pzaDict:
                    if variantData[7] == pzaDict[variantData[3]]: #skip known non dr causing markers
                        #raw_input("Found a case of a non DR causing mutation in pncA skipping this one")
                        continue
                    else: #score all other pnca mutations as DR
                        #[[2, ['2288764', 'pncA', 'Rv2043c', '478', 'T/G', '', 'PYRAZINAMIDE', '160', 'ACA/CCA', '', 'T/P', '', 'TBDreaMDB', '', '', '', '', '', '', '', '', '', '', '', '3\n']]]
    
                        temp_PHENO_data.append([1, [variantData[0], 'pncA', 'Rv2043c', '', '', '', 'PYRAZINAMIDE', variantData[3], variantData[4]+"/"+variantData[5], '', variantData[7], '', 'ASSUME_DR', '', '', '', '', '', '', '', '', '', '', '', '3\n']])
                        print fileX
                        #raw_input("pnca resistance foound")
                        #return PHENO_data
                else: #score all other pnca mutations as DR
                    temp_PHENO_data.append([1, [variantData[0], 'pncA', 'Rv2043c', '', '', '', 'PYRAZINAMIDE', variantData[3], variantData[4]+"/"+variantData[5], '', variantData[7], '', 'ASSUME_DR', '', '', '', '', '', '', '', '', '', '', '', '3\n']])
                    #return PHENO_data
                    print fileX
                    #raw_input("pnca resistance foound")

        
        #print "^"*15
        #print snpData
        bestMatch = [-1,-1] #The best matching dabase entry for this vcf mutation --> #LEVEL, DATA
        #Here level is the quality of the match 
            #level 0 is exact match on Pos, CodonNum, CodonBP+MutBP
            #level 1 is excact match on Pos + AA Letter Change OR ref+mut BP change 
            #level 2 is codon position match of a new non-synonoumous mutation but without exact match...also must be synonoumous
            #level 3 is non-syn change in gene that is at a new codon
        
        #EXTRACT DATA FOR THIS POSITION FROM DB
        vcfPos = variantData[0]
        vcfRef = variantData[1]
        vcfMut = variantData[2]
        vcfAACodonNum = variantData[3]
        vcfRefCodon = variantData[4]
        vcfMutCodon = variantData[5]
        vcfLocus = variantData[6]
        vcfLastAALetterChange = variantData[7]
        
        #check if position matches
        if vcfPos in variantPositionDict: #FIRST CHECK IF THE POSITION MATHCES
            #if debugMode:
            #    print "known position found"
            #can either be an excat match or a new/similar mutation at same pos or a synonomous mutation
            #iterate over position dictionary for best match
            knownVariantCount = 0
            for knownVariant in variantPositionDict[vcfPos]: #can be >1 known mutation at this pos
                knownVariantCount += 1
                #if debugMode:
                #    print vcfPos
                #    print "comparing to", knownVariant
                #
                #    print "================"
                #    print variantData
                #    print "VS"
                #    print knownVariant
                #    print "================"
                #    print "KNOWNVARIANT NUM FOR THIS POS", knownVariantCount,"/",len(variantPositionDict[vcfPos]), "previous bestmatch level", bestMatch[0]
                #    raw_input()
                #each element contains: [chromosomePos, locus,locus_tag,gene_coordinates,refBP,mutBP,Drug(phenotype),extrainfo1...extrainfoN  
                #EXTRACT DB DATA FOR THIS KNOWN VARIANT POSITION
                #print knownVariant
                #raw_input()
                DBLocus = knownVariant[2]
                DBlocus_tag = knownVariant[3]
                DBGene_coordinates  = knownVariant[4] 
                DBReference = knownVariant[6]  #can be "-","A","AA","A.....A"
                ###########
                #uncompact DB mutation A/C or ACACACA/G
                slashCount = 0
                for char in DBReference:
                    if char == "/":
                        slashCount += 1
                if slashCount > 1:
                    DBReference = "FORMAT_ERROR"
                    DBMutation = "FORMAT_ERROR"
                elif slashCount ==1:
                    DBReference = knownVariant[6].split("/")[0]   #can be "-","A","AA","A.....A"
                    DBMutation = knownVariant[6].split("/")[1]
               
                DBPPhenotype = knownVariant[9]
                DBAACodonNum = knownVariant[5]
                if "/" in knownVariant[7]:
                    DBAACodonRef = knownVariant[7].split("/")[0]
                    DBAACodonMut = knownVariant[7].split("/")[1]
                else:
                    DBAACodonRef = ""
                    DBAACodonMut = ""
                DBAALetterChange = knownVariant[8]
                #print "BUT:", knownVariant
                #print "database variables are initialized as follows"
                #print [DBAACodonNum],[DBAACodonRef],[DBAACodonMut],[DBAALetterChange]
                
                #QUALITY CONTROL - all variants with nucleoptide position info MUST have AA codonNum info
                #if DBAACodonNum == "" or DBAACodonNum == "-":
                #    print knownVariant
                #    raw_input("Have position match but no AA num info - fix the database now")
                #if DBReference <> vcfRef:
                #    print "Fix this, should not happen!"
                #    print "error, mismatch between ref sequences:"
                #    print vcfRef, DBReference
                #    print "from:", snpData
                #    print "DB data was ",knownVariant
                #    raw_input()
                #END QUAL CONTROL
                
                #Assume that have aa codon num info...
                #SECOND CHECK IF THE CODONPOS MATCH
                if vcfAACodonNum == DBAACodonNum and DBAACodonNum <> "": #it is a knwon codon position 
                    #if debugmode:
                    #    print "known position, known codon position"
                    #    #THRID CHECK IF THE CODON BP INFO MATCH 
                    #    print [vcfMutCodon], [DBAACodonMut], vcfMutCodon == DBAACodonMut
                    #    raw_input("?debugging?")
                    if vcfMutCodon == DBAACodonMut:
                        print "found exact match:",knownVariant # on Pos+ CodonNum + CodonBP"
                        bestMatch = [0,knownVariant]
                        temp_PHENO_data.append(bestMatch)
                        #print bestMatch
                        #print 
                        #raw_input()
                        
                    elif vcfLastAALetterChange == DBAALetterChange: #There is now AA bp info but the aa change A:Y is known
                        print "found exact match:",knownVariant # on Pos + CodonNum + AALETTER CHANGE" , variantData, knownVariant
                        bestMatch = [1,knownVariant]
                        temp_PHENO_data.append(bestMatch)
                        #raw_input()
            
                    elif "/" in vcfLastAALetterChange and "syn" not in vcfLastAALetterChange:
                        print "new codon change found at known codon", knownVariant
                        bestMatch = [2,knownVariant]
                        temp_PHENO_data.append(bestMatch)
                        #raw_input()
                    #else:
                    #    print "Error 676, ignoring syn change or no info on AA change in VCf", [vcfLastAALetterChange]
                    #    print variantData
                    #    print "the best match thusfar is", bestMatch
                    #    raw_input()
                                
                                
                #IF NO CODON BP INFO 
                #CHECK IF THE REF/MUT DATA MATCHES <--- important
                elif DBAACodonNum == "" or DBAACodonNum == "-": 
                    #print "no DBAACodon info, matching to pos and ref/mut"
                    #print [vcfRef],[DBReference], [vcfMut],[DBMutation], vcfRef == DBReference , vcfMut == DBMutation
                    #raw_input() 
                    #Check if match using only chr pos and ref and mut data, nothing else - usally for indels or promoter mutations
                    if vcfRef == DBReference and vcfMut == DBMutation: #an exact nucleotide match to the lookupdatabase
                        slashCount = 0
                        for char in DBMutation:
                            if char == "/":
                                slashCount += 1
                        if slashCount > 1:
                            raw_input("Instance where using multiple slahses but prog did not use codonBP info... will have to fix this entry in the DB")
                        print "Exact mutation found:", knownVariant #,"and", variantData
                        bestMatch = [1,knownVariant]
                        temp_PHENO_data.append(bestMatch)  
                        #raw_input()  
    
                else:
                    print "Close match found..."
                    #print "matching error begin here:"
                    #print bestMatch 
                    #print variantData
                    #print knownVariant
                    #print "should not happen - getting to this point means that the chr postion matches but not the aa codon number...this means a database formatting error"
                    #raw_input()      
        #Getting to here means that there is no nucleotide info for this postion or did not fully match a known postion      
        #lets see if we can find a better match than the by-position method above        
        elif vcfLocus in allGenesDict and bestMatch[0] <> 1: #The genes match, now check if AA codon and AA change match
            print "known gene found", bestMatch
            #Iterate over the gene dictionary data:
            knownVariantCount = 0
            for knownVariant in allGenesDict[vcfLocus]:
                print "comparing to (gene)", knownVariant
                knownVariantCount += 1
                print "KNOWNVARIANT NUM FOR THIS GENE", knownVariantCount
                #THERE IS NO POSITION DATA FOR THIS 
                DBLocus = knownVariant[2]
                DBlocus_tag = knownVariant[3]
                DBGene_coordinates  = knownVariant[4]
                DBReference = knownVariant[6]  #can be "-","A","AA","A.....A"
                ###########
                #uncompact DB mutation A/C or ACACACA/G
                slashCount = 0
                for char in DBReference:
                    if char == "/":
                        slashCount += 1
                if slashCount > 1:
                    DBReference = "FORMAT_ERROR"
                    DBMutation = "FORMAT_ERROR"
                elif slashCount ==1:
                    DBReference = knownVariant[6].split("/")[0]   #can be "-","A","AA","A.....A"
                    DBMutation = knownVariant[6].split("/")[1]
                #DBMutation = knownVariant[5]   #can be "-","A","AA","A.....A"
                DBPPhenotype = knownVariant[9]
                DBAACodonNum = knownVariant[5]
                if "/" in knownVariant[7]:
                    DBAACodonRef = knownVariant[7].split("/")[0]
                    DBAACodonMut = knownVariant[7].split("/")[1]
                else:
                    DBAACodonRef = ""
                    DBAACodonMut = ""
                DBAALetterChange = knownVariant[8]
                
                print "found a matching PhenoType associated gene", vcfLocus,"in", fileX
                #print variantData
                print "comparing to",knownVariant
                ################################################################
                if vcfAACodonNum == DBAACodonNum: #it is a knwon codon position 
                    print "known position, known codon position"
                    #THRID CHECK IF THE CODON BP INFO MATCH 
                    if vcfMutCodon == DBAACodonMut:
                        print "found exact match:",knownVariant # on Pos+ CodonBP"
                        bestMatch = [0,knownVariant]
                        temp_PHENO_data.append(bestMatch)
                        print
                        #raw_input()
                    elif vcfLastAALetterChange == DBAALetterChange: #There is now AA bp info but the aa change A:Y is known
                        print "found exact match:", knownVariant # on Pos + CodonNum + AALETTER CHANGE" 
                        bestMatch = [1,knownVariant]
                        temp_PHENO_data.append(bestMatch)
                        #raw_input()
                        print
                    else:
                        if "/" in vcfLastAALetterChange and "syn" not in vcfLastAALetterChange:
                            bestMatch = [2,knownVariant]
                            temp_PHENO_data.append(bestMatch)
                    #IF NO CODON BP INFO 
                        #CHECK IF THE REF/MUT DATA MATCHES <--- important
                elif DBAACodonNum == "" or DBAACodonNum == "-": 
                    #Check if match using only chr pos and ref and mut data, nothing else - usally for indels or promoter mutations
                    if vcfRef == DBReference and vcfMut == DBMutation: #an exact nucleotide match to the lookupdatabase
                        slashCount = 0
                        for char in DBMutation:
                            if char == "/":
                                slashCount += 1
                        if slashCount > 1:
                            raw_input("Instance where using multiple slahses but prog did not use codonBP info... will have to fix this entry in the DB")
                            
                        print "Exact mutation found:", knownVariant,"and", variantData
                        bestMatch = [1,knownVariant]
                        temp_PHENO_data.append(bestMatch)
                        #raw_input()
                        print            
                
                    elif "/" in vcfLastAALetterChange and "syn" not in vcfLastAALetterChange and bestMatch[0] <> 2:
                        bestMatch = [3,knownVariant]
                        temp_PHENO_data.append(bestMatch)
                        #raw_input("new mutation at new codon in gene")
                    else:
                        print "Error 767, ignoring syn change or no info on AA change in VCf", [vcfLastAALetterChange]
                        print variantData
                        raw_input()
                ###########################################################################
        
        #From all the data points detected for this variant, select only the relevant ones
        #These could be same pos with more than one phenotype
        #also to remove multiple hits to the same AA change - to just store the optimal match
        if temp_PHENO_data <> []:
            PHENO_data.append(temp_PHENO_data) 
        
        #bestHits = []
        #for hit in temp_DR_data: 
        #    pos+coconBP
        #    pos+AAchange
        #    print hit
        #    print "bestMatch is"
        #    print bestMatch
        #    raw_input()
    #print "HERE IT IS " *10
    #print PHENO_data
    return PHENO_data # a list of DR matches from snp and indel data.

def findPhenoMarkersLARGE_DEL(inputFile,FILTERED_DELETIONS,variantPositionDict, allGenesDict, MTB):
    '''
    scan the list of known positions and genes for matches
    '''
    DR_status_list = []
    os.chdir(FILTERED_DELETIONS)
    for fileX in os.listdir(FILTERED_DELETIONS):
        if fileX.split("_")[0] == inputFile.split("_")[0]:
            #del_file=fileX
            break  
    #print FILTERED_DELETIONS
    #raw_input()          
    f = open(fileX,"r")
    #print "Rv2043c" in allGenesDict
    for line in f:
        #print [line]
        #raw_input()
        deleted_rvNum = line.split("\t")[14]
        deleted_rvNum = deleted_rvNum.strip()
        deleted_rvNum = deleted_rvNum.replace("'","")
        deleted_rvNum = deleted_rvNum.replace('"',"")

        if deleted_rvNum == "":
            continue
        #print [deleted_rvNum]
        #print deleted_rvNum in allGenesDict
        if deleted_rvNum not in allGenesDict:
            continue
            
        elif MTB and deleted_rvNum == "Rv2043c":
            DR_status_list.append([0,["PYRAZINAMIDE","pncA","deletion"]])
        else:
            DR_status_list.append([3,deleted_rvNum])
    f.close()
    return DR_status_list
    
def processData(fileX,outputFolder,snp_and_indel_Data,largeDelData,DR_Order,DR_CLASS,MTB):
    #print largeDelData
    #raw_input()
    all_DR_data_per_DR = {}
    #shortNames = ["RIF",
    #"CAP",
    #"PZA",
    #"AMI",
    #"CYC",
    #"EMB",
    #"ETH",
    #"INH",
    #"KAN",
    #"FLQ",
    #"FLQ" ,
    #"PAS",
    #"RBU",
    #"STR",
    #"BEDAQUILINE",
    #"CLOFAZIMINE",
    #"LINEZOLID"]
    for drug in DR_Order:
        all_DR_data_per_DR[drug] = []
    #print out DR in DR order as in DR_order list
    #
    #print largeDelData
    #raw_input()
    #largeDelData is either [-1.-1] or [0, pnca-deletion] or [3,rvnum]
    #one line per file
    #format is:
    #RIF	Capr	PZA	AMI	Cyclos	EMB	ETH	INH	Kana	Moxi	Oflox	PAS	RBU	SM
    '''
    --------------------------------------
    These are the keywords used in kvarq
    Ethambutol
    Fluoroquinolones
    Isoniazid
    Rifampicin
    Streptomycin
    ----------------------------------------------------------
    
    These are the resistances from the DB
            
    PYRAZINAMIDE
    ISONIAZID
    RIFAMPICIN
    PARA-AMINOSALISYLIC_ACID
    CAPREOMYCIN
    ETHIONAMIDE
    BEDAQUILINE
    CLOFAZIMINE
    ETHAMBUTOL
    FLUOROQUINOLONES
    STREPTOMYCIN
    LINEZOLID
    AMIKACIN
    KANAMYCIN
    --------------------------------------------------------
    '''
    os.chdir(outputFolder)
    if MTB:
        f = open("DR_pred_results_new_format.txt",'a')
        f.write(fileX+"\t")
    
    #print "*"*100
    for all_var_hits in snp_and_indel_Data:
        #print "-" * 20
        #Fist the best hist - might be more than one since could be more than one phenotype
        phenoBestHits = {}
        for hit in all_var_hits:
            #print hit
            #raw_input()
            pheno = hit[1][9]
            if pheno not in phenoBestHits:
                phenoBestHits[pheno] = hit #Store the data for this phenotype, its the only one found
            elif phenoBestHits[pheno][0] > hit[0]:
                phenoBestHits[pheno] = hit #A better raking match was found for thite phenotype, override the previous hit
        #raw_input("will now write the best data to file")
        for pheno in phenoBestHits:
            #f.write(str(phenoBestHits[pheno][1])+"\t")
            #for x in range(len((phenoBestHits[pheno][1]))):
            #    print x, [phenoBestHits[pheno][1][x]]
            #raw_input()
            toWrite = [phenoBestHits[pheno][1][9],phenoBestHits[pheno][1][2],phenoBestHits[pheno][1][3],"AA_POS:"+phenoBestHits[pheno][1][5],phenoBestHits[pheno][1][7]]
            if phenoBestHits[pheno][1][5] == "" or phenoBestHits[pheno][1][5] == "-" : #if there is no codon info
                toWrite = [phenoBestHits[pheno][1][9],phenoBestHits[pheno][1][2],phenoBestHits[pheno][1][3],"Gene_POS:"+phenoBestHits[pheno][1][4],phenoBestHits[pheno][1][7]]   
            if MTB:
                f.write(str(toWrite)+"\t")
            all_DR_data_per_DR[phenoBestHits[pheno][1][9]].append(toWrite)
#
            #f.write(str(phenoBestHits[pheno][1][:-1]+[phenoBestHits[pheno][1][-1].replace("\n","")])+"\t")
    for deletion in largeDelData:
        if deletion[0] == 0:
            toWrite = str(deletion[1])
            if MTB:
                f.write(toWrite+"\t") 
            all_DR_data_per_DR[deletion[1][0]].append(toWrite)       
    if MTB:
        f.write("\n")     
        f.close()
    
    f = open("PHENO_RESULTS.txt",'a')
    f.write(fileX)
    drug = False
    for drug in DR_Order:
        f.write("\t")
        for x in all_DR_data_per_DR[drug]:
            #print x[1:]
            f.write(str(x[1:])+",")
        #f.write(str(all_DR_data_per_DR[drug])+"\t")
    if not drug:
        f.write("[]")
    if MTB:
        f.write("\t"+DR_CLASS+"\n")
    else:
        f.write("\n")
    f.close()
    return
    
def processData_short_codes(fileX,outputFolder,DR,DR_ORDER): #Only used for M.tb
    #DR is a tab separated string "RIFAMPACIN/tSTREPTOMYCIN etc
    #largeDelData is either [-1.-1] or [0.pnca-deletion] or [3,rvnum]
    #print "this is DR"
    #print [DR]
    #raw_input("Line 691")
    os.chdir(outputFolder)
    f = open("DR_pred_results_codes.txt",'a')
    f.write(fileX.split("\t")[0]+"\t")
    DR_Classification = "DS"
    for drug in DR_ORDER: 
        #print "processing", drug
        if drug in DR:
            f.write("R\t")
            DR_Classification = "DR"
        else:
            f.write("S\t")  
    if "RIFAMPICIN" in DR and "ISONIAZID" in DR:
            DR_Classification = "MDR"
            
    if "RIFAMPICIN" in DR and "ISONIAZID" in DR and "FLUOROQUINOLONES" in DR and ("CAPREOMYCIN" in DR or "AMIKACIN" in DR or "STREPTOMYCIN" in DR):   
        DR_Classification = "XDR"
    
    if "RIFAMPICIN" in DR and "ISONIAZID" in DR and "FLUOROQUINOLONES" in DR and ("CAPREOMYCIN" in DR or "AMIKACIN" in DR or "STREPTOMYCIN" in DR) and "ETHAMBUTOL" in DR and "ETHIONAMIDE" in DR and "STREPTOMYCIN" in DR and "PYRAZINAMIDE" in DR:
        DR_Classification = "XXDR" 
    
    f.write(DR_Classification+"\n")
    f.close()
    return DR_Classification

def processData_kvarq(fileX,outputFolder,snp_and_indel_Data,largeDelData):
    #largeDelData is either [-1.-1] or [0.pnca-deletion] or [3,rvnum]
    DR = ""
    if largeDelData <> []:
        for x in largeDelData:
            if x[0] == 0:
                DR = "PYRAZINAMIDE\t"
                break
    for all_var_hits in snp_and_indel_Data:
        #print "-" * 20
        #Fist the best hist - might be more than one since could be more than one phenotype
        phenoBestHits = {}
        for hit in all_var_hits:
            #print hit
            pheno = hit[1][9]
            if pheno not in phenoBestHits:
                phenoBestHits[pheno] = hit #Store the data for this phenotype, its the only one found
            elif phenoBestHits[pheno][0] > hit[0]:
                phenoBestHits[pheno] = hit #A better raking match was found for thite phenotype, override the previous hit
        #raw_input("will now write the best data to file")
        #print "best result was"
        for pheno in phenoBestHits:
            DR+=pheno+"\t"
    return DR
    
def MATCH_PHENO_TO_VARIANTS(variantListName,FILTERED_VARIANTS,FILTERED_DELETIONS,outputFolder,MTB,debugMode):
    #############################
    variantPositionDict, allGenesDict, PHENO_ORDER = loadAllKnownVariants(variantListName)
    os.chdir(outputFolder)
    f=open("PHENO_RESULTS.txt",'w')
    f.write("SAMPLE_NAME")
    for x in PHENO_ORDER:
        f.write("\t"+x)
    if MTB:
        f.write("\tDR_CLASS\n")
    else:
        f.write("\n")
    f.close()
    #################    MTB SPECIFIC DR SUMMARY FILE   ################
    if MTB:
        f0 = open("DR_pred_results_new_format.txt",'w')
        f0.close()
        f1 = open("DR_pred_results_codes.txt",'w')
        #f = open("MTB_DR_pred_results.txt",'w')
        #f.write("SAMPLE")
        f1.write("SAMPLE")
        for drug in PHENO_ORDER:
            #f.write("\t"+drug)
            f1.write("\t"+drug)
        #f.write("\n")
        f1.write("\tDR_CLASS\n")
        #f.close()
        f1.close()
        
    ####################################################################

    for fileX in os.listdir(FILTERED_VARIANTS): 
        if not fileX.endswith(".vcf"):
            continue
        snp_and_indel_Data = findPhenoMarkersSNP_indel(fileX,FILTERED_VARIANTS,variantPositionDict,allGenesDict,MTB)
        #This is a list of [level,DB reference line] , so [1,text text text]
        largeDelData = findPhenoMarkersLARGE_DEL(fileX,FILTERED_DELETIONS,variantPositionDict, allGenesDict, MTB)

        if MTB:
            PHENO = processData_kvarq(fileX,outputFolder,snp_and_indel_Data,largeDelData)
            DR_CLASS = processData_short_codes(fileX,outputFolder,PHENO,PHENO_ORDER)
        
        processData(fileX,outputFolder,snp_and_indel_Data,largeDelData,PHENO_ORDER,DR_CLASS,MTB)
    return
#END of phenotype matching
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
def autoAnnotateEMBL(emblDir, annotationFile, outputDir, VCF_Location_List,mapperOrderList):
    def recalculate_AA_changes(overlapData,annotatedData):
        '''
        for each line and for each mapper, change the codon and AA change to be the correct info taking into account the snps before and after its pos
        this does not condence the snps because there could be differencs between the mappers
        '''
        def convertAALetterChange(s):
            #To convert A:I,I,I to A/I
            s = s.replace('"','')
            if ":" not in s:
                return ""
            ref = s[0]
            data = s[2:]
            data = data.split(",")
            mut = data[2]
            if mut == "-":
                mut = data[1]
            if mut == "-":
                mut = data[2]  
            return ref+"/"+mut #may sometimes return A/Syn
        
        def merge(lastRefCodon,lastMutCodon,currentMutCodon):
            #print [lastRefCodon,currentRefCodon,lastMutCodon,currentMutCodon]
            #if lastRefCodon <> currentRefCodon:
            #    print "Error matching during codon merge step:"
            #    print lastRefCodon, currentRefCodon
            #    raw_input()
            result = ""
            for pos in range(3):
                #print lastMutCodon, pos
                #print lastMutCodon[pos]
                if lastMutCodon[pos] <> lastRefCodon[pos]:
                    result += lastMutCodon[pos]
                elif currentMutCodon[pos] <> lastRefCodon[pos]:
                    result += currentMutCodon[pos]
                else: 
                    result += lastRefCodon[pos]
            return result
            
        def calculateMutResult(refCodon, mutCodon1,mutCodon2,mutCodon3):
            result = merge(refCodon,mutCodon1,mutCodon2)
            if mutCodon3 <> "":
                result = merge(refCodon,result,mutCodon3)
            refAA = translate(refCodon)
            mutAA = translate(result)
            if refAA <> mutAA:
                new_syn_nonSyn = "nonSyn"
            else:
                new_syn_nonSyn = "Syn"
            return result,refAA+":"+mutAA, new_syn_nonSyn

        ##############################################################
        #new_annoData = annotatedData[:]
        maxPos = len(overlapData)-1
        pos = -1
        while pos < maxPos: #using look ahead method
            pos += 1
            temp_overlap_data1 = overlapData[pos]
            temp_anno_data1 = annotatedData[pos][1] #    [['nonSyn', 'I:T', 245, 'ATC', 'ACC', 733],[],[]]
            if temp_anno_data1 == ['intergenic']:
                #keep this line as it is
                continue
                    
            currentPos1 = temp_overlap_data1[0]
            ''' GUIDE IS:
                syn_non_syn1 = mapperData[0]
                aaChange1 = mapperData[1]
                codon1 = mapperData[2]
                refCodon1 = mapperData[3]
                mutCodon1 = mapperData[4]
                genePos1 = mapperData[5]
            '''
            #print pos
            #print temp_overlap_data1
            #print temp_anno_data1
            #raw_input("entering test zone here")

            for mapperPos in range(len(temp_anno_data1)):  #for each mapper update, data is: [['nonSyn', 'I:T', 245, 'ATC', 'ACC', 733], ['nonSyn', 'I:T', 245, 'ATC', 'ACC', 733], ['nonSyn', 'I:T', 245, 'ATC', 'ACC', 733]]
                #extract the mapper data for this position and up to two positions ahead
                mapperData1 = temp_anno_data1[mapperPos] #bwa,novo,smalt
                if mapperData1 == "N/A":
                    continue
                #print "reach one ahead"
                if pos < maxPos:
                    temp_overlap_data2 = overlapData[pos+1]
                    currentPos2 = temp_overlap_data2[0]
                    temp_anno_data2 = annotatedData[pos+1][1]

                    if mapperData1 == "INDEL" or mapperData1 == ['', '', '', '', '', ''] or temp_anno_data2 == ['intergenic'] or int(currentPos2)-int(currentPos1) <> 1: #snps must be 1 bp apart
                        #print "SKIPPING11"
                        continue
                    mapperData2 = temp_anno_data2[mapperPos]
                    if len(mapperData2) == 1:
                        continue
                    if mapperData2[2] == mapperData1[2] and mapperData1[2] <> "INDEL": #if the codons match
                        #print "two codons match!", mapperData1[2], mapperData2[2]
                        #reach one more ahead
                        if pos+1 < maxPos:
                            temp_overlap_data3 = overlapData[pos+2]
                            currentPos3 = temp_overlap_data3[0]
                            temp_anno_data3 = annotatedData[pos+2][1]
                            if temp_anno_data3 <> ['intergenic'] and int(currentPos3) - int(currentPos2) == 1: #snps must be 1 bp apart
                                mapperData3 = temp_anno_data3[mapperPos]
                                if len(mapperData3) <> 1:
                                    if mapperData1[2] == mapperData2[2] and mapperData2[2] == mapperData3[2] and mapperData3[2] <> "INDEL": # all 3 snps are in the same AA codon for THIS mapper
                                        #print "3 codons match!"
                                        #calculate new ATG -> ATC, new AA letter change, new syn/non syn 
                                        new_mutCodon, new_AA_letter,new_syn_nonSyn = calculateMutResult(mapperData1[3],mapperData1[4],mapperData2[4],mapperData3[4]) #refCodon, mutCodon1,mutCodon2,mutCodon3)
                                        #update the snp positions of new_annoData to reflect this new data
                                        annotatedData[pos  ][1][mapperPos][0] = new_syn_nonSyn 
                                        annotatedData[pos+1][1][mapperPos][0] = new_syn_nonSyn  
                                        annotatedData[pos+2][1][mapperPos][0] = new_syn_nonSyn 
                                        annotatedData[pos  ][1][mapperPos][4] = new_mutCodon
                                        annotatedData[pos+1][1][mapperPos][4] = new_mutCodon  
                                        annotatedData[pos+2][1][mapperPos][4] = new_mutCodon 
                                        annotatedData[pos  ][1][mapperPos][1] = new_AA_letter 
                                        annotatedData[pos+1][1][mapperPos][1] = new_AA_letter  
                                        annotatedData[pos+2][1][mapperPos][1] = new_AA_letter 
                                        #print "found one case of tripple!", pos, currentPos1
                                        #raw_input()
                                        continue
                        #calculate the new AA, new syn/non syn, add 2 lines to new_annoData using updated data
                        #print "running prog with these params"
                        #print [mapperData1[3],mapperData1[4],mapperData2[4],""]
                        new_mutCodon, new_AA_letter,new_syn_nonSyn = calculateMutResult(mapperData1[3],mapperData1[4],mapperData2[4],"") #refCodon, mutCodon1,mutCodon2,mutCodon3)
                         
                        annotatedData[pos] [1][mapperPos][0] = new_syn_nonSyn 
                        annotatedData[pos+1][1][mapperPos][0] = new_syn_nonSyn  
                        annotatedData[pos  ][1][mapperPos][4] = new_mutCodon
                        annotatedData[pos+1][1][mapperPos][4] = new_mutCodon   
                        annotatedData[pos  ][1][mapperPos][1] = new_AA_letter 
                        annotatedData[pos+1][1][mapperPos][1] = new_AA_letter  
                        #print "found one case of double!", pos, currentPos1
                        #raw_input()
                        #update the snp positions of new_annoData to reflect this new data
                        continue
                    #add the existing line as is, no changes needed
                    #Everything stays the same 
                    else:
                        #print "codons do not match", mapperData1[2], mapperData2[2]
                        continue
        return annotatedData

    ####################################################################
    def load_embl_feature_coordinates(annotationFile,dirX):
        def extractFeatureType(line):
            line = line.replace("FT", "")
            flag1 = False
            flag2 = False
            s = ""
            for char in line:
                if char <> " ":
                    s += char
                    flag1 = True
                if flag1 and char == " ":
                    return s
            return "ERROR"
        #######################################################    
        def extractRanges(line,nums):
        
            orientation = ""
            flag1 = False
            flag2 = False 
            #print "Processing feature:", [line]
            start = ""
            end = ""
            if not "complement" in line:
                orientation = "+"
            elif "complement" in line:
                orientation = "-"    
            for char in line:
                if char == "." and flag1:
                    flag2 = True
                    continue
                elif char not in nums:
                    continue
                if char in nums and not flag2: 
                    start += char
                    flag1 = True
                elif char in nums and flag2:
                    end += char
            return [start,end,orientation]
        ##################################################################
        #store the ranges
        #store the data for these ranges
        embl_ref_seq = ''
        pos_to_feature_num_dict = {} # Tier1 Key: a dict of entire genome linking to a feature range, 1:Gene1, 2:Gene1, ...1300:Gene1, 1331:Gene2
        featureNum_to_Anno_DataDict = {} #Tier2 Key: a dict of feature/gene number to the annotation data GeneNum:[Anno]
        featureNum_to_seq_DataDict = {} #Tier2 Key: a dict of feature/gene number to the SEQUENCE data GeneNum:[Anno]

        #print "loading EMBL file..."
        os.chdir(dirX)
        try:
            f = open(annotationFile)
        except:
            f = open(annotationFile+".embl")
            
        embl_data = []
        line = f.readline()
        if "ID" not in line:
            print "EMBL file format error, the genome length in BP was never found"
            raw_input()
            return "ERROR"
        elif "ID  " in line:
            gen_len = line.split(" BP")[0]
            gen_len = gen_len.split(";")[-1]
            gen_len = gen_len.replace(" ","")
            try:
                gen_len = int(gen_len)
            except:
                print "EMBL file format, the ID line should contain a ID header for example:"
                print "ID   AL123456; SV 3; ; DNA; ; UNC; 4411532 BP."
                print "However, this line was not present or the format is incorrect"
                raw_input()
                return "ERROR"
                
        for x in range(gen_len): #Initialize entire genome to contain no anno data
            pos_to_feature_num_dict[x] = []
                       
        orientation = "+"
        line = "START"
        nums = ["1","2","3","4","5","6","7","8","9","0"] 
        feature_types = []
        known_feature_properties = []
        featureNumber = 1
        while line and "SQ  " not in line: # and "SQ\t" not in line:   
            #if "FT" not in line and ".." not in line:
            #    line = f.readline() #a[pos]
                
            #print "HERE IS CURRENT LINE EVALUATED", [line]
            if "FT" in line and "source" in line:
                line = f.readline() #a[pos]
                continue
            if "FT" not in line or ".." not in line:
                #print "skipping line", line
                #print "-"*50
                line = f.readline()
                continue
            #else:
                #print "Evaluating this line:", [line]
            featureNumber += 1
            feature_type = extractFeatureType(line) 
            #print "detected as", feature_type
            if feature_type not in feature_types:
                feature_types.append(feature_type)
            runOnceFlag = False
            ranges = []
            multiJoin = 0
            while line and ("," in line or runOnceFlag == False) and ("FT" in line and ".." in line):
                runOnceFlag = True 
                multiJoin += 1
                start,end,orientation = extractRanges(line,nums) 
                ranges.append([start,end])
                line = f.readline()
                #print "moving on to this line1", [line]
                continue
            if multiJoin > 1 and  ("FT" in line and ".." in line):
                start,end,orientation = extractRanges(line,nums) 
                ranges.append([start,end])
                line = f.readline()
                #print "now moving on to this next line2", [line]
                continue
            #else:
            #    print "not multijoin situation for line", line
                
            temp_features = []
            meta_data = False
            #print "current line is", line
            #print "/" in line
            #print "=" in line
            #while (line) and ("/" in line) and ('="' in line): # add additional features
            while (line) and ("/" in line) and ('=' in line) and ("%)" not in line) and ("Identities" not in line): # add additional features
                #meta_data = True
                temp_feature_type = line.split("/")[1].split("=")[0]
                #if ['895821', '898084'] in ranges:
                #    print "debug featyre type is"
                '''
                if temp_feature_type not in known_feature_properties:
                    print "TO ADD:", line
                    print "temp line", [line]
                    print "new feature type found:"
                    print temp_feature_type
                    raw_input()
                '''
                
                if temp_feature_type not in known_feature_properties:
                    known_feature_properties.append(temp_feature_type)
                temp_feature_detail = line.split("=")[1].replace("\n"," ")
                
                line = f.readline()
                #print "is this the problem", [line]
                if line and "/" in line and "=" in line:
                    temp_features.append([temp_feature_type,temp_feature_detail])
                    continue
                while "FT                   " in line : #add addional data for a long line
                    #if ['895821', '898084'] in ranges:
                    #    print "debug2:"
                    #    print line
                        
                    temp_feature_detail += line.replace( "FT                   ","")
                    temp_feature_detail = temp_feature_detail.replace("\n","")
                    line = f.readline()
                    #if ['895821', '898084'] in ranges:
                    #    print "and now the next line is"
                    #    print [line]
                    #    print "FT                   " in line
                    #    print "?"
                    if line and "/" in line and "=" in line:
                        #if ['895821', '898084'] in ranges:
                        #    print "/" in line
                        #    print [line]
                        #    print "OH NO!"
                        temp_features.append([temp_feature_type,temp_feature_detail])
                        break
                    continue
                continue
            temp_features.append([temp_feature_type,temp_feature_detail])
            #if ['895821', '898084'] in ranges:
            #print "spiffy data"
            #print "this is the info for a single entry"
            #print feature_type
            #print ranges
            #print orientation
            #print temp_features
            #raw_input("exact or not?")

            for rangeX in ranges: #store the gene feature number in lookup key dict for rapid access
                for position in range(int(rangeX[0]),int(rangeX[1])):
                    if position+1 in pos_to_feature_num_dict: #already have data for this genomic position
                        #print featureNumber, pos_to_feature_num_dict[position+1]
                        if featureNumber not in  pos_to_feature_num_dict[position+1]:
                            pos_to_feature_num_dict[position+1].append(featureNumber) #A genomic location can code for more than one gene
                    else:
                        pos_to_feature_num_dict[position+1] = [featureNumber] #store the anno+seq lookup key for this genoimic position
                    #also need to know what each key is for, store the anno and seq data for this loopkup key:
                    if featureNumber not in featureNum_to_Anno_DataDict:
                        featureNum_to_Anno_DataDict[featureNumber] = [ranges,feature_type,orientation,temp_features]
                        #remember to also append the sequence to this list now also
                    
                    
            #embl_data.append([ranges,feature_type,orientation,temp_features])
            #print embl_data
            #raw_input()
   
        ################## LOAD REF INTO MEM ###############################
        #print "before search for SQ"
        #print [line]
        #raw_input()
        if "SQ  " in line:
            line = f.readline()
            #print [line]
            while line:
                for char in line:
                    if char.lower() not in ["a","t","g","c","u"]:
                        continue
                    embl_ref_seq += char
                line = f.readline()
            
            
        #Store the seq of each featureKeyNumber so {1:ATGTCA}
        counterX = 0
        for featureElement in featureNum_to_Anno_DataDict:
            counterX +=1
            #print counterX
            try:
                ranges = featureNum_to_Anno_DataDict[featureElement][0]
                orientation = featureNum_to_Anno_DataDict[featureElement][2]
            except:
                print "ERROR X6375!"
                print featureElement
                print featureNum_to_Anno_DataDict[featureElement]
                raw_input()
            
            
            seq = retrieve_embl_seq(ranges, embl_ref_seq, orientation) #fetch this seq and store it in mem
            #raw_input("currently fixing this part, for bovis fetching the seq is not working")
            if seq == "":
                print "error loaing embl sequence file"
                print "ranges are", [ranges]
                
                #print "embl ref seq is",[embl_ref_seq]
                #print "orientation is ",[orientation]
                raw_input("ERROR, SEQ LEN 0 RETURNED FOR THIS ENTRY")
##            print [seq]
##            print "was for", featureNum_to_Anno_DataDict[featureElement]
##            raw_input("does seq match?")
            featureNum_to_seq_DataDict[featureElement] = seq
            
        print len(featureNum_to_seq_DataDict), "reference genomic features loaded"
        #embl_data = sorted(embl_data, key = lambda x: int(x[0][0][0]))
        f.close()
        #print known_feature_properties
        known_feature_properties.sort()
        return pos_to_feature_num_dict, featureNum_to_Anno_DataDict, featureNum_to_seq_DataDict, known_feature_properties

        ###################################################################
    
    def obtain_file_list(VCF_Location_List):
        #Returns dictionary of all VCF files (allows missing data)
        unique = {}
        for VCF_Dir in VCF_Location_List:
            os.chdir(VCF_Dir)
            for fileX in os.listdir(VCF_Dir):
                if not fileX.endswith(".vcf"):
                    continue
                if fileX.split("_")[0] not in unique:

                    unique[fileX.split("_")[0]] = True
        return unique #R100, R153, S55125
    ###################################################
    
    def getMapperOverlap(mapperDataDictList): #[ {pos: data, pos: data} , {pos: data, pos: data} ]
        allSNPPositions = {}
        overlapData = []
        mapperAmount = 0
        #overlapData.append("#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tEXTRAINF\tBWA_QUAL\tBWA_INFO\tBWA_EXTRA_INFO\tNOVO_QUAL\tNOVO_INFO\tNOVO_EXTRA_INFO\tSMALT_QUAL\tSMALT_INFO\tSMALT_EXTRA_INFO\tMUTATIONS\tMAPPER_COUNT\n")
        for dataDict in mapperDataDictList:
            mapperAmount += 1
            for pos in dataDict:
                if pos not in allSNPPositions:
                    allSNPPositions[pos] = True                
        for pos in allSNPPositions: #for each known variant position
            if pos == "":
                continue
            pos = str(pos)
            mapperCount = 0
            mutations = ""
            alreadyHaveBodyData = False
            snpLineData = []
            bodyData = None
            for mapperDataDict in mapperDataDictList:
                if pos in mapperDataDict: 
                    if not alreadyHaveBodyData:
                        bodyData = mapperDataDict[pos]# .replace("\n","\t")
                        alreadyHaveBodyData = True
                    temp = mapperDataDict[pos].split("\t")
                    #snpLineData.append([temp[5],temp[7],temp[9].replace("\n","\t")]) #I am skipping the use of this info from col 7
                    snpLineData.append([temp[3],temp[4],temp[5],temp[9].replace("\n","\t")])
                    mutations += temp[4]+"/" # the alt data from the 4th coloumb
                    mapperCount +=1
                else:
                    snpLineData.append(["\t","\t","\t","\t"])
                    mutations    += "*/"

            temp = bodyData.split("\t")
            pos = temp[1]
            chrom = temp[0]

            #Mapper1: col 3(ref),4(alt),5(qual),7(info1),9(info2)
            #Mapper2: col 3(ref),4(alt),5(qual),7(info1),9(info2)
            #Mapper3: col 3(ref),4(alt),5(qual),7(info1),9(info2)
            
            overlapData.append([int(pos),chrom, snpLineData, str(mapperCount),mutations[:-1]]) #mutations[:-1]+"\tMSum="+str(mapperCount)])
            #print "-"*20
            #print overlapData
            #print "-"*20
            #raw_input("This is the current overlap data")

        overlapData.sort()
##        for x in overlapData:
##            print x
##            raw_input("TESTING 6667")
        return overlapData, mapperAmount
    
    #######################################################################
        
        
    def reverse_complement(seq):
        seq = seq[::-1]
        temp = ""
        ura = False
        for char in seq:
            char = char.upper()
            if char == "A":
                temp+=("T")
            elif char == "T":
                temp+=("A")
            elif char == "C":
                temp+=("G")
            elif char == "G":
                temp+=("C")
            elif char == "U":
                ura = True
                temp += "A"
            else:
                print "Error base error",[char],"found in",
                print [seq]
                temp+=("?")
    ##            raw_input()
        if ura:
            seq = seq.replace("U","T")
        return temp
        
    def translate(seq):
        seq = seq.upper()
        gencode = {
        'ATA':'I', 'ATC':'I', 'ATT':'I', 'ATG':'M',
        'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',
        'AAC':'N', 'AAT':'N', 'AAA':'K', 'AAG':'K',
        'AGC':'S', 'AGT':'S', 'AGA':'R', 'AGG':'R',
        'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',
        'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',
        'CAC':'H', 'CAT':'H', 'CAA':'Q', 'CAG':'Q',
        'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',
        'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',
        'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',
        'GAC':'D', 'GAT':'D', 'GAA':'E', 'GAG':'E',
        'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',
        'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',
        'TTC':'F', 'TTT':'F', 'TTA':'L', 'TTG':'L',
        'TAC':'Y', 'TAT':'Y', 'TAA':'*', 'TAG':'*',
        'TGC':'C', 'TGT':'C', 'TGA':'*', 'TGG':'W',
        }
     
        #print seq
        protSeq = ''
        for x in range(0,len(seq),3):
            if gencode.has_key(seq[x:x+3]) == True:
                protSeq += gencode[seq[x:x+3]]
            else:
           #     print "Warning, sequence is not a multiple of 3...truncating sequence..."
                return protSeq
        return protSeq
                   
    def getSynorNonSynonMutation(mapperData,orientation,snpPosition,ref,mutList2,annotationStartSite,annotationEndSite,seq,annoData):
        if "," not in mutList2 and len(mutList2) > 1:
            changeType ="INDEL"
            aminoAcidChange = "INDEL"
            changedAminoAcidPosition = "INDEL"
            codonsRef = "INDEL"
            codonsMut = "INDEL"
            codonPosition = "INDEL"
            return [changeType,aminoAcidChange,changedAminoAcidPosition,codonsRef,codonsMut,codonPosition]
        elif "," in mutList2:
            temp = mutList2.split(",")
            if len(temp[0]) >1 or len(temp[1]) >1:
                changeType ="INDEL"
                aminoAcidChange = "INDEL"
                changedAminoAcidPosition = "INDEL"
                codonsRef = "INDEL"
                codonsMut = "INDEL"
                codonPosition = "INDEL"
                return [changeType,aminoAcidChange,changedAminoAcidPosition,codonsRef,codonsMut,codonPosition]
            else:
                changeType ="ERROR_Multiple_Bases"
                aminoAcidChange = "ERROR_Multiple_Bases"
                changedAminoAcidPosition = "ERROR_Multiple_Bases"
                codonsRef = "ERROR_Multiple_Bases"
                codonsMut = "ERROR_Multiple_Bases"
                codonPosition = "ERROR_Multiple_Bases"
                return [changeType,aminoAcidChange,changedAminoAcidPosition,codonsRef,codonsMut,codonPosition]
           
        mutList2 = mutList2.replace(",","")
        mutList2 = mutList2.replace('"',"")
        codonsMut = ""
        codonsRef = ""
        codonPosition = snpPosition - annotationStartSite #This means we start from 0, so pos 0 is the fist codon
        changedAminoAcidPosition = (codonPosition/3)+1
        #Thus we have:
        #012 345 678 --> BP Pos
        #111 222 333 --> AA Pos
        changeType = "EMPTY" # indicates no data was added and there was error
        coding_dna = seq
        
        if len(ref) > 1:
            changeType ="INDEL"
            aminoAcidChange = "INDEL"
            changedAminoAcidPosition = "INDEL"
            codonsRef = "INDEL"
            codonsMut = "INDEL"
            codonPosition = "INDEL"
            return [changeType,aminoAcidChange,changedAminoAcidPosition,codonsRef,codonsMut,codonPosition]
        try:
            if coding_dna[codonPosition].lower() <> ref.lower():
                print coding_dna
                print "ERROR, ref does not match the calculated embl NT, fasta and embl mismatch?"
                print coding_dna[codonPosition]
                print "REF",[ref.lower()], orientation
                print "mapper data here was:", mapperData
                print annoData
                raw_input()
        except:
            print "Error, index out of range, debug poin 2083"
            raw_input()
            
        temp_coding_dna = coding_dna    
        if orientation == "-":
            changedAminoAcidPosition = ((len(coding_dna)-1 - codonPosition) /3) +1
            #Then the coding DNA loaded above is in protein coding orientation, and thus must be rev comp to get the ref FASTA seq so lets do that now
            temp_coding_dna = reverse_complement(coding_dna)
            #Now the DNA seq matches the reference FASTA seq and we can add the mutated DNA at the correct position 
            #Keep in mind we must rev compl the DNA back to coding orientation before we can translate to protein to get the changed AA and AA position
                    
        protein = translate(temp_coding_dna)
        mutCount = 0
        for mut in mutList2: #one snp at a time... G/G/G
            if mut == "/":
                continue
            if mut.upper() not in ["A","T","G","C","*",] or len(mut) > 1:
                print "error, mutation is not in [A,T,G,C]"
                print "mut is, ",mut
                print "mutlistis ", mutList2
                raw_input()
            mutCount +=1
            if mut == "*":
                if changeType == "EMPTY":
                    changeType = "-"
                    aminoAcidChange = "-"
                    continue
                else:
                    changeType += ",-"
                    aminoAcidChange += ",-"
                    continue
            if coding_dna[codonPosition].lower() == mut.lower(): 
    ##                    print "FASTA_AND_EMBL_MISMATCH"
                if changeType == "EMPTY":    
                    changeType = 'FASTA_AND_EMBL_MISMATCH'
                    aminoAcidChange = "None"
                else:
                    if "FASTA_AND_EMBL_MISMATCH" in changeType:
                        changeType += ",FASTA_AND_EMBL_MISMATCH"
                    else:
                        changeType += ",None"
                        aminoAcidChange += ",None"
                continue
            mutatedCoding_dna = str(coding_dna) #make a copy so that we can introduce a mutation 
            #Keep in mind the dna is linear from left to right and codonPosition just shows which pos in string is mutated, regardless of orientation
            #Here we introdude the mutation into the dna
            if codonPosition == 0:
                mutatedCoding_dna = mut.lower()+mutatedCoding_dna[1:]
            else:        
                mutatedCoding_dnaA = mutatedCoding_dna[:codonPosition]
                mutatedCoding_dnaB = mutatedCoding_dna[codonPosition+1:]                
                mutatedCoding_dna = mutatedCoding_dnaA+mut.lower()+mutatedCoding_dnaB
            if orientation == "-":
                mutatedCoding_dna = str(reverse_complement(mutatedCoding_dna)) # need to reverse it back to protien coding orientation before translating to protein
            codonsMut = ""
            codonsRef = ""

            #Here we store the 3 bases for the affected aa codon, both original and mutated codons. so idea is to keep all shifts like: ATG-->AGG
            # SPECIAL NOTE!!! THE CODONS STORE ARE IN READING FRAME ORIENTATION!! SO watch out, change from GCC in  ref gene which is negative orientation
            # will and should show as GGC in mutatedcodon 
            ###########################################################################################################################
            if orientation == "+":
                state = codonPosition % 3
                if state == 0: #mutation is at first codon out of 3 possibles [X][][]
                    codonsMut = mutatedCoding_dna[codonPosition]
                    codonsRef = str(coding_dna)[codonPosition]
                    if codonPosition+1 >= len(mutatedCoding_dna):
                        codonsMut += "?"
                        codonsRef += "?" 
                    else:
                        codonsMut += mutatedCoding_dna[codonPosition+1]
                        codonsRef += str(coding_dna)[codonPosition+1]
                    if codonPosition+2 >= len(mutatedCoding_dna):
                        codonsMut += "?"
                        codonsRef += "?" 
                    else:
                        codonsMut += mutatedCoding_dna[codonPosition+2]
                        codonsRef += str(coding_dna)[codonPosition+2]
                        
                elif state == 1: #mutation is at second codon out of 3 possibles [][X][]
                    codonsMut = mutatedCoding_dna[codonPosition-1] #add one upstream of current
                    codonsRef = str(coding_dna)[codonPosition-1] # add one upstream of current
                    codonsMut += mutatedCoding_dna[codonPosition] #add current pos
                    codonsRef += str(coding_dna)[codonPosition] # add current pos
                    if codonPosition+1 >= len(mutatedCoding_dna):
                        codonsMut += "?"
                        codonsRef += "?"
                    else:
                        codonsMut += mutatedCoding_dna[codonPosition+1] #add one upstream
                        codonsRef += str(coding_dna)[codonPosition+1] #add one upstream
                        
                elif state == 2: #mutation is at third codon out of 3 possibles [][][X]
                    codonsMut = mutatedCoding_dna[codonPosition-2]+mutatedCoding_dna[codonPosition-1]+mutatedCoding_dna[codonPosition]
                    codonsRef = str(coding_dna)[codonPosition-2]+str(coding_dna)[codonPosition-1]+str(coding_dna)[codonPosition]
                ######################################################################################################################3    
##                print "This was calculated::"
##                print codonsMut
##                print codonsRef
##                raw_input("6665")
            elif orientation == "-":
                mutatedBP = len(mutatedCoding_dna)-1 - codonPosition # snpPosition-annotationSEndSite
                state = mutatedBP % 3

                if state == 0: #mutation is at first codon out of 3 possibles [X][][]
                    codonsMut = mutatedCoding_dna[mutatedBP]
                    codonsRef = str(reverse_complement(coding_dna))[mutatedBP]
                    if mutatedBP+1 >= len(mutatedCoding_dna):
                        codonsMut += "?"
                        codonsRef += "?" 
                    else:
                        codonsMut += mutatedCoding_dna[mutatedBP+1]     
                        codonsRef += str(reverse_complement(coding_dna))[mutatedBP+1]       
                    if mutatedBP+2 >= len(mutatedCoding_dna):
                        codonsMut += "?"
                        codonsRef += "?" 
                    else:
                        try:
                            codonsMut += mutatedCoding_dna[mutatedBP+2]
                            codonsRef += str(reverse_complement(coding_dna))[mutatedBP+2]
                        except:
                            print "Error indexing"
                            print "mutated BP =",mutatedBP
                            print mutatedBP+1
                            print mutatedBP+2
                            print "len is",[len(mutatedCoding_dna)]
                            print "len is",[len(reverse_complement(coding_dna))]
                            raw_input("here is error:")
                            print mutatedCoding_dna[mutatedBP]
                            print mutatedCoding_dna[mutatedBP+1]
                            print mutatedCoding_dna[mutatedBP+2]
                            raw_input()
    ##                raw_input("state 0 results above")
                            
                elif state == 1: #mutation is at second codon out of 3 possibles [][X][]
                    codonsMut = mutatedCoding_dna[mutatedBP-1] #add one upstream of current
                    codonsRef = str(reverse_complement(coding_dna))[mutatedBP-1] # add one upstream of current
                    codonsMut += mutatedCoding_dna[mutatedBP] #add current pos
                    codonsRef += str(reverse_complement(coding_dna))[mutatedBP] # add current pos
                    if mutatedBP+1 >= len(mutatedCoding_dna):
                        codonsMut += "?"
                        codonsRef += "?"
                    else:
                        codonsMut += mutatedCoding_dna[mutatedBP+1] #add one upstream
                        codonsRef += str(reverse_complement(coding_dna))[mutatedBP+1] #add one upstream
                        
                elif state == 2: #mutation is at third codon out of 3 possibles [][][X]
                    codonsMut = mutatedCoding_dna[mutatedBP-2]+mutatedCoding_dna[mutatedBP-1]+mutatedCoding_dna[mutatedBP]
                    codonsRef = str(reverse_complement(coding_dna))[mutatedBP-2]+str(reverse_complement(coding_dna))[mutatedBP-1]+str(reverse_complement(coding_dna))[mutatedBP]
            try:
                mutatedProtein = translate(mutatedCoding_dna)
##                print "-" * 15
##                print mutatedCoding_dna
##                print "." * 15
##                print "THIS IS THE PROTEIN"
##                print mutatedProtein
##                raw_input("6666")
            except:
                 print "error during annotation "
                 print "mut is ", mut
                 print "this is mutList", mutList2
                 print snpPosition
                 print str(mutatedCoding_dna)
                 raw_input()
                
            pos = 0
            protein = str(protein)
            mutatedProtein = str(mutatedProtein)
            
            if len(protein) <> len(mutatedProtein):
                print "protein len mismatches"
                #print rvNumber
                print len(protein),len(mutatedProtein)
                print protein
                print "and"
                print mutatedProtein
                raw_input()
            
            if str(protein) == str(mutatedProtein):
    ##            print "protein and mutated protein are:"
    ##            print [protein]
    ##            print [mutatedProtein]
    ##            raw_input()
                if changeType == "EMPTY":
                    changeType = "syn"
                    aminoAcidChange = "None"                  
                else:
                    changeType += ",syn"
                    aminoAcidChange += ",None"
            else: #there is a AA change
    ##            print "non Syn mutation"
                while protein[pos] == mutatedProtein[pos] and pos < len(protein)-1:
                    pos += 1
                    #print protein[pos], mutatedProtein[pos],  protein[pos] == mutatedProtein[pos], pos
                if protein[pos] <> mutatedProtein[pos]:
                    mutatedAACodon = pos+1 #plus one because we started counting from 0
                else:
                    print "could not find any protein differences? error, fix"
                    raw_input()

                #Failsafe to make sure the actual changed AA matches the calculated AA change position
                if mutatedAACodon <> changedAminoAcidPosition:
                    print "error, aa codon problem" #errorReport()
                    print protein
                    print "and"
                    print mutatedProtein

                    print  "Codons are",mutatedAACodon, changedAminoAcidPosition
                    print "and mut seq is"
                    print mutatedCoding_dna
                    print "anno is", annoData
                    print mapperData
                    print orientation
                    print snpPosition
                    print ref
                    print mutList2
                    print annotationStartSite,annotationEndSite
##                    print seq
                    print annoData
                    raw_input()

                if changeType == "EMPTY": #Then its the first entry in the mutated AA list and syn/nonSyn list
                    changeType = "nonSyn"
                    aminoAcidChange = protein[pos]+":"+mutatedProtein[pos]
                    
                else: #its not the frist entry and we need a comma to separate them
                    changeType += ",nonSyn"                        
                    aminoAcidChange += ","+mutatedProtein[pos]
                
##                print changeType,aminoAcidChange,changedAminoAcidPosition, orientation
##                print codonsRef.upper(),codonsMut.upper(),codonPosition
##                raw_input("debug point 9978")
##        return [changeType,aminoAcidChange,changedAminoAcidPosition,codonsRef.upper(),codonsMut.upper(),codonPosition]
            if orientation == "-":
                codonPosition = annotationEndSite - snpPosition
        return [changeType,aminoAcidChange,changedAminoAcidPosition,codonsRef.upper(),codonsMut.upper(),str(int(codonPosition)+1)]


    #def retrieveEMBLData(pos,embl_data):
    #    #this returns all the ranges for this feature, not just the hit one
    #    posX = int(pos)
    #    maxPos = len(embl_data)-1
    #    minPos = 0
    #    debugLimit = 100
    #    debugCount = 0
    #    while True:
    #        debugCount += 1
    #        hit = False
    #        current = minPos + (maxPos-minPos)/2  
    #        annoData = embl_data[current]
    #        posRanges = annoData[0]
    #        if debugCount >= debugLimit:
    #            print "stuck!"
    #            print annoData
    #            print "---------------current, min max pos---------------"
    #            print "looking for:", posX
    #            print "currentPos:", current,"->", posRanges
    #            print "MIN:",minPos,"->",embl_data[minPos][0]
    #            print "MAX:",maxPos,"->",embl_data[maxPos][0]
    #            raw_input()
    #            
    #        for rangeX in posRanges:
    #            start = int(rangeX[0])
    #            end = int(rangeX[1])
    #            if posX >= start and posX <= end:
    #                hit = True
    #                break
    #        if hit:
    #            return annoData, posRanges
    #        
    #        elif minPos == maxPos or maxPos-minPos <= 7:
    #            for tempPos in range(minPos,maxPos+1):
    #                try:
    #                    posRanges = annoData[0]
    #                    for rangeX in posRanges:
    #                        start = int(rangeX[0])
    #                        end = int(rangeX[1])
    #                        if posX >= start and posX <= end:
    #                            hit = True
    #                            return annoData, posRanges                        
    #                except:
    #                    print "should not happen"
    #                    raw_input("ERROR!")
    #            if not hit:
    #                #raw_input("THERE IS NO HIT!!!")
    #                return "ncDNA", [posX]
    #        ###########################
    #        elif posX > end:
    #            #print "posx, end:"
    #            #print posX, end
    #            #print "value is to the right, raising lower limit", current
    #            minPos = current
    #            #raw_input("stuck in infinate loop!")           
    #                            
    #            continue
    #        elif posX < start:
    #            #print "posX, start:"
    #            #print "Value is to the left, dropping upper limit", current
    #            maxPos = current
    #            #raw_input("stuck in infinate loop!")    
    #            continue
    #    return
        #########################################################
    def retrieve_embl_seq(found_ranges, embl_ref_seq, orientation):
        def countBP(s):
            count = 0
            for x in s:
                if x.lower() in ["a","t","g","c","u"]:
                    count+=1
            return count
        def rem_non_bp(line):
            s = ""
            for x in line:
                if x not in ["a","t","g","c","u"]:
                    continue
                else:
                    s += x
            return s
        ########################  
        seq = ""
        for found_range in found_ranges:
            start = int(found_range[0])-1
            if start <= 0:
                start = 1
            #Changes on 5 december 2015 to -1
            end = int(found_range[1])
            #print start,end, found_range
            #print embl_ref_seq[start-1:end]
            #raw_input("debug777444")
            seq += embl_ref_seq[start-1:end]
        return seq 
                
       
    def getAnnoData(fileX, overlapData, mapperAmount, known_feature_properties, pos_to_feature_num_dict, featureNum_to_Anno_DataDict, featureNum_to_seq_DataDict):
        annoDataList = []
        total = len(overlapData)
        oldProgress = 0
        count = 0.0
        for element in overlapData:
            count+=1
            progress =  (100*count) / total
            if progress % 10 == 0:
                if oldProgress <> progress:
                    print str(int(progress))+"% of annotation completed for:",fileX
                    oldProgress = progress

            pos = element[0]
            key = pos_to_feature_num_dict[pos]
            #print "(((((((((("
            #print [key],
            #raw_input("llll")
            #have_anno = True
            if key == []:
                #calculatedAnnoData.append(["intergenic"])
                heteroDataList = []
                numReads = []
                for mapperData in element[2]:
                    #print mapperData
                    #raw_input()
                    if mapperData == ["\t","\t","\t","\t"]:
                        heteroDataList.append("N/A")
                        numReads.append("N/A")
                        continue
                    else:
                        temp = mapperData[3].split(":")
##                        print element
##                        print mapperData
##                        print temp
                        try:
                            heteroDataList.append(float(temp[1].split(",")[1])/float(temp[2]))
                            numReads.append(int(temp[2]))
                        except:
##                            print "error calculating heterogeneity:",element,mapperData,fileX
                            heteroDataList.append("error")
                            numReads.append("error")
                annoDataList.append([["intergenic"], ["intergenic"],heteroDataList,numReads])
                continue
            else:
                key=key[0] #Need to modify this if there can be more than one annotation for a single pos, like multi genese from same DNA, ie dont use [0],. iterate over all the items in key[0]
   
            #print featureNum_to_Anno_DataDict[key]
            #print "spilling data here"
            #print pos, key
            #raw_input()
            annoData = featureNum_to_Anno_DataDict[key] 
            #print element
            #print annoData
            ranges = []
            for x in annoData[0]:
                for y in x:
                    ranges.append(int(y))
            ranges.sort()

            orientation = annoData[2]
            if orientation not in ["+","-"]:
                print [orientation]
                raw_input("annotation data format error, orientation is not a +/-")         
            seq = featureNum_to_seq_DataDict[key]
            
            #print "the SEQ for this anno:"
            #print annoData
            #print "is"
            #print seq
            #raw_input()
            
            #Next calculate the SNP effect: NTPos, NTChange, AAPos and AA change, Syn/NonSyn
            #Calculated:
            mutationData = []
            prevRef = ""
            prevMut = ""
            heteroDataList = []
            numReads = []
            for mapperData in element[2]:
                if mapperData == ["\t","\t","\t","\t"]:
                    mutationData.append(['','','','','','']) #debug entry 476891
                    heteroDataList.append("N/A")
                    numReads.append("N/A")
                    continue
                ref = mapperData[0]
                if ref == "\t":
                    print "WTWTGWQ", mapperData
                    raw_input("??????1234")
                mut = mapperData[1]    
                if mapperData <> ["\t","\t","\t","t"]:
                    temp = mapperData[3].split(":")
                    try:
                        heteroDataList.append(float(temp[1].split(",")[1])/float(temp[2]))
                        numReads.append(int(temp[2]))
                    except:
##                        print "error calculating heterogeneity:",element,mapperData,fileX
                        heteroDataList.append("error")
                        numReads.append("error")
                else:
                    heteroDataList.append("N/A")
                #numReads = temp[2]     
                annotationStartSite = ranges[0]-1 #lowest value 
                annotationEndSite = ranges[-1] #highest value 
                if ref == prevRef and mut == prevMut: # no need to recalculate, copy prev results
##                    print "using prev results"
##                    print ref, prevRef
##                    print mut, prevMut
##                    raw_input()
                    mutationData.append(mutationData[-1])
                   
                else:
                    mutationData.append(getSynorNonSynonMutation(mapperData,orientation,pos,ref,mut,annotationStartSite,annotationEndSite,seq,annoData))
##                    if "761155" in str(mutationData[-1]):
##                        print " test 2"
##                        print element
##                        print mutationData
##                        raw_input("--> 761155")
##                        raw_input("This is [changeType,aminoAcidChange,changedAminoAcidPosition,codonsRef,codonsMut,codonPosition]")
                    
                    prevRef = ref
                    prevMut = mut
            
##            print mutationData
            annoDataList.append([annoData, mutationData,heteroDataList,numReads]) # [snpPosition,ref,mut,"CDS_MUTATION",mutationData,annList[annPos+2]])
##            print "this is the annodatalist"
##            print annoDataList
##            raw_input()
                
                #calculatedAnnoData.append([NTPos,NTChange,AAPos,AAChange,SynNonSyn,refData,mutData,heteroData,numReads])
        ################################################################################################  
             
        return annoDataList

    def writeData(outputDir,fileX,overlapData,annotatedData,known_feature_properties,mapperAmount,mapperOrderList):
        '''
        overlapData:
        [4013, 'gi|444893469|emb|AL123456.3|', [['T', 'C', '1564.77', '1/1:0,40:40:99:1593,120,0\t'], ['T', 'C', '1681.77', '1/1:0,40:40:99:1710,120,0\t'], ['T', 'C', '1625.77', '1/1:0,40:40:99:1654,120,0\t']], '3', 'C/C/C']
        annotatedData:
        [[[['3281', '4437']], 'CDS', '+', [['pfam', '"Q59586" '], ['function', '"The RECF protein is involved in DNA metabolism and recombination; it is required for DNA replication andnormal sos inducibility. RECF binds preferentially tosingle-stranded, linear DNA. It also seems to bind ATP."'], ['product', '"DNA replication and repair protein RecF (single-strand DNA binding protein)"'], ['funcCat', '"information pathways" '], ['locus_tag', '"Rv0003" '], ['mass', '"42180.2" '], ['GO', '"GO:0003697,GO0005524,GO0005737,GO0006260,GO0006281,GO0 009432"']]], [['nonSyn', 'I:T', 245, 'atc', 'acc', 733], ['nonSyn', 'I:T', 245, 'atc', 'acc', 733], ['nonSyn', 'I:T', 245, 'atc', 'acc', 733]], [1.0, 1.0, 1.0]][pos],annotatedData[pos])
        now updated that the last ellement contains list of numreads mapped  
        '''
        #writes the updated line to contain annotation data
        os.chdir(outputDir)
        newFileName = fileX+"_ANNO.vcf"
        f = open(newFileName,'w')
        header = "pos\tchromosome\t"
        posX = 0
        for x in range(mapperAmount):
            header += mapperOrderList[posX]+"_ref\t"+mapperOrderList[posX]+"_mut\t"+mapperOrderList[posX]+"_qual\t"+mapperOrderList[posX]+"_info\t"+mapperOrderList[posX]+"_hetero_freq\t"+mapperOrderList[posX]+"_num_Reads\t"
            posX += 1
        header+="mapper_sum\tmut_per_mapper\tAnno_feature_range\tfeature_type\torientation\t"
        for x in known_feature_properties:
            header += x+"\t" 
        posX = 0
        for x in mapperOrderList:
            header += mapperOrderList[posX]+"_AA_change_type\t"+mapperOrderList[posX]+"_AA_change\t"+mapperOrderList[posX]+"_AA_Codon_Pos\t"+mapperOrderList[posX]+"_ref_codon\t"+mapperOrderList[posX]+"_mut_codon\t"+mapperOrderList[posX]+"_gene_pos\t"
            posX += 1
        header+="\n"
        headerData = header.replace("\n","").split("\t")
        
        f.write(header)
        #f.write("POS\tREF\tMUTATION BWA\tMUTATION NOVO\tMUTATION SMALT\tBWA_QUAL\tBWA_INFO\tBWA_EXTRA_INFO\tNOVO_QUAL\tNOVO_INFO\tNOVO_EXTRA_INFO\tSMALT_QUAL\tSMALT_INFO\tSMALT_EXTRA_INFO\tMAPPER_COUNT\tTYPE\tSYN/nonSyn\tAACHANGE\tAACODON\tREFCODON\tMUTATEDCODON\tNAME\tREGION\tLENGTH\tSTART\tEND\tORIENTATION\tPRODUCT\tFUNCTIONALCATAGORY\tFUNCTION\tDR\tPFAM\tGO\tMOLECULARMASS(Dalton)\tOPERON\tOPERON_START\tOPERON_END\tOPERON_ORIENTATION\tLENGTH\tOPERON\tOPERON_START\tOPERON_END\tOPERON_ORIENTATION\tLENGTH\n")
        
        for pos in range(len(overlapData)):
            #print pos
            #print overlapData[pos]
            #print "and-->"
            #print annotatedData[pos]
            #raw_input()
            #print overlapData[pos]
            #print annotatedData[pos]
            #store overlapData:
            chromosome = overlapData[pos][1]
            variant_pos = str(overlapData[pos][0])
            mapperData = overlapData[pos][2]  #iterate over this - use mapper heading to mark it
            msum = overlapData[pos][3]
            mutChanges = overlapData[pos][4]
            #store annotatedData:
            #print annotatedData[pos]
            #print annotatedData[pos] == ["intergenic"]
            #for x in range(len(annotatedData[pos])):
            #    print "----------BEGIN-------------------------"
            #    print "ANNO ELEMENT #", x
            #    print annotatedData[pos][x]
            #    print "----------END---------------------------"
            if annotatedData[pos][0] == ['intergenic']:
                anno_ranges = ""
                anno_type = "intergenic"
                orientation = ""
                anno_features = ""
                calculatedData = ""
                heteroData = annotatedData[pos][2]
                numReads = annotatedData[pos][3]
            else: #not intergenic
                anno_ranges = annotatedData[pos][0][0]
                anno_type = annotatedData[pos][0][1]
                orientation = annotatedData[pos][0][2]
                anno_features = annotatedData[pos][0][3] #sort this in same order as known_feature_properties which comes from the embl anno file, iterate over it to get consistent order
                calculatedData = annotatedData[pos][1] #iterate over this - use mapper heading to mark it
                heteroData = annotatedData[pos][2]
                numReads = annotatedData[pos][3]
            
            toWrite = [variant_pos,chromosome]
            posX = -1
            for x in mapperData: 
                posX += 1
                for y in x:
                    toWrite.append(y.replace("\t",""))
                toWrite.append(heteroData[posX])
                toWrite.append(numReads[posX])
            toWrite.append(msum)
            toWrite.append(mutChanges)
            toWrite.append(anno_ranges)
            toWrite.append(anno_type)
            toWrite.append(orientation)
            #Write the annotation features in consistent sorted order
            temp = {}
            
            #print "*"*10
            for x in anno_features:
                #print "this is one anno feature"
                #print x
                #raw_input("ok?")
                temp[x[0]] = x[1].replace("\t","")
            for known_feature in known_feature_properties:
                if known_feature in temp:
                    toWrite.append(temp[known_feature])
                else:
                    toWrite.append(" ")
            for x in calculatedData:
                for y in x:
                    toWrite.append(y)
            #posX = 0
            dataString = ""
            toRemove = ['\r','"',"'","\n"]
            for x in toWrite:
                #print headerData[posX], "=", [x]
                #posX +=1
                tempString = str(x)
                for d in toRemove:
                    tempString=tempString.replace(d,'')
                f.write(tempString+"\t") #.replace(" +","&")+'\t')
                dataString+=(str(x)+"\t")
            
            f.write('\n')
            dataString+="\n"
            #print anno_features
            #print dataString
            #print [dataString]
            #raw_input("data written to flie")
        f.close()
        return

    def loadVariantData(fileX,filesToAnno,VCF_Location_List_File_List):
        #load the variants for each mapper into 3 lists, or as many mappers as are present
        fileVariantData = [] 
        for element in VCF_Location_List_File_List:
            VCF_Location = element[0]
            vcf_files = element[1]
            os.chdir(VCF_Location)
            tempDataDict = {}
            for vcfFile in vcf_files:
                if vcfFile.split("_")[0] == fileX: #.split("_")[0]: #match
                    f = open(vcfFile,'r')
                    for line in f:
                        if line[0] <> "#":
                            temp = line.split("\t")
                            tempDataDict[temp[1]] = line
                    f.close()
                    break
                continue
            fileVariantData.append(tempDataDict) #data for mapper 1,2,3 like this : [[lines1-N], [lines 1-N], [lines 1-N]] excluding the header of course
        return fileVariantData #returns the data from each mapper for this file in the samee order as in VCF_Location_List_File_List

        #########################################

    def annotateMain(EMBLDIR,annotationFile,outDir,VCF_Location_List,mapperOrderList,debugMode):
        #########################################
        print "Obtaning file list to annotate..."
        filesToAnno = obtain_file_list(VCF_Location_List) #{}
        print "Loading reference..."
        pos_to_feature_num_dict, featureNum_to_Anno_DataDict, featureNum_to_seq_DataDict, known_feature_properties = load_embl_feature_coordinates(annotationFile,EMBLDIR) 
        print "Reference loaded into system memory"
        if debugMode:
            print [known_feature_properties]
            raw_input()
        #Test to see if I can retreve the correct data for a snp position
        #testSNPpos = 897209
        #print pos_to_feature_num_dict[testSNPpos]
        #for x in pos_to_feature_num_dict[testSNPpos]:
        #    print featureNum_to_Anno_DataDict[x]
        #    print featureNum_to_seq_DataDict[x]
        #raw_input("DONE")
        
        VCF_Location_List_File_List = []
        for VCF_Location in VCF_Location_List:
            VCF_Location_List_File_List.append([VCF_Location,[f for f in os.listdir(VCF_Location) if f.endswith('.vcf')]]) #[BWA,vcfs], [NOVO, vcfs], [smalt. vcfs]
            
            VCF_Location_List_File_List.sort() 
        #print VCF_Location_List_File_List
        #raw_input()     
        
        if True:
            if filesToAnno == {}:
                raw_input("Error, no files found to annotate")
            for fileX in filesToAnno:
##                if "E-113" not in fileX:
##                    raw_input("DEBUG Forcing skip 77777")
##                    continue
                print "Annotating file: ",fileX,"..."
                startTime = time.time()
                fileVariantData = loadVariantData(fileX,filesToAnno,VCF_Location_List_File_List) #Return vcf data lists for each mapper 
                overlapData, mapperAmount = getMapperOverlap(fileVariantData) # [ pos, chrom, [ref,alt,qual,info,info],[ref,alt,qual,info,info],[ref,alt,qual,info,info] , mutations]
                print "annotating:",fileX          
                annotatedData  = getAnnoData(fileX, overlapData, mapperAmount, known_feature_properties, pos_to_feature_num_dict, featureNum_to_Anno_DataDict, featureNum_to_seq_DataDict)
                #A list consisting of lists of [annoData, mutationData,heteroDataList,numReads]
                print "annotation time:",round((time.time() - startTime),2), "seconds"
                print "compressing variant data"
                #writeData(outDir,fileX,overlapData,annotatedData,known_feature_properties,mapperAmount,mapperOrderList)
                #raw_input("rename it so than can write another...")
                annoData = recalculate_AA_changes(overlapData,annotatedData)
                print "Writing data to file..."
                writeData(outDir,fileX,overlapData,annoData,known_feature_properties,mapperAmount,mapperOrderList)
        print "annotation complete."
        return pos_to_feature_num_dict, featureNum_to_Anno_DataDict, known_feature_properties 
    
    pos_to_feature_num_dict, featureNum_to_Anno_DataDict, known_feature_properties  = annotateMain(emblDir, annotationFile, outputDir, VCF_Location_List,mapperOrderList,debugMode)        
    return pos_to_feature_num_dict, featureNum_to_Anno_DataDict, known_feature_properties 

#########################################################################################
#########################################################################################
#########################################################################################
#########################################################################################
                    #End of automated annotation
#########################################################################################
#########################################################################################
#########################################################################################

      
def getFastaName(inputFileName):
    fileExt = inputFileName.split(".")[-1]
    return inputFileName[:len(inputFileName)-len(fileExt)-1]

def fastQCSH():
    fileArray = []
    totalLoaded = 0
    os.chdir(params.outputDir)  
    try:
        os.makedirs(params.fastQCStatsDir)
    except:
        print "Directory",params.fastQCStatsDir,"already exists, proceeding"
        
    for fileX in os.listdir(params.fastQ):    
        if fileX.endswith(".fastq") or fileX.endswith(".fastq.gz"):
            totalLoaded +=1
            print "reading file: ",fileX
            if fileX.endswith(".fastq.gz") and params.trimMethod == "Fixed_Amount_Trim":
                print "ERROR: This trimming method can only be used on uncompressed FASTQ files, a .gz file extention was detected, please uncompress the files and try again"
                return False
            fileArray.append(str(fileX))              
    print "A total of ",totalLoaded,"FASTQ files were loaded"
    fileArray.sort()
    
    totalShForFastQC = "FastQC.sh"
    os.chdir(params.scripts_trimming)
    f = open(str(totalShForFastQC),'w')
    f.write(params.fastQCPath+" --nogroup -t "+str(params.cores)+" ")
    for x in fileArray:
        f.write(params.fastQ+x)
        f.write(' ')
    f.write('-o '+params.fastQCStatsDir)
    
    print "FastQC script created as",totalShForFastQC
    f.close()
    return True
    ################################################






###############################################################################
#MAPPER SCRIPTS
###############################################################################

def partitionFastQList(fastQDir):
    data = []
    def sampleName(fileX):
        return fileX.split("_")[0]

    os.chdir(fastQDir)
    for fileX in os.listdir(fastQDir):
        if fileX.endswith(".fastq") or fileX.endswith("fastq.gz"):
            data.append(fileX)
    data.sort()
    singleData = []
    pairedData = []
    previous = ''
    pos = 0
    flag = True
    while pos <= len(data)-1:
        if pos == 0 or flag:
            previous = data[pos]
            if pos == len(data)-1:
                singleData.append(previous.replace("\n",""))
            pos+=1
            flag = False
            continue

        current = data[pos]
        if sampleName(current) == sampleName(previous):
            pairedData.append(previous.replace("\n",''))
            pairedData.append(current.replace("\n",''))
            pos+=1
            flag=True
            continue
        else:
            if pos == len(data) -1:
                singleData.append(previous.replace("\n",''))
                singleData.append(current.replace("\n",''))
                pos+=2
                continue
            else:
                singleData.append(previous.replace("\n",''))
                flag=True
                continue
    return [singleData, pairedData] 

##def loadReferenceFasta(directoryX):
##    #searches for the file name of the reference .fasta file in the directory given
##    #returns the file name (string)
##    #find the ref name
##    refName = None
##    
##    os.chdir(directoryX)
##    countX = 0
##    for fileX in glob.glob('*.fasta'):
##        countX +=1 
##        refName = fileX
##        #print "using reference file: ",refName
##    if countX > 1:
##        print "you have more than one .fasta reference HERE - change the python file - more or less at the end file in ", directoryX
##        print "please manually exit using control+C and try again"
##        raw_input()
##        countX = 0
##        for fileX in glob.glob('*.fasta'):
##            countX +=1 
##            refName = fileX
##            print "using refence file: ",refName
##        if countX > 1:
##            print "you have more than one .fasta reference file in ", directoryX
##            print "please manually exit using control+C and try again"
##            raw_input()
##    if refName == None:
##        print "error loading reference fasta file from the specified directory"
##        raw_input()
##    #print "using "+directoryX+refName+" as reference."
##    return refName


def partitionFileList(fileArray, trimArray ,ID ,SM, bins):
    '''
    this splits a list of files into 5 groups,
    the output is a list of lists which contains 5 lists
    the last one contains all the extras which did not fit into the division of 5
    so will have up to 4 more at times
    if there are less than 5 files, all are added to the first list
    so in other words whatever_List[0]
    '''
    lenX =len(fileArray)
    print "splitting ", lenX,"files up into ",bins,"lists..."
    splitList = [] #the file array into 5
    splitTrimArray =[]
    splitID = []
    splitSM = []
    
    for x in range(bins):
        splitList.append([])  #has 5 sublists
        splitTrimArray.append([])
        splitID.append([])
        splitSM.append([]) 

    roughAmount = lenX/5
    print "dividing into ",roughAmount,"`s"
    pos = -1
    for x in range(bins):
        for y in range(roughAmount):
            pos += 1
            splitList[x].append(fileArray[pos])
            splitTrimArray[x].append(trimArray[pos])
            splitID[x].append(ID[pos])
            splitSM[x].append(SM[pos])
##            print listX[pos]
    print "final position: ", pos
    rest = lenX-pos-1
    print "need more", rest

    for x in range(rest):
        pos+=1
        splitList[0].append(fileArray[pos])
        splitTrimArray[0].append(trimArray[pos])
        splitID[0].append(ID[pos])
        splitSM[0].append(SM[pos])
    print "rest went up to pos ", pos
        
    return splitList, splitTrimArray, splitID, splitSM

##################################################################
#2 Find trim cutoff and write to sh file to perform trimming to new dir: /params.trimmedFastQ/
# Looks for trim data in fastQCStats dir
#output the trimming commangs to autrim.sh 

def tidyName2(name,trim):
    #takes filename - outputs trim file name
    sampleName = ""
    for char in name:
        if char == "_":
            break
        else:
            sampleName += char              
    if "R1" in name[2:] or "Read1" in name[2:]:
        sampleName+="_R1"
    elif "R2" in name[2:] or "Read2" in name[2:]:
        sampleName +="_R2"
    else:
        print "ERROR RENAMING FILES, R1 or R2 not in fileNAME, fix and restart"
        raw_input()
    return sampleName+'_trim'+'.fastq'   #str(trim)+'.fastq'

def tidyNameX(name,trim):
    #takes filename - outputs trim file name
    #find location of "pool_"
    sampleName = ""
    count = 0
    for char in name:
        if char == "_":
            count+=1
        if count == 3:
            if char <> "_":
                sampleName+=char
        elif count>3:
            break
        
    if "R1" in name:
        sampleName+="_R1"
    elif "R2" in name:
        sampleName +="_R2"
    else:
        print "ERROR RENAMING FILES, R1 or R2 not in fileNAME, fix and restart"
        raw_input()
    return sampleName+'_trim'+'.fastq' #+str(trim)+'.fastq'

def tidyName(name,trim):
    pos = 0
    copyPos = 0
    newName = ''
    print name
    while pos < len(name) and (str.lower(name[pos]) <> "p" or (str.lower(name[pos]) <> "_" and str.lower(name[pos+1]) <> "_")):
        pos +=1
       # print pos
       # print name[pos]
        if pos+6 < len(name):
            if str.lower(name[pos]) == "_" and name[pos+1] =="_":
               # print "the file used __"
               # raw_input()
                temp = pos+2
                while name[temp] <> "_" and temp < len(name):
                    temp+=1
            
                copyPos = temp
                break
            if str.lower(name[pos]) == 'p' and name[pos+5] == '_':
                copyPos = pos+5
                break
            elif str.lower(name[pos]) == 'p' and name[pos+6] == '_':
                copyPos = pos+6
                break
        else:
            print "file handling error, using long name instead"
           # raw_input()
            return name +'_trim'+'.fastq'  #str(trim)+'.fastq'
    newName = name[copyPos+1:(len(name)-6)]

    newName = newName +'_trim'+'.fastq' #str(trim)+'.fastq'
    return newName


########################################################################
#to combine the names of read1 and read 2 into one file name#
########################################################################

def extractFileNameData(fastQFileArray):
    '''
    input: Sorted list of fastq files
    output: List of identifiers needed to add readgroups to bam files
    lits are ID, SM and LB corresponding to each fastq file.
    Assumes file names are in the format: SAMENAME_POOL_LIBRARY_R1.fastq
    If not - pool and library info gets atrificially generated using sample name
    '''
    ID = []
    SM = []
    LB = []
    barcodes = []
    skipFullFileNameData = False
    for fileName in fastQFileArray:
        tempID = ""
        tempSM = "12345"
        tempLB = "unknown"
        if "_" not in fileName:
            print "Filename error, files must be named using the convention of SampleIdentifier_poolIdentifier_LibraryIdentifier_ReadIdentifier.fastq"
            print "for example: 'R1234_Pool4_Library24_R1.fastq'"
            print "or with optional additional barcode info: R1234_Pool4_Library24_TCATTC_R1.fastq"
            print "Please rename all files using this naming convention and re-run USAP."
            print "the '_' separator was missing from file:", fileName
            raw_input("Press enter to exit the program.")
            exit()
        fileNameData = fileName.split("_")
        try:
            if fileNameData[3] <> "R1" and fileNameData[3] <> "R2":
                barcodes.append(fileNameData[3])
        except:
            barcodes.append("")
            
        try:
            tempID = fileNameData[0]
            tempSM = fileNameData[1]
            tempLB = fileNameData[2]
            
        except:
            print fileNameData
            print "Filename error, files must be named using the convention of SampleIdentifier_poolIdentifier_LibraryIdentifier_ReadIdentifier.fastq"
            print "for example: R1234_Pool4_Library24_R1.fastq"
            print "or with optional additional barcode info: R1234_Pool4_Library24_TCATTC_R1.fastq"
            print "Please rename all files using this naming convention where possible and re-run USAP."
            ans = ""
            if not skipFullFileNameData:
                while ans not in ["Y","y","n","N","A","a"]:
                    ans = raw_input("Would you like to continue anyway ? Y/N/A (not reccomended, SM '12345' and library 'unknown' will be used for readgroup info)")
                    if ans == "A" or ans == "a":
                        skipFullFileNameData = True 
                    if ans.upper() == "Y" or ans.upper == "A" :
                        tempID = fileNameData[0]
                        tempSM = "12345"
                        tempLB = "unknown"
                    elif ans.upper() == "N":
                        raw_input("Press enter to exit the program.")
                        exit()
            else:
                tempID = fileNameData[0]
                tempSM = "12345"
                tempLB = "unknown"

        if tempLB == "R1" or tempLB == "R2" or ".fastq" in tempLB:
            tempLB = "unknown"
        if tempSM == "R1" or tempSM == "R2" or ".fastq" in tempLB:
            tempSM = "12345"
##        print "Read group info extraction analysis:"
##        print fileName
##        print "ID:",tempID
##        print "SM:",tempSM
##        print "LB:",tempLB
##        raw_input()
        ID.append(tempID)
        SM.append(tempSM)
        LB.append(tempLB)
    ###############################################################
    trimArray = []
    if params.trimMethod == "Fixed_Amount_Trim":
        for fileT in fastQFileArray:
            pos = 8 #this many characters from the right
            trimAmount = []
            try:
##                while fileT[len(fileT)-pos] not in str(range(10)):
##                    pos-=1
                while fileT[len(fileT)-pos] <> ".":
                    trimAmount.append(fileT[len(fileT)-pos])
                    pos-=1
                j = ""
                j=j.join(trimAmount)
                trimArray.append(j)
            except:
                trimArray.append(str(0))
    return ID,SM,LB,barcodes,trimArray
    
 
def BWAAlign_combine(trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE):    
    '''
    cleanedUpFileData = partitionFastQList(params.trimmedFastQ)

    fileArraySE = cleanedUpFileData[0]
    fileArrayPE = cleanedUpFileData[1]
    
    IDSE, SMSE, LBSE, barcodesSE, trimArraySE = extractFileNameData(fileArraySE) #change dirX to params.trimmedFastQ
    IDPE, SMPE, LBPE, barcodesPE, trimArrayPE = extractFileNameData(fileArrayPE) #change dirX to params.trimmedFastQ
    '''
    fileArraySE = trimmedFilesSE
    fileArrayPE = trimmedFilesPE 
    #reads can be either "pairedEnd",  "singleEnd", "mixed"  
    if params.reads == "singleEnd":
        fileArray = trimmedFilesSE
        ID = IDSE
        SM = SMSE
        LB
    if params.reads == "pairedEnd":
        fileArray = trimmedFilesPE
        ID = IDPE
        SM = SMPE
    if params.reads == "mixed":
        fileArray = trimmedFilesSE+trimmedFilesPE
        ID = IDSE + IDPE
        SM = SMSE + SMPE
    
    os.chdir(params.scripts_BWA)
    BWA = "1_BWAAlign.sh"
    BWA_Cleanup = "1_BWAAlign_cleanup.sh"
    f = open(BWA,'w')
    f2 = open(BWA_Cleanup,'w')

    carryOverList_new = []
    
    #updated for multi mode
    #Updated for multi ref fasta

    for referenceBWA in params.fastaList:
        referenceBWA_shortName = getFastaName(referenceBWA) 
        for pos in range(len(fileArray)): #fileName in fileArray:
##            f.write(params.bwa+' aln -t '+str(params.coreSplit[0])+' '+params.reference+referenceBWA+' '+params.trimmedFastQ+fileArray[pos]+" > "+params.BWAAligned_aln+fileArray[pos][:-6]+"_bwa.sai")
            f.write(params.bwa+' aln -t '+str(params.coreSplit[0])+' '+params.reference+referenceBWA+' '+params.trimmedFastQ+fileArray[pos]+" > "+params.BWAAligned_aln+fileArray[pos][:-9]+"_bwa.sai")
            f.write('\n')
            f2.write("rm "+params.BWAAligned_aln+fileArray[pos][:-9]+"_bwa.sai\n")
            carryOverList_new.append(fileArray[pos][:-9]+"_bwa.sai")
    f.close()
    f2.close()
   
    combineReads = "2_combineReads.sh"
    combineReads_Cleanup = "2_combineReads_Cleanup.sh"
    os.chdir(params.scripts_BWA)
    f2 = open(combineReads,'w')
    f3 = open(combineReads_Cleanup,'w')

##    referenceBWA = loadReferenceFasta(params.bwaRef)

    if params.reads == "singleEnd" or params.reads == "mixed":
##        print params.fastaList
##        raw_input("OK?")
##        for referenceBWA in params.fastaList:
##            print referenceBWA
##            raw_input()
        pos = 0
        for pos in range(len(fileArraySE)):
            readGroup = "'@RG\\tID:"+IDSE[pos]+"\\tSM:"+SMSE[pos]+"\\tLB:"+LBSE[pos]+"\\tPL:Illumina'"
##            f2.write(params.bwa+" samse -r '@RG\\tID:"+IDSE[pos]+"\\tSM:"+SMSE[pos]+"\\tPL:Illumina' "+params.reference+referenceBWA+" "+params.BWAAligned_aln+carryOverList_new[pos]+" "+params.trimmedFastQ+fileArraySE[pos]+" > "+params.BWAAligned_aln+IDSE[pos]+"_"+referenceBWA+"_bwa.sam")
            f2.write(params.bwa+" samse -r "+readGroup+" "+params.reference+referenceBWA+" "+params.BWAAligned_aln+carryOverList_new[pos]+" "+params.trimmedFastQ+fileArraySE[pos]+" > "+params.BWAAligned_aln+IDSE[pos]+"_bwa.sam") 
            f2.write('\n')
            f3.write("rm "+params.BWAAligned_aln+IDSE[pos]+"_bwa.sam\n")
    
    if params.reads == "pairedEnd" or params.reads == "mixed":
        for referenceBWA in params.fastaList:
            referenceBWA_shortName = getFastaName(referenceBWA)
            pos = 0
            while pos <= len(fileArrayPE)-2:
                if IDPE[pos] <> IDPE[pos+1]:
                    print "error is ", IDPE[pos], "and ", IDPE[pos+1]
                    print "bwa sampe error - the two files are not matching - cannot proceed - please manually quit and fix the error"
                    raw_input()
                    exit
                readGroup = "'@RG\\tID:"+IDPE[pos]+"\\tSM:"+SMPE[pos]+"\\tLB:"+LBPE[pos]+"\\tPL:Illumina'"
##                f2.write(params.bwa+" sampe -r '@RG\\tID:"+IDPE[pos]+"\\tSM:"+SMPE[pos]+"\\tPL:Illumina' "+params.reference+referenceBWA+" "+params.BWAAligned_aln+carryOverList_new[pos+len(fileArraySE)]+" "+params.BWAAligned_aln+carryOverList_new[pos+1+len(fileArraySE)]+" "+params.trimmedFastQ+fileArrayPE[pos]+" "+params.trimmedFastQ+fileArrayPE[pos+1]+" > "+params.BWAAligned_aln+IDPE[pos]+"_"+referenceBWA_shortName+"_"+"trim_bwa.sam")
                f2.write(params.bwa+" sampe -r "+readGroup+" "+params.reference+referenceBWA+" "+params.BWAAligned_aln+carryOverList_new[pos+len(fileArraySE)]+" "+params.BWAAligned_aln+carryOverList_new[pos+1+len(fileArraySE)]+" "+params.trimmedFastQ+fileArrayPE[pos]+" "+params.trimmedFastQ+fileArrayPE[pos+1]+" > "+params.BWAAligned_aln+IDPE[pos]+"_bwa.sam")           
                f2.write('\n')
                f3.write("rm "+params.BWAAligned_aln+IDPE[pos]+"_bwa.sam\n")
                pos +=2

    f2.close()
    f3.close()
    #print "combine list for 'bwa sampe' creates as: ",combineReads
    return


       
#############################################################################
# NOVO ALIGNMENT
# 

'''This program collects the names of all the trimemd fastq files and creates a
new .sh file to automate NOVO alignment of all files '''


def NOVOAlign(trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE):
    '''
    cleanedUpFileData = partitionFastQList(params.trimmedFastQ)
    fileArraySE = cleanedUpFileData[0]
    fileArrayPE = cleanedUpFileData[1]

    IDSE, SMSE, LBSE, barcodesSE, trimArraySE = extractFileNameData(fileArraySE) #change dirX to params.trimmedFastQ
    IDPE, SMPE, LBPE, barcodesPE, trimArrayPE = extractFileNameData(fileArrayPE) 
    '''
    fileArraySE = trimmedFilesSE
    fileArrayPE = trimmedFilesPE
    
    if params.reads == "singleEnd":
        #fileArray , trimArray , ID, SM  = GenerateCleanFileNamesSEMode(params.trimmedFastQ)
        fileArray = trimmedFilesSE
##        trimArray = trimmedFilesPE
        ID = IDSE
        SM = SMSE
    if params.reads == "pairedEnd":
        #fileArray , trimArray , ID, SM = GenerateCleanFileNames(params.trimmedFastQ)
        fileArray = trimmedFilesPE
##        trimArray = trimArrayPE
        ID = IDPE
        SM = SMPE
    if params.reads == "mixed":
        fileArray = trimmedFilesSE+trimmedFilesPE
##        trimArray = trimArraySE+trimArrayPE
        ID = IDSE + IDPE
        SM = SMSE + SMPE

    #fileArray , trimArray , ID, SM = GenerateCleanFileNames(params.trimmedFastQ)

    os.chdir(params.scripts_NOVO)
    NOVO = "1_1_NOVOAlign.sh"
    NOVO_Cleanup = "1_1_NOVOAlign_cleanup.sh"

    pos = 0

    f = open(NOVO,'w')
    f2 = open(NOVO_Cleanup,'w')
    if fileArray == []:
        print "the were no fastq files found in ", params.trimmedFastQ," fix path and run again"
        raw_input()
        return
    if params.reads == "singleEnd":
        for referenceNOVO in params.fastaList:
            referenceNOVO_shortName = getFastaName(referenceNOVO)
            for pos in range(len(fileArraySE)):
##                f.write(params.novoalign+" -d "+params.reference+referenceNOVO_shortName+".ndx"+" -f "+params.trimmedFastQ+fileArraySE[pos]+" -o SAM '@RG\tID:"+IDSE[pos]+"\tSM:"+SMSE[pos]+"\tPL:Illumina' 2> "+params.NOVOAligned_aln+IDSE[pos]+"_"+"trim"+"_"+trimArraySE[pos]+"_novo_stats.novodist > "+params.NOVOAligned_aln+IDSE[pos]+"_"+referenceNOVO_shortName+"_"+"trim_novo.sam")
                readGroup = "'@RG\tID:"+IDSE[pos]+"\tSM:"+SMSE[pos]+"\tLB:"+LBSE[pos]+"\tPL:Illumina'"
                f.write("gunzip -c "+params.trimmedFastQ+fileArraySE[pos]+" > "+params.trimmedFastQ+fileArraySE[pos][:-3]+" && "+params.novoalign+" -d "+params.reference+referenceNOVO_shortName+".ndx"+" -f "+params.trimmedFastQ+fileArraySE[pos][:-3]+" -o SAM "+readGroup+" 2> "+params.NOVOAligned_aln+IDSE[pos]+"_"+trimArraySE[pos]+"_novo_stats.novodist > "+params.NOVOAligned_aln+IDSE[pos]+"_"+"trim_novo.sam")
                f.write(" && rm "+params.trimmedFastQ+fileArraySE[pos][:-3]+"\n")
                f2.write("rm "+params.NOVOAligned_aln+IDSE[pos]+"_"+"trim_novo.sam\n")
    
    if params.reads == "pairedEnd":
        for referenceNOVO in params.fastaList:
            pos = 0
            referenceNOVO_shortName = getFastaName(referenceNOVO)
            while pos <= len(fileArray)-2:
##                f.write(params.novoalign+" -d "+params.reference+referenceNOVO_shortName+".ndx"+" -f "+params.trimmedFastQ+fileArray[pos]+" "+params.trimmedFastQ+fileArray[pos+1]+" -o SAM '@RG\tID:"+ID[pos]+"\tSM:"+SM[pos]+"\tPL:Illumina' 2> "+params.NOVOAligned_aln+ID[pos]+"_"+"trim_novo_stats.novodist > "+params.NOVOAligned_aln+ID[pos]+"_"+referenceNOVO_shortName+"_"+"trim_novo.sam")
                readGroup = "'@RG\tID:"+IDPE[pos]+"\tSM:"+SMPE[pos]+"\tLB:"+LBPE[pos]+"\tPL:Illumina'"
                f.write("gunzip -c "+params.trimmedFastQ+fileArrayPE[pos]+" > "+params.trimmedFastQ+fileArrayPE[pos][:-3]+" && ")
                f.write("gunzip -c "+params.trimmedFastQ+fileArrayPE[pos+1]+" > "+params.trimmedFastQ+fileArrayPE[pos+1][:-3]+" && ")
                f.write(params.novoalign+" -d "+params.reference+referenceNOVO_shortName+".ndx"+" -f "+params.trimmedFastQ+fileArray[pos][:-3]+" "+params.trimmedFastQ+fileArray[pos+1][:-3]+" -o SAM "+readGroup+"+ 2> "+params.NOVOAligned_aln+ID[pos]+"_"+"trim_novo_stats.novodist > "+params.NOVOAligned_aln+ID[pos]+"_"+"trim_novo.sam")
                f.write(" && rm "+params.trimmedFastQ+fileArrayPE[pos][:-3])
                f.write(" && rm "+params.trimmedFastQ+fileArrayPE[pos+1][:-3]+"\n")
                f.write('\n')
                f2.write("rm "+params.NOVOAligned_aln+ID[pos]+"_"+"trim_novo.sam\n")
                pos += 2
                
    if params.reads == "mixed":
        for referenceNOVO in params.fastaList:
            pos = 0
            referenceNOVO_shortName = getFastaName(referenceNOVO)
            for pos in range(len(fileArraySE)):
                readGroup = "'@RG\tID:"+IDSE[pos]+"\tSM:"+SMSE[pos]+"\tLB:"+LBSE[pos]+"\tPL:Illumina'"
##                f.write(params.novoalign+" -d "+params.reference+referenceNOVO_shortName+".ndx"+" -f "+params.trimmedFastQ+fileArraySE[pos]+" -o SAM '@RG\tID:"+IDSE[pos]+"\tSM:"+SMSE[pos]+"\tPL:Illumina' 2> "+params.NOVOAligned_aln+IDSE[pos]+"_"+"trim"+"_novo_stats.novodist > "+params.NOVOAligned_aln+IDSE[pos]+"_"+referenceNOVO_shortName+"_"+"trim_novo.sam")
                f.write("gunzip -c "+params.trimmedFastQ+fileArraySE[pos]+" > "+params.trimmedFastQ+fileArraySE[pos][:-3]+" && ")
                f.write(params.novoalign+" -d "+params.reference+referenceNOVO_shortName+".ndx"+" -f "+params.trimmedFastQ+fileArraySE[pos][:-3]+" -o SAM "+readGroup+" 2> "+params.NOVOAligned_aln+IDSE[pos]+"_"+"trim"+"_novo_stats.novodist > "+params.NOVOAligned_aln+IDSE[pos]+"_"+"trim_novo.sam")
                f.write(" && rm "+params.trimmedFastQ+fileArraySE[pos][:-3]+"\n")
                f2.write("rm "+params.NOVOAligned_aln+IDSE[pos]+"_"+"trim_novo.sam\n")
        for referenceNOVO in params.fastaList:
            referenceNOVO_shortName = getFastaName(referenceNOVO)    
            pos = 0
            while pos <= len(fileArrayPE)-2:
                readGroup = "'@RG\tID:"+IDPE[pos]+"\tSM:"+SMPE[pos]+"\tLB:"+LBPE[pos]+"\tPL:Illumina'"
                #f.write(params.novoalign+" -d "+params.refNovo+referenceNOVO[0:-6]+".ndx"+" -f "+params.trimmedFastQ+fileArrayPE[pos]+" "+params.trimmedFastQ+fileArrayPE[pos+1]+" -o SAM '@RG\tID:"+IDPE[pos]+"\tSM:"+SMPE[pos]+"\tPL:Illumina' 2> "+params.NOVOAligned_aln+IDPE[pos]+"_"+"trim"+"_"+trimArrayPE[pos]+"and"+trimArrayPE[pos+1]+"_novo_stats.novodist > "+params.NOVOAligned_aln+IDPE[pos]+"_"+"trim_novo.sam")
##                f.write(params.novoalign+" -d "+params.reference+referenceNOVO_shortName+".ndx"+" -f "+params.trimmedFastQ+fileArrayPE[pos]+" "+params.trimmedFastQ+fileArrayPE[pos+1]+" -o SAM '@RG\tID:"+IDPE[pos]+"\tSM:"+SMPE[pos]+"\tPL:Illumina' 2> "+params.NOVOAligned_aln+IDPE[pos]+"_"+"trim"+"_novo_stats.novodist > "+params.NOVOAligned_aln+IDPE[pos]+"_"+referenceNOVO_shortName+"_"+"trim_novo.sam")
                f.write("gunzip -c "+params.trimmedFastQ+fileArrayPE[pos]+" > "+params.trimmedFastQ+fileArrayPE[pos][:-3]+" && ")
                f.write("gunzip -c "+params.trimmedFastQ+fileArrayPE[pos+1]+" > "+params.trimmedFastQ+fileArrayPE[pos+1][:-3]+" && ")
                f.write(params.novoalign+" -d "+params.reference+referenceNOVO_shortName+".ndx"+" -f "+params.trimmedFastQ+fileArrayPE[pos][:-3]+" "+params.trimmedFastQ+fileArrayPE[pos+1][:-3]+" -o SAM "+readGroup+" 2> "+params.NOVOAligned_aln+IDPE[pos]+"_"+"trim"+"_novo_stats.novodist > "+params.NOVOAligned_aln+IDPE[pos]+"_"+"trim_novo.sam")
                f.write(" && rm "+params.trimmedFastQ+fileArrayPE[pos][:-3])
                f.write(" && rm "+params.trimmedFastQ+fileArrayPE[pos+1][:-3]+"\n")
                f2.write("rm "+params.NOVOAligned_aln+IDPE[pos]+"_"+"trim_novo.sam\n")
                pos += 2
                
    #print "novo alignment .sh list file created as: ",params.scripts_NOVO+NOVO
    f.close()
    f2.close()
    return

def NOVOAlignMulti():
    #splits the sh file from novoalign into several files
    #since the new novo align does not support parralelizing anymore by specifying the amount of threads, instead you need to run more instances at the same time.
    #thus if you have 4 cores, run 4 novoalgns.

    os.chdir(params.scripts_NOVO)
    NOVO = open("1_1_NOVOAlign.sh",'r')
    data = []
    pos = 0
    flag = False
    for line in NOVO:
        line = line.replace("\n","")
        if pos == int(params.cores)-1:
            pos = 0
            data.append(line+" &\n")
            data.append("wait\n")
            flag = True
        else:
            data.append(line+" &\n")
        pos+=1
    if not flag:
        data.append("wait\n")
    NOVO.close()

    NOVO = open("1_2_NOVOAlign_multi.sh",'w')
    for x in data:
        NOVO.write(x)
    NOVO.close()
    
##    for x in range(int(params.cores)):
##        if splitList[x] <> []:
##            NOVO.write("sh ./Scripts/NOVO/1_3_NOVOAlign"+str(x+1)+".sh &\n")
##    NOVO.write("wait\n")
##    NOVO.close()
##    
##    for x in range(int(params.coreSplit[1])):
##        if splitList[x] == []:
##            break
##        NOVO = open("NOVOAlign"+str(x+1)+".sh",'w')
##        for line in splitList[x]:
##            NOVO.write(line)
##        NOVO.close()
    return
        
##############################################################################
# SMALT ALIGNMENT
# 

'''This program collects the names of all the trimemd fastq files and creates a
new .sh file to automate SMALT alignment of all files '''

def SMALTAlign(trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE):
    '''
    cleanedUpFileData = partitionFastQList(params.trimmedFastQ)
    fileArraySE = cleanedUpFileData[0]
    fileArrayPE = cleanedUpFileData[1]

    IDSE, SMSE, LBSE, barcodesSE, trimArraySE = extractFileNameData(fileArraySE) #change dirX to params.trimmedFastQ
    IDPE, SMPE, LBPE, barcodesPE, trimArrayPE = extractFileNameData(fileArrayPE) 
    '''
    fileArraySE = trimmedFilesSE
    fileArrayPE = trimmedFilesPE
    
    if params.reads == "singleEnd":
        #fileArray , trimArray , ID, SM  = GenerateCleanFileNamesSEMode(params.trimmedFastQ)
        fileArray = trimmedFilesSE
##        trimArray = trimArraySE
        ID = IDSE
        SM = SMSE
        LB = LBSE
    if params.reads == "pairedEnd":
        #fileArray , trimArray , ID, SM = GenerateCleanFileNames(params.trimmedFastQ)
        fileArray = trimmedFilesPE
##        trimArray = trimArrayPE
        ID = IDPE
        SM = SMPE
        LB = LBPE
    if params.reads =="mixed":
        fileArray = trimmedFilesSE+trimmedFilesPE
##        trimArray = trimArraySE+trimArrayPE
        ID = IDSE + IDPE
        SM = SMSE + SMPE
        LB = LBSE + LBPE

    os.chdir(params.scripts_SMALT)
    SMALT = "1_SMALTAlign.sh"
    SMALT_cleanup = "1_SMALTAlign_cleanup.sh"
    carryOverList_old = []
    pos = 0
    f = open(SMALT,'w')
    f2 = open(SMALT_cleanup,'w')

    if fileArray == []:
        print "there were no fastq files found in ", params.trimmedFastQ," fix path and run again"
        raw_input()
        return
    if params.reads == "singleEnd":
        for referenceSMALT in params.fastaList:
            if referenceSMALT.endswith(".fasta"):
                referenceSMALT = referenceSMALT[:-6]
            if referenceSMALT.endswith(".fa"):
                referenceSMALT = referenceSMALT[:-3]  
            referenceSMALT_shortName = getFastaName(referenceSMALT)
            for pos in range(len(fileArraySE)):
##                f.write(params.smaltBinary+" map -f sam -o "+params.SMALTAligned_aln+IDSE[pos]+"_"+referenceSMALT_shortName+"_"+"trim_smalt.sam "+params.reference+referenceSMALT+" "+params.trimmedFastQ+fileArraySE[pos])
                f.write(params.smaltBinary+" map -x -f sam -o "+params.SMALTAligned_aln+IDSE[pos]+"_"+"trim_smalt.sam "+params.reference+referenceSMALT+" "+params.trimmedFastQ+fileArraySE[pos])
                carryOverList_old.append(IDSE[pos]+"_"+"trim_smalt.sam")
                f.write('\n')
                f2.write("rm "+params.SMALTAligned_aln+IDSE[pos]+"_"+"trim_smalt.sam\n")
    
    if params.reads == "pairedEnd":
        for referenceSMALT in params.fastaList:
            if referenceSMALT.endswith(".fasta"):
                referenceSMALT = referenceSMALT[:-6]
            if referenceSMALT.endswith(".fa"):
                referenceSMALT = referenceSMALT[:-3] 
            pos = 0
            referenceSMALT_shortName = getFastaName(referenceSMALT)
            while pos <= len(fileArray)-2:
##                f.write(params.smaltBinary+" map -i "+str(params.insertmax)+" -j "+params.insertmin+" -f sam -n "+str(params.coreSplit[2])+" -O -o "+params.SMALTAligned_aln+IDPE[pos]+"_"+referenceSMALT_shortName+"_"+"trim"+"_smalt.sam "+params.reference+referenceSMALT+" "+params.trimmedFastQ+fileArrayPE[pos]+" "+params.trimmedFastQ+fileArrayPE[pos+1])
                f.write(params.smaltBinary+" map -x -i "+str(params.insertmax)+" -j "+params.insertmin+" -f sam -n "+str(params.coreSplit[2])+" -O -o "+params.SMALTAligned_aln+IDPE[pos]+"_"+"trim"+"_smalt.sam "+params.reference+referenceSMALT+" "+params.trimmedFastQ+fileArrayPE[pos]+" "+params.trimmedFastQ+fileArrayPE[pos+1])
                carryOverList_old.append(IDPE[pos]+"_"+"trim_smalt.sam")
                f.write('\n')
                f2.write("rm "+params.SMALTAligned_aln+IDPE[pos]+"_"+"trim"+"_smalt.sam\n")
                pos +=2
                
    
    if params.reads == "mixed":
        for referenceSMALT in params.fastaList:
            if referenceSMALT.endswith(".fasta"):
                referenceSMALT = referenceSMALT[:-6]
            if referenceSMALT.endswith(".fa"):
                referenceSMALT = referenceSMALT[:-3] 
            pos = 0
            referenceSMALT_shortName = getFastaName(referenceSMALT)
            for pos in range(len(fileArraySE)):
##                f.write(params.smaltBinary+" map -f sam -o "+params.SMALTAligned_aln+IDSE[pos]+"_"+referenceSMALT_shortName+"_"+"trim_smalt.sam "+params.reference+referenceSMALT+" "+params.trimmedFastQ+fileArraySE[pos])
                f.write(params.smaltBinary+" map -x -f sam -o "+params.SMALTAligned_aln+IDSE[pos]+"_"+"trim_smalt.sam "+params.reference+referenceSMALT+" "+params.trimmedFastQ+fileArraySE[pos])
                carryOverList_old.append(IDSE[pos]+"_"+"trim_smalt.sam")
                f.write('\n')
                f2.write("rm "+params.SMALTAligned_aln+IDSE[pos]+"_"+"trim_smalt.sam\n")
        for referenceSMALT in params.fastaList:
            if referenceSMALT.endswith(".fasta"):
                referenceSMALT = referenceSMALT[:-6]
            if referenceSMALT.endswith(".fa"):
                referenceSMALT = referenceSMALT[:-3] 
            pos = 0
            referenceSMALT_shortName = getFastaName(referenceSMALT) 
            while pos <= len(fileArrayPE)-2:
                #f.write(params.novoalign+" -d "+params.refNovo+referenceNOVO[0:-6]+".ndx"+" -f "+params.trimmedFastQ+fileArrayPE[pos]+" "+params.trimmedFastQ+fileArrayPE[pos+1]+" -o SAM '@RG\tID:"+IDPE[pos]+"\tSM:"+SMPE[pos]+"\tPL:Illumina' 2> "+params.NOVOAligned_aln+IDPE[pos]+"_"+"trim"+"_"+trimArrayPE[pos]+"and"+trimArrayPE[pos+1]+"_novo_stats.novodist > "+params.NOVOAligned_aln+IDPE[pos]+"_"+"trim_novo.sam")
##                f.write(params.smaltBinary+" map -i "+str(params.insertmax)+" -j "+str(params.insertmin)+" -f sam -n "+str(params.coreSplit[2])+" -O -o "+params.SMALTAligned_aln+IDPE[pos]+"_"+referenceSMALT_shortName+"_"+"trim"+"_smalt.sam "+params.reference+referenceSMALT+" "+params.trimmedFastQ+fileArrayPE[pos]+" "+params.trimmedFastQ+fileArrayPE[pos+1])
                f.write(params.smaltBinary+" map -x -i "+str(params.insertmax)+" -j "+str(params.insertmin)+" -f sam -n "+str(params.coreSplit[2])+" -O -o "+params.SMALTAligned_aln+IDPE[pos]+"_"+"trim"+"_smalt.sam "+params.reference+referenceSMALT+" "+params.trimmedFastQ+fileArrayPE[pos]+" "+params.trimmedFastQ+fileArrayPE[pos+1])
                carryOverList_old.append(IDPE[pos]+"_"+"trim_smalt.sam")
                f.write('\n')
                f2.write("rm "+params.SMALTAligned_aln+IDPE[pos]+"_"+"trim"+"_smalt.sam\n")
                pos += 2
    f.close()
    f2.close()
    ########################3
    #This creates the FIRST picard sort sh file to sort the sam file

    os.chdir(params.scripts_SMALT)
    picardSort = "2_sortSmaltSam.sh"
    picardSort_cleanup = "2_sortSmaltSam_cleanup.sh"
    carryOverList_new = []
    f = open(picardSort,'w')
    f2 = open(picardSort_cleanup,'w')
    for x in carryOverList_old: #SAMFileList:            
        f.write("java -Xmx"+params.mem+"g -jar "+params.SortSamDir+" I="+params.SMALTAligned_aln+x+" O="
        +params.SMALTAligned_aln+x[:(len(x)-4)]+"_sort.sam SORT_ORDER=coordinate VALIDATION_STRINGENCY=LENIENT")
        carryOverList_new.append(x[:(len(x)-4)]+"_sort.sam")                           
        f.write('\n')
        f2.write("rm "+params.SMALTAligned_aln+x[:(len(x)-4)]+"_sort.sam\n")
    #print "first picard sort sh file created as sortSmaltSam.sh"
    f.close()
    f2.close()

    ###########################
    #This adds read groups to the alignend and sorted sam files
    os.chdir(params.scripts_SMALT)
    addReadGroups = "3_addReadGroupsToSortedSam.sh"
    addReadGroups_cleanup = "3_addReadGroupsToSortedSam_cleanup.sh"
    pos = 0
    carryOverList_old = carryOverList_new
    carryOverList_new = []                             
    f = open(addReadGroups,'w')
    f2 = open(addReadGroups_cleanup,'w')

    for x in carryOverList_old:  #SAMFileList:
        readGroup = " RGID="+ID[pos]+" RGSM="+SM[pos]+" RGPL=Illumina RGPU=run RGLB="+LB[pos]
        f.write("java -jar "+params.picardAddReadGroup+" INPUT="+params.SMALTAligned_aln+x+" OUTPUT="+params.SMALTAligned_aln+x[:-4]+"_RG.sam"+readGroup)
        carryOverList_new.append(x[:-4]+"_RG.sam")
        f.write('\n')
        f2.write("rm "+params.SMALTAligned_aln+x[:-4]+"_RG.sam\n")
        pos += 1
    f.close()
    f2.close()
    return

#############################################-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
#scripts creation
def variantScriptsBWA(trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE):
    '''
    cleanedUpFileData = partitionFastQList(params.trimmedFastQ)
    fileArraySE = cleanedUpFileData[0]
    fileArrayPE = cleanedUpFileData[1]

    IDSE, SMSE, LBSE, barcodesSE, trimArraySE = extractFileNameData(fileArraySE) 
    IDPE, SMPE, LBPE, barcodesPE, trimArrayPE = extractFileNameData(fileArrayPE) 
    '''
    fileArraySE = trimmedFilesSE
    fileArrayPE = trimmedFilesPE
    if params.reads == "singleEnd":
        #fileArray , trimArray , ID, SM  = GenerateCleanFileNamesSEMode(params.trimmedFastQ)
        fileArray = trimmedFilesSE
##        trimArray = trimArraySE
        ID = IDSE
        SM = SMSE
    if params.reads == "pairedEnd":
        #fileArray , trimArray , ID, SM = GenerateCleanFileNames(params.trimmedFastQ)
        fileArray = trimmedFilesPE
##        trimArray = trimArrayPE
        ID = IDPE
        SM = SMPE
    if params.reads == "mixed":
        fileArray = trimmedFilesSE+trimmedFilesPE
##        trimArray = trimArraySE+trimArrayPE
        ID = IDSE + IDPE
        SM = SMSE + SMPE

##    fileArray , trimArray , ID, SM = GenerateCleanFileNames(params.trimmedFastQ) #change dirX to params.trimmedFastQ
    carryOverList_old = ID
    carryOverList_new = []
##########################################
    #this part creates the picard .sh file 
    os.chdir(params.BWAAligned_aln)
    #samFileList = []
    if params.reads == "singleEnds":
        for referenceBWA in params.fastaList:
            referenceBWA_shortName = getFastaName(referenceBWA)
            for pos in range(len(fileArraySE)): #Add all
##                carryOverList_new.append(carryOverList_old[pos]+"_"+referenceBWA_shortName+"_"+"trim_bwa.sam")
                carryOverList_new.append(carryOverList_old[pos]+"_"+"bwa.sam")
        
    if params.reads == "pairedEnd":
        for referenceBWA in params.fastaList:
            referenceBWA_shortName = getFastaName(referenceBWA)
            for pos in range(len(fileArrayPE)): # add every 2nd one
                if pos % 2 <> 0:
                    #samFileList.append(carryOverList_old[pos]+"_"+"trim_bwa.sam")
##                    carryOverList_new.append(carryOverList_old[pos]+"_"+referenceBWA_shortName+"_"+"trim_bwa.sam")
                    carryOverList_new.append(carryOverList_old[pos]+"_"+"bwa.sam")
     
    if params.reads == "mixed": # add both
        for referenceBWA in params.fastaList:
            referenceBWA_shortName = getFastaName(referenceBWA)
            for pos in range(len(fileArraySE)):
##                carryOverList_new.append(carryOverList_old[pos]+"_"+referenceBWA_shortName+"_"+"trim_bwa.sam")
                carryOverList_new.append(carryOverList_old[pos]+"_"+"bwa.sam")
            for pos in range(len(fileArrayPE)):
                if pos % 2 <> 0:
##                    carryOverList_new.append(carryOverList_old[pos+len(fileArraySE)]+"_"+referenceBWA_shortName+"_"+"trim_bwa.sam")
                    carryOverList_new.append(carryOverList_old[pos+len(fileArraySE)]+"_"+"bwa.sam")


    os.chdir(params.scripts_BWA)
    picard = "3_picardValidate.sh"
##    picard_Cleanup = "3_picardValidate_cleanup.sh"
    f = open(picard,'w')
    carryOverList_old = carryOverList_new
    carryOverList_new = []
    for x in carryOverList_old:
        f.write("java -jar "+params.picardDirOnPc+" I="+params.BWAAligned_aln+x+" O="+params.BWAAligned+picardReport+x[0:len(x)-4]+"_validateReport.txt")
        f.write('\n')
    #print "picard validation sh file created as picardValidate.sh"
    f.close()
    ############################################3
    
    #This part creates the sam to bam .sh file
         
    os.chdir(params.scripts_BWA)
    samToBam = "4_createSamToBam.sh"
    samToBam_cleanup = "4_createSamToBam_cleanup.sh"
    carryOverList_new = []
    f = open(samToBam,'w')
    f2 = open(samToBam_cleanup,'w')
    for x in carryOverList_old:   #samFileList:
        f.write(params.samtools+" view -Sb "+params.BWAAligned_aln+x+" | "+params.samtools+" sort - "+params.BWAAligned_aln+x[:(len(x)-4)]+"_sorted")
        f.write('\n')
        f2.write("rm "+params.BWAAligned_aln+x[:(len(x)-4)]+"_sorted.bam\n")
                 
        carryOverList_new.append(x[:(len(x)-4)]+"_sorted.bam")
    #print "sam to bam .sh file created as createSamToBam.sh"
    f.close()
    f2.close()

    #This part creates the indexing of the bam .sh file
    carryOverList_new = []    
    os.chdir(params.scripts_BWA)
    index = "5_indexBam.sh"
    index_cleanup = "5_indexBam_cleanup.sh"
    f = open(index,'w')
    f2 = open(index_cleanup,'w')
    for x in carryOverList_old:    #samFileList:
        f.write(params.samtools+" index "+params.BWAAligned_aln+x[:(len(x)-4)]+"_sorted.bam")
        f.write('\n')
        f2.write("rm "+params.BWAAligned_aln+x[:(len(x)-4)]+"_sorted.bam.bai\n")
        carryOverList_new.append(x[:(len(x)-4)]+"_sorted"+".bam")
    #print "bam indexing sh file created as indexBam.sh"
    f.close()


    #getMappedReads
    os.chdir(params.scripts_BWA)
    getMappedReads = "13_getMappedReads.sh"

    f = open(getMappedReads,'w')
    for x in carryOverList_new:    #samFileList:
        #print x[:(len(x)-11)]+"_realigned_resorted_dedup.bam"
        f.write(params.samtools+" flagstat "+params.BWAAligned_aln+x[:(len(x)-11)]+"_realigned_resorted_dedup.bam > "+params.BWAAligned_aln+x[:(len(x)-11)]+"_realigned_resorted_dedup_samtools_stats.txt")
        f.write('\n')
    #print "samtools flagstat .sh file created as getMappedReads.sh "
    f.close()

 #########################
    #This creates the GATK sh files
    
    GATK = "6_1_GATK.sh"
    GATK2 = "6_2_GATK.sh"

    GATK_cleanup = "6_1_GATK_cleanup.sh"
    GATK2_cleanup = "6_2_GATK_cleanup.sh"                 

    carryOverList_old = carryOverList_new
    carryOverList_new = []

    #ADD THIS FEATURE: Note - if there is a problem then the program can also run with the extra parameter "-fixMisencodedQuals" which subtracts 31 from the qualities
    # Thus run the program below, if the output file is not generated, then run with this additional parameter.
##    referenceBWA = loadReferenceFasta(params.bwaRef)
    referenceBWA = ""
    os.chdir(params.scripts_BWA)           
    f = open(GATK,'w')
    f2 = open(GATK2,'w')
    f3 = open(GATK_cleanup,'w')
    f4 = open(GATK2_cleanup,'w')
    for x in carryOverList_old:   #samFileList:
        for ref in params.fastaList:
            referenceBWA_shortName = getFastaName(ref)
            if referenceBWA_shortName in x:
                referenceBWA = ref
                break
            referenceBWA = ref
        if referenceBWA == "":
            print "FATAL ERROR, could not match any reference to the filename!"
            raw_input("press enter to exit")
            exit()
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T RealignerTargetCreator -R "+params.reference+referenceBWA+" -I "+params.BWAAligned_aln+x+" -o "+params.BWAAligned_aln+x[:(len(x)-4)]+".intervals")
        f.write('\n')
        f3.write("rm "+params.BWAAligned_aln+x[:(len(x)-4)]+".intervals\n")
##        f2.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -fixMisencodedQuals -T RealignerTargetCreator -R "+params.reference+referenceBWA+" -I "+params.BWAAligned_aln+x+" -o "+params.BWAAligned_aln+x[:(len(x)-4)]+".intervals")
##        f2.write('\n')
        f4.write("rm "+params.BWAAligned_aln+x[:(len(x)-4)]+".intervals"+"\n")
                 
        carryOverList_new.append(x[:(len(x)-4)]+".intervals") #This is the interval file, not the main bam file, so keep both old and new lists
    #print "GATK intervals sh file created as GATK.sh"
    f.close()
    f2.close()
    f3.close()
    f4.close()
    '''
    if samArray == []:
        if params.recal == True: 
            os.chdir(params.BWAAligned_aln)
            baseRecalArray = []   
            for fileX in glob.glob("*_bwa_sorted_realigned_recal.bam"):
                baseRecalArray.append(fileX)
            os.chdir(params.scripts_BWA)           
            f = open(GATK,'w')
            for x in baseRecalArray:
                f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T RealignerTargetCreator -R "+params.bwaRef+referenceNOVO+" -I "+params.BWAAligned_aln+x+" -o "+params.BWAAligned_aln+x[:(len(x)-4)]+"_sorted.intervals")
                f.write('\n')
            f.close()
     '''       

    ########################3
    #This creates the realignment sh file
    os.chdir(params.scripts_BWA)
    Realignment = "7_1_Realignment.sh"
    Realignment2 = "7_2_Realignment.sh"

    Realignment_cleanup = "7_1_Realignment_cleanup.sh"
    Realignment2_cleanup = "7_2_Realignment_cleanup.sh"
    temp = []     
    f = open(Realignment,'w')
    f2 = open(Realignment2,'w')
    f3 = open(Realignment_cleanup,'w')
    f4 =  open(Realignment2_cleanup,'w')
    for pos in range(len(carryOverList_old)): #samFileList:
        for ref in params.fastaList:
            referenceBWA_shortName = getFastaName(ref)
            if referenceBWA_shortName in carryOverList_old[pos]:
                referenceBWA = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T IndelRealigner -R "+params.reference+referenceBWA+" -I "+params.BWAAligned_aln+carryOverList_old[pos]+" -o "+params.BWAAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bam -targetIntervals "+params.BWAAligned_aln+carryOverList_new[pos])
        f.write('\n')
        f3.write("rm "+params.BWAAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bam\n")
        f3.write("rm "+params.BWAAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bai\n")
##        f2.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -fixMisencodedQuals -T IndelRealigner -R "+params.reference+referenceBWA+" -I "+params.BWAAligned_aln+carryOverList_old[pos]+" -o "+params.BWAAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bam -targetIntervals "+params.BWAAligned_aln+carryOverList_new[pos])
##        f2.write('\n')
        f4.write("rm "+params.BWAAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bam\n")
        temp.append(carryOverList_old[pos][:len(carryOverList_old[pos])-4]+"_realigned.bam")        
    #print "GATK realignment sh file created as Realignment.sh"
    f.close()
    f2.close()
    f3.close()
    f4.close()
    carryOverList_new = temp
    
    ########################3
    #This creates the base quality recalibration table
    #AND creates the base qualitry recal step script, this 2 steps in one script
    #creating temp bam array to save on space:
    if params.BQSRPossible:
        os.chdir(params.scripts_BWA)
        carryOverList_old = carryOverList_new
        carryOverList_new = []
        f1 = open("8.1_baseQualRecalBWA.sh",'w')
        f2 = open("8.2_baseQualRecalBWA.sh",'w')
        f3 = open("8.1_baseQualRecalBWA_cleanup.sh",'w')
        f4 = open("8.2_baseQualRecalBWA_cleanup.sh",'w')
        for x in carryOverList_old: #bAmFileList:
            for ref in params.fastaList:
                referenceBWA_shortName = getFastaName(ref)
                if referenceBWA_shortName in x:
                    referenceBWA = ref
                    break
                referenceBWA = ref
            f1.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T BaseRecalibrator -R "+params.reference+referenceBWA+" -I "+params.BWAAligned_aln+x+" -knownSites "+params.dbSNP+"dbSNP.vcf -o "+params.BWAAligned_aln+x[:len(x)-20]+"recal_data.table\n")
            f2.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T PrintReads -R "+params.reference+referenceBWA+" -I "+params.BWAAligned_aln+x+" -BQSR "+params.BWAAligned_aln+x[:len(x)-20]+"recal_data.table -o "+params.BWAAligned_aln+x[:len(x)-20]+"realigned_recal.bam\n")
            f3.write("rm "+params.BWAAligned_aln+x[:len(x)-20]+"recal_data.table\n")
            f4.write("rm "+params.BWAAligned_aln+x[:len(x)-20]+"realigned_recal.bam\n")
            f4.write("rm "+params.BWAAligned_aln+x[:len(x)-20]+"realigned_recal.bai\n")
            carryOverList_new.append(x[:(len(x)-20)]+"realigned_recal.bam")
        #print "picard sort sh file created as picardSort.sh"
        f1.close()
        f2.close()
        f3.close()
        f4.close()        
    else:
        print "Skipping Base quality score recalibration due to missing dbSNP file"

    ########################
    #This creates the picard sort sh file
    #depends on sam array
    # creating temp bam array to save on space:
    os.chdir(params.scripts_BWA)
    picardSort = "9_picardSort.sh"
    picardSort_cleanup = "9_picardSort_cleanup.sh"
    carryOverList_old = carryOverList_new
    carryOverList_new = []
    
    f = open(picardSort,'w')
    f2 = open(picardSort_cleanup,'w')
    if params.BQSRPossible:
        amount = 20
    else:
        amount = 21
    for x in carryOverList_old: #samFileList:          
        f.write("java -Xmx"+params.mem+"g -jar "+params.SortSamDir+" I="+params.BWAAligned_aln+x+" O="
        +params.BWAAligned_aln+x[:(len(x)-amount)]+"_realigned_resorted.bam SORT_ORDER=coordinate VALIDATION_STRINGENCY=LENIENT")
        f.write('\n')
        f2.write("rm "+params.BWAAligned_aln+x[:(len(x)-amount)]+"_realigned_resorted.bam\n")
        f2.write("rm "+params.BWAAligned_aln+x[:(len(x)-amount)]+"_realigned_resorted.bai\n")
        carryOverList_new.append(x[:(len(x)-amount)]+"_realigned_resorted.bam")
    #print "picard sort sh file created as picardSort.sh"
    f.close()
    f2.close()


    ########################3
    #This creates the RE-indexing of the bam files sh file
    os.chdir(params.scripts_BWA)
    reIndexBamFiles = "10_reIndexBamFiles.sh"
    reIndexBamFiles_cleanup = "10_reIndexBamFiles_cleanup.sh"
    carryOverList_old = carryOverList_new
    carryOverList_new = []
     
    f = open(reIndexBamFiles,'w')
    f2 = open(reIndexBamFiles_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        f.write(params.samtools+" index "+params.BWAAligned_aln+x)               
        f.write('\n')
        f2.write("rm "+params.BWAAligned_aln+x+".bai\n")
    #print "re-Index of bam files sh writen to reIndexBamFiles.sh"
    f.close()
    f2.close()


    ########################3
    #This creates the remove PCR duplicates sh file
    os.chdir(params.scripts_BWA)
    removePCRDuplicates = "11_removePCRDuplicates.sh"
    removePCRDuplicates_cleanup = "11_removePCRDuplicates_cleanup.sh" 
    f = open(removePCRDuplicates,'w')
    f2 = open(removePCRDuplicates_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        f.write("java -jar "+params.markDuplicatesDir+" I="+params.BWAAligned_aln+x+" O="+params.BWAAligned_aln+x[:(len(x)-4)]+"_dedup.bam VALIDATION_STRINGENCY=LENIENT REMOVE_DUPLICATES=TRUE M=duplicate_metrics TMP_DIR=tmp ASSUME_SORTED=true > "+params.BWAAligned_aln+x[:(len(x)-4)]+"_rmdup.log")
        f.write('\n')
        f2.write("rm "+params.BWAAligned_aln+x[:(len(x)-4)]+"_dedup.bam\n")
    #print "picard remove pcr duplicates sh writen to removePCRDuplicates.sh"
    f.close()
    f2.close()
    
    ########################3
    #This creates the RE-indexing of the bam files sh file a 1nd time after the removal of pcr duplicates as in previous step
    os.chdir(params.scripts_BWA)
    reIndexBamFiles2 = "12_reIndexBamFiles2.sh"
    reIndexBamFiles2_cleanup = "12_reIndexBamFiles2_cleanup.sh"
    carryOverList_new = []     
    f = open(reIndexBamFiles2,'w')
    f2 = open(reIndexBamFiles2_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        f.write(params.samtools+" index "+params.BWAAligned_aln+x[:(len(x)-4)]+"_dedup.bam")              
        f.write('\n')
        f2.write("rm "+params.BWAAligned_aln+x[:(len(x)-4)]+"_dedup.bam\n")
        carryOverList_new.append(x[:(len(x)-4)]+"_dedup.bam")
    #print "re-Index (FOR SECOND TIME) of bam files sh writen to reIndexBamFiles2.sh"
    f.close()
    f2.close()

    ########################3
    #This creates the UNIFIED GENOTYPER SNP calling sh file using GATK
    
    os.chdir(params.scripts_BWA)
    snpCalling = "14_SNPCallingGATK.sh"
    carryOverList_old = carryOverList_new
     
    f = open(snpCalling,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceBWA_shortName = getFastaName(ref)
            if referenceBWA_shortName in x:
                referenceBWA = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T UnifiedGenotyper -l INFO -R "+params.reference+referenceBWA+" -I "+params.BWAAligned_aln+x+" -o "+params.BWAAligned+snpDir+x[:-38]+"_gatk_snps.vcf -stand_call_conf 50 -stand_emit_conf 10.0 -dcov 2000 > "+params.BWAAligned+snpDir+x[:-38]+"_Genotype.log")  
        f.write('\n')
##    print "SNP calling using GATK sh writen to SNPCallingGATK.sh"
    f.close()

    ########################3
    #This creates the UNIFIED GENOTYPER INDEL calling sh file using GATK
    '''
    os.chdir(params.scripts_BWA)
    indelCalling = "INDELCallingGATK.sh"
     
    f = open(indelCalling,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceBWA_shortName = getFastaName(ref)
            if referenceBWA_shortName in x:
                referenceBWA = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T UnifiedGenotyper -l INFO --genotype_likelihoods_model INDEL -R "+params.reference+referenceBWA+" -I "+params.BWAAligned_aln+x+" -o "+params.BWAAligned+indelDir+x[:-38]+"_gatk_indels.vcf -stand_call_conf 50 -stand_emit_conf 10.0 -dcov 2000 > "+params.BWAAligned+indelDir+x[:-38]+"_Indel_Genotype.log")  
        f.write('\n')
    #print "SNP calling using GATK sh writen to INDELCallingGATK.sh"
    f.close()
    '''
    ########################3
    #This creates the SNP CALLER sh file using SAMTOOLS

    os.chdir(params.scripts_BWA)
    snpCalling = "14_2_VARIANT_CALLING_SAMTOOLS.sh"
     
    f = open(snpCalling,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceBWA_shortName = getFastaName(ref)
            if referenceBWA_shortName in x:
                referenceBWA = ref
                break
        f.write(params.samtools+" mpileup -ug -f "+params.reference+referenceBWA+" "+params.BWAAligned_aln+x+" | "+params.bcftools+" view -bcvg | "+params.bcftools+" view > "+params.BWAAligned+snpDir+x[:-33]+"_VARIANTS_SAMTOOLS.vcf"+"\n")
    f.close()
    
    ########################3
    #This creates the SNP HAPLOTYPE CALLER sh file using GATK


    os.chdir(params.scripts_BWA)
    snpCalling = "14_SNP_INDEL_Calling_GATK_HAPLOTYPECALLER.sh"
     
    f = open(snpCalling,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceBWA_shortName = getFastaName(ref)
            if referenceBWA_shortName in x:
                referenceBWA = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T HaplotypeCaller -R "+params.reference+referenceBWA+" -I "+params.BWAAligned_aln+x+" -o "+params.BWAAligned+snpDir+x[:-33]+"_gatk_HC_snps.vcf -stand_call_conf 30 -stand_emit_conf 10.0 > "+params.BWAAligned+snpDir+x[:-33]+"_gatk_HC_snps.log")  
        #f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T HaplotypeCaller -nct "+str(coreSplit[0])+" -R "+params.bwaRef+referenceBWA+" -I "+params.BWAAligned_aln+x+" -o "+params.BWAAligned_aln+snpDir+x[:-38]+"_gatk_HC_snps.vcf -stand_call_conf 30 -stand_emit_conf 10.0 > "+params.BWAAligned+snpDir+x[:-38]+"_SNPHC_Genotype.log")  
        f.write('\n')
##    print "SNP calling using GATK sh writen to SNPCallingGATK.sh"
    f.close()

    ########################3
    #This creates the genome coverage gatk sh file
    os.chdir(params.scripts_BWA)
    GenomeCoverage = "15_GenomeCoverage.sh"
    GenomeCoverage_cleanup = "15_GenomeCoverage_cleanup.sh"
    f = open(GenomeCoverage,'w')
    f2 = open(GenomeCoverage_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceBWA_shortName = getFastaName(ref)
            if referenceBWA_shortName in x:
                referenceBWA = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T DepthOfCoverage -R "+params.reference+referenceBWA+" -o "+params.BWAAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov.txt -I "+params.BWAAligned_aln+x)#+" -nt"+str(coreSplit[0]))
        f.write('\n')
        f2.write("rm "+params.BWAAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov.txt\n")
    #print "genome coverage bed tools sh file created as GenomeCoverage.sh"
    f.close()
    f2.close()
    
    ########################3
    #This creates the genome coverage bedtools sh file
    os.chdir(params.scripts_BWA)
    GenomeCoverage = "15_2_GenomeCoverage.sh"
    GenomeCoverage_cleanup = "15_2_GenomeCoverage_cleanup.sh"
    f = open(GenomeCoverage,'w')
    f2 = open(GenomeCoverage_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceBWA_shortName = getFastaName(ref)
            if referenceBWA_shortName in x:
                referenceBWA = ref
                break
        f.write(params.bedtools+"genomeCoverageBed -bga -ibam "+params.BWAAligned_aln+x+" -g "+params.reference+referenceBWA+" > "+params.BWAAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov_bed.txt\n") 
        f2.write("rm "+params.BWAAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov_bed.txt\n")
    #print "genome coverage bed tools sh file created as GenomeCoverage.sh"
    f.close()
    f2.close()

 ########################3
    #This creates the regions with zero coverage sh file
    os.chdir(params.scripts_BWA)
    ZeroCov = "16_ZeroCov.sh"
    ZeroCov_cleanup = "16_ZeroCov_cleanup.sh"
     
    f = open(ZeroCov,'w')
    f2 = open(ZeroCov_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        f.write("awk 'NF && $4<2' "+params.BWAAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov_bed.txt > "+params.BWAAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov=0.txt")
##        f.write("grep -w 0$ "+params.BWAAligned+genomeCovDir+x[:(len(x)-38)]+"_genomecov.txt > "+params.BWAAligned+genomeCovDir+x[:(len(x)-38)]+"_genomecov=0.txt")
##        f.write("grep -w 0$ "+fileX[:-4]+"_gen_cov.txt > "+fileX[:-4]+"_gen_0-cov.txt\n")               
        f.write('\n')
        f2.write(params.BWAAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov=0.txt\n")
    #print "zero coverage sh file written as ZeroCov.sh"
    f.close()
    f2.close()
    return

#############################################-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
#scripts creation
def variantScriptsNOVO(trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE):
    '''
    cleanedUpFileData = partitionFastQList(params.trimmedFastQ)
    fileArraySE = cleanedUpFileData[0]
    fileArrayPE = cleanedUpFileData[1]

    IDSE, SMSE, LBSE, barcodesSE, trimArraySE = extractFileNameData(fileArraySE)
    IDPE, SMPE, LBPE, barcodesPE, trimArrayPE = extractFileNameData(fileArrayPE) 
    '''
    fileArraySE = trimmedFilesSE
    fileArrayPE = trimmedFilesPE
    if params.reads == "singleEnd":
        #fileArray , trimArray , ID, SM  = GenerateCleanFileNamesSEMode(params.trimmedFastQ)
        fileArray = trimmedFilesSE
##        trimArray = trimArraySE
        ID = IDSE
        SM = SMSE
    if params.reads == "pairedEnd":
        #fileArray , trimArray , ID, SM = GenerateCleanFileNames(params.trimmedFastQ)
        fileArray = trimmedFilesPE
##        trimArray = trimArrayPE
        ID = IDPE
        SM = SMPE
    if params.reads == "mixed":
        fileArray = trimmedFilesSE+trimmedFilesPE
##        trimArray = trimArraySE+trimArrayPE
        ID = IDSE + IDPE
        SM = SMSE + SMPE

    carryOverList_old = ID
    carryOverList_new = []
##########################################
    #this part creates the picard .sh file 

    os.chdir(params.NOVOAligned_aln)
    if params.reads == "singleEnd":
        for referenceNOVO in params.fastaList:
            referenceNOVO_shortName = getFastaName(referenceNOVO)
            for pos in range(len(fileArraySE)): #Add all
##                carryOverList_new.append(carryOverList_old[pos]+"_"+referenceNOVO_shortName+"_"+"trim_novo.sam")
                carryOverList_new.append(carryOverList_old[pos]+"_"+"trim_novo.sam")
            
    if params.reads == "pairedEnds":
        for referenceNOVO in params.fastaList:
            referenceNOVO_shortName = getFastaName(referenceNOVO)
            for pos in range(len(fileArrayPE)): # add every 2nd one
                if pos % 2 <> 0:
                    #samFileList.append(carryOverList_old[pos]+"_"+"trim_bwa.sam")
##                    carryOverList_new.append(carryOverList_old[pos]+"_"+referenceNOVO_shortName+"_"+"trim_novo.sam")
                    carryOverList_new.append(carryOverList_old[pos]+"_"+"trim_novo.sam")
         
    if params.reads == "mixed": # add both
        for referenceNOVO in params.fastaList:
            referenceNOVO_shortName = getFastaName(referenceNOVO)
            for pos in range(len(fileArraySE)):
##                carryOverList_new.append(carryOverList_old[pos]+"_"+referenceNOVO_shortName+"_"+"trim_novo.sam")
                carryOverList_new.append(carryOverList_old[pos]+"_"+"trim_novo.sam")
            for pos in range(len(fileArrayPE)):
                if pos % 2 <> 0:
##                    carryOverList_new.append(carryOverList_old[pos+len(fileArraySE)]+"_"+referenceNOVO_shortName+"_"+"trim_novo.sam")
                    carryOverList_new.append(carryOverList_old[pos+len(fileArraySE)]+"_"+"trim_novo.sam")
                
    os.chdir(params.scripts_NOVO)
    picard = "2_picardValidate.sh"
##    picard_cleanup = "2_picardValidate_cleanup.sh"
    f = open(picard,'w')
##    f2 = open(picard_cleanup,'w')
    carryOverList_old = carryOverList_new
    carryOverList_new = []
    pos = 0
    for x in carryOverList_old: #while pos <= len(carryOverList_old)-1:
        f.write("java -jar "+params.picardDirOnPc+" I="+params.NOVOAligned_aln+x+" O="+params.NOVOAligned+picardReport+x[0:len(x)-4]+"_validateReport.txt")
        f.write('\n')
##        f2.write()
        pos += 1
    #print "picard validation sh file created as picardValidate.sh"
    f.close()
    ############################################3
    
    #This part creates the sam to bam .sh file
       
    os.chdir(params.scripts_NOVO)
    samToBam = "3_createSamToBam.sh"
    samToBam_cleanup = "3_createSamToBam_cleanup.sh"
    f = open(samToBam,'w')
    f2 = open(samToBam_cleanup,'w')
    for x in carryOverList_old:   #samFileList:
        f.write(params.samtools+" view -Sb "+params.NOVOAligned_aln+x+" | "+params.samtools+" sort - "+params.NOVOAligned_aln+x[:(len(x)-4)]+"_sorted")
        f.write('\n')
        f2.write("rm "+params.NOVOAligned_aln+x[:(len(x)-4)]+"_sorted.bam\n")
    #print "sam to bam .sh file created as createSamToBam.sh"
    f.close()

    #This part creates the indexing of the bam .sh file
        
    os.chdir(params.scripts_NOVO)
    index = "4_indexBam.sh"
    index_cleanup = "4_indexBam_cleanup.sh"
    f = open(index,'w')
    f2 = open(index_cleanup,'w')
    for x in carryOverList_old:    #samFileList:
        f.write(params.samtools+" index "+params.NOVOAligned_aln+x[:(len(x)-4)]+"_sorted"+".bam")
        f.write('\n')
        f2.write("rm "+params.NOVOAligned_aln+x[:(len(x)-4)]+"_sorted"+".bam.bai")
        carryOverList_new.append(x[:(len(x)-4)]+"_sorted.bam")
    #print "bam indexing sh file created as indexBam.sh"
    f.close()
    f2.close()


    #getMappedReads
    #depends on samArray created above
    os.chdir(params.scripts_NOVO)
    getMappedReads = "12_getMappedReads.sh"
    f = open(getMappedReads,'w')
    for x in carryOverList_old:    #samFileList:
        f.write(params.samtools+" flagstat "+params.NOVOAligned_aln+x[:(len(x)-4)]+"_realigned_resorted_dedup.bam > "+params.NOVOAligned_aln+x[:(len(x)-4)]+"_realigned_resorted_dedup_samtools_stats.txt")
        f.write('\n')
    #print "samtools flagstat .sh file created as getMappedReads.sh "
    f.close()

 #########################
    #This creates the GATK sh files
    carryOverList_old = carryOverList_new
    carryOverList_new = []
    GATK = "5_1_GATK.sh"
    GATK2 = "5_2_GATK.sh"
    GATK_cleanup = "5_1_GATK_cleanup.sh"
##    GATK2_cleanup = "5_2_GATK_cleanup.sh"
    
##    referenceNOVO = loadReferenceFasta(params.refNovo)
    os.chdir(params.scripts_NOVO)
    f = open(GATK,'w')
    f2 = open(GATK2,'w')

    f3 = open(GATK_cleanup,'w')
##    f4 = open(GATK2_cleanup,'w')
    for x in carryOverList_old:   #samFileList:
        for ref in params.fastaList:
            referenceNOVO_shortName = getFastaName(ref)
            if referenceNOVO_shortName in x:
                referenceNOVO = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T RealignerTargetCreator -R "+params.reference+referenceNOVO+" -I "+params.NOVOAligned_aln+x+" -o "+params.NOVOAligned_aln+x[:(len(x)-4)]+".intervals")
        f.write('\n')
##        f2.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -fixMisencodedQuals -T RealignerTargetCreator -R "+params.reference+referenceNOVO+" -I "+params.NOVOAligned_aln+x+" -o "+params.NOVOAligned_aln+x[:(len(x)-4)]+".intervals")
##        f2.write('\n')
        carryOverList_new.append(x[:(len(x)-4)]+".intervals") #This is the interval file, not the main bam file, so keep both old and new lists

        f3.write("rm "+params.NOVOAligned_aln+x[:(len(x)-4)]+".intervals\n")
##        f4.write("rm "+params.NOVOAligned_aln+x[:(len(x)-4)]+".intervals\n")
    #print "GATK intervals sh file created as GATK.sh"
    
    f.close()
    f2.close()
    f3.close()
    
    ########################3
    #This creates the realignment sh file
    temp = []  
    os.chdir(params.scripts_NOVO)
    Realignment = "6_1_Realignment.sh"
    Realignment2 = "6_2_Realignment.sh"
    Realignment_cleanup = "6_x_Realignment_cleanup.sh"
    f = open(Realignment,'w')
    f2 = open(Realignment2,'w')
    f3 = open(Realignment_cleanup,'w')
    for pos in range(len(carryOverList_old)): #samFileList:
        for ref in params.fastaList:
            referenceNOVO_shortName = getFastaName(ref)
            if referenceNOVO_shortName in carryOverList_old[pos]:
                referenceNOVO = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T IndelRealigner -R "+params.reference+referenceNOVO+" -I "+params.NOVOAligned_aln+carryOverList_old[pos]+" -o "+params.NOVOAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bam"+" -targetIntervals "+params.NOVOAligned_aln+carryOverList_new[pos])
        f.write('\n')
##        f2.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -fixMisencodedQuals -T IndelRealigner -R "+params.reference+referenceNOVO+" -I "+params.NOVOAligned_aln+carryOverList_old[pos]+" -o "+params.NOVOAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bam"+" -targetIntervals "+params.NOVOAligned_aln+carryOverList_new[pos])
##        f2.write('\n')
        f3.write("rm "+params.NOVOAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bam\n")
        f3.write("rm "+params.NOVOAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bai\n")
        temp.append(carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bam")
    #print "GATK realignment sh file created as Realignment.sh"
    f.close()
    f2.close()
    f3.close()
    carryOverList_new = temp
    
    ########################3
    #This creates the base quality recalibration sh file
    #creating temp bam array to save on space:
    if params.BQSRPossible:
        os.chdir(params.scripts_NOVO)
        carryOverList_old = carryOverList_new
        carryOverList_new = []
        f1 = open("7.1_baseQualRecalNOVO.sh",'w')
        f2 = open("7.2_baseQualRecalNOVO.sh",'w')
        f3 = open("7.1_baseQualRecalNOVO_cleanup.sh",'w')
        f4 = open("7.2_baseQualRecalNOVO_cleanup.sh",'w')
        for x in carryOverList_old: 
            for ref in params.fastaList:
                referenceNOVO_shortName = getFastaName(ref)
                if referenceNOVO_shortName in x:
                    referenceNOVO = ref
                    break
                referenceNOVO = ref
            f1.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T BaseRecalibrator -R "+params.reference+referenceNOVO+" -I "+params.NOVOAligned_aln+x+" -knownSites "+params.dbSNP+"dbSNP.vcf -o "+params.NOVOAligned_aln+x[:len(x)-20]+"recal_data.table\n")
            f2.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T PrintReads -R "+params.reference+referenceNOVO+" -I "+params.NOVOAligned_aln+x+" -BQSR "+params.NOVOAligned_aln+x[:len(x)-20]+"recal_data.table -o "+params.NOVOAligned_aln+x[:len(x)-20]+"realigned_recal.bam\n")
            carryOverList_new.append(x[:(len(x)-20)]+"realigned_recal.bam")
            f3.write("rm "+params.NOVOAligned_aln+x[:len(x)-20]+"recal_data.table\n")
            f4.write("rm "+params.NOVOAligned_aln+x[:len(x)-20]+"realigned_recal.bam\n")
            f4.write("rm "+params.NOVOAligned_aln+x[:len(x)-20]+"realigned_recal.bai\n")
        #print "picard sort sh file created as picardSort.sh"
        f1.close()
        f2.close()
        f3.close()
        f4.close()
    else:
        print "Skipping Base quality score recalibration due to missing dbSNP file"

    ########################3
    #This creates the picard sort sh file
    #depends on sam array
    # creating temp bam array to save on space:

    os.chdir(params.scripts_NOVO)
    picardSort = "8_picardSort.sh"
    picardSort_cleanup = "8_picardSort_cleanup.sh"
    carryOverList_old = carryOverList_new
    carryOverList_new = []
    
    if BQSRPossible:
        amount = 20
    else:
        amount = 21
    f = open(picardSort,'w')
    f2 = open(picardSort_cleanup,'w')
    for x in carryOverList_old: #samFileList:            
        f.write("java -Xmx"+params.mem+"g -jar "+params.SortSamDir+" I="+params.NOVOAligned_aln+x+" O="
        +params.NOVOAligned_aln+x[:(len(x)-amount)]+"_realigned_resorted.bam SORT_ORDER=coordinate VALIDATION_STRINGENCY=LENIENT")
        f.write('\n')
        f2.write("rm "+params.NOVOAligned_aln+x[:(len(x)-amount)]+"_realigned_resorted.bam\n")
##        f2.write("rm "+params.NOVOAligned_aln+x[:(len(x)-20)]+"_realigned_resorted.bam.bai\n")
        carryOverList_new.append(x[:(len(x)-amount)]+"_realigned_resorted.bam")
    #print "picard sort sh file created as picardSort.sh"
    f.close()
    f2.close()


    ########################3
    #This creates the RE-indexing of the bam files sh file
    os.chdir(params.scripts_NOVO)
    reIndexBamFiles = "9_reIndexBamFiles.sh"
    reIndexBamFiles_cleanup = "9_reIndexBamFiles_cleanup.sh"
    carryOverList_old = carryOverList_new
    carryOverList_new = []  
    f = open(reIndexBamFiles,'w')
    f2= open(reIndexBamFiles_cleanup,'w')

    for x in carryOverList_old: #bamFileList:
        f.write(params.samtools+" index "+params.NOVOAligned_aln+x)               
        f.write('\n')
        f2.write("rm "+params.NOVOAligned_aln+x+".bai\n")
    #print "re-Index of bam files sh writen to reIndexBamFiles.sh"
    f.close()
    f2.close()

    ########################3
    #This creates the remove PCR duplicates sh file
    os.chdir(params.scripts_NOVO)
    removePCRDuplicates = "10_removePCRDuplicates.sh"
    removePCRDuplicates_cleanup = "10_removePCRDuplicates_cleanup.sh"
     
    f = open(removePCRDuplicates,'w')
    f2 = open(removePCRDuplicates_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        f.write(params.java7+" -jar "+params.markDuplicates+" I="+params.NOVOAligned_aln+x+" O="+params.NOVOAligned_aln+x[:(len(x)-4)]+"_dedup.bam VALIDATION_STRINGENCY=LENIENT REMOVE_DUPLICATES=TRUE M=duplicate_metrics TMP_DIR=tmp ASSUME_SORTED=true > "+params.NOVOAligned_aln+x[:(len(x)-4)]+"_rmdup.log")
        f.write('\n')
        f2.write("rm "+params.NOVOAligned_aln+x[:(len(x)-4)]+"_dedup.bam\n")
    #print "picard remove pcr duplicates sh writen to removePCRDuplicates.sh"
    f.close()
    f2.close()
  
    ########################3
    #This creates the RE-indexing of the bam files sh file a 1nd time after the removal of pcr duplicates as in previous step
    os.chdir(params.scripts_NOVO)
    reIndexBamFiles2 = "11_reIndexBamFiles2.sh"
    reIndexBamFiles2_cleanup = "11_reIndexBamFiles2_cleanup.sh"
    carryOverList_new = [] 
    f = open(reIndexBamFiles2,'w')
    f2 = open(reIndexBamFiles2_cleanup,'w')
    
    for x in carryOverList_old: #bamFileList:
        f.write(params.samtools+" index "+params.NOVOAligned_aln+x[:(len(x)-4)]+"_dedup.bam")              
        f.write('\n')
        f2.write("rm "+params.NOVOAligned_aln+x[:(len(x)-4)]+"_dedup.bai\n")
        carryOverList_new.append(x[:(len(x)-4)]+"_dedup.bam")
    #print "re-Index (FOR SECOND TIME) of bam files sh writen to reIndexBamFiles2.sh"
    f.close()
    f2.close()

    ########################3
    #This creates the SNP calling sh file using GATK
    os.chdir(params.scripts_NOVO)
    snpCalling = "SNPCallingGATK.sh"
    carryOverList_old = carryOverList_new
    f = open(snpCalling,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceNOVO_shortName = getFastaName(ref)
            if referenceNOVO_shortName in x:
                referenceNOVO = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T UnifiedGenotyper -l INFO -R "+params.reference+referenceNOVO+" -I "+params.NOVOAligned_aln+x+" -o "+params.NOVOAligned+snpDir+x[:-39]+"_gatk_snps.vcf -stand_call_conf 50 -stand_emit_conf 10.0 -dcov 2000 > "+params.NOVOAligned+snpDir+x[:-39]+"_Genotype.log")  
        f.write('\n')
    #print "SNP calling using GATK sh writen to SNPCallingGATK.sh"
    f.close()

    ########################3
    #This creates the INDEL calling sh file using GATK
    '''
    os.chdir(params.scripts_NOVO)
    indelCalling = "INDELCallingGATK.sh"
     
    f = open(indelCalling,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceNOVO_shortName = getFastaName(ref)
            if referenceNOVO_shortName in x:
                referenceNOVO = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T UnifiedGenotyper -l INFO --genotype_likelihoods_model INDEL -R "+params.reference+referenceNOVO+" -I "+params.NOVOAligned_aln+x+" -o "+params.NOVOAligned+indelDir+x[:-39]+"_gatk_indels.vcf -stand_call_conf 50 -stand_emit_conf 10.0 -dcov 2000 > "+params.NOVOAligned+indelDir+x[:-39]+"_Indel_Genotype.log")  
        f.write('\n')
    #print "SNP calling using GATK sh writen to INDELCallingGATK.sh"
    f.close()
    '''
    
    
    ########################3
    #This creates the SNP HAPLOTYPE CALLER sh file using GATK
    os.chdir(params.scripts_NOVO)
    snpCalling = "13_2_VARIANT_CALLING_SAMTOOLS.sh"
     
    f = open(snpCalling,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceNOVO_shortName = getFastaName(ref)
            if referenceNOVO_shortName in x:
                referenceNOVO = ref
                break              
        f.write(params.samtools+" mpileup -ug -f "+params.reference+referenceNOVO+" "+params.NOVOAligned_aln+x+" | "+params.bcftools+" view -bcvg | "+params.bcftools+" view > "+params.NOVOAligned+snpDir+x[:-4]+"_VARIANTS_SAMTOOLS.vcf"+"\n")
    #print "SNP calling using GATK sh writen to SNPCallingGATK.sh"
    f.close()
    
    
    ########################3
    #This creates the SNP HAPLOTYPE CALLER sh file using GATK
    os.chdir(params.scripts_NOVO)
    snpCalling = "13_SNP_INDEL_Calling_GATK_HAPLOTYPECALLER.sh"
     
    f = open(snpCalling,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceNOVO_shortName = getFastaName(ref)
            if referenceNOVO_shortName in x:
                referenceNOVO = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T HaplotypeCaller -nct "+str(params.coreSplit[1])+" -R "+params.reference+referenceNOVO+" -I "+params.NOVOAligned_aln+x+" -o "+params.NOVOAligned+snpDir+x[:-4]+"_gatk_HC_snps.vcf -stand_call_conf 30 -stand_emit_conf 10.0 > "+params.NOVOAligned+snpDir+x[:-4]+"_SNPHC_Genotype.log")  
        f.write('\n')
    #print "SNP calling using GATK sh writen to SNPCallingGATK.sh"
    f.close()

    ########################3
    #This creates the genome coverage bed tools sh file
    
    os.chdir(params.scripts_NOVO)
    GenomeCoverage = "14_GenomeCoverage.sh"
    GenomeCoverage_cleanup = "14_GenomeCoverage_cleanup.sh"
     
    f = open(GenomeCoverage,'w')
    f2 = open(GenomeCoverage_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceNOVO_shortName = getFastaName(ref)
            if referenceNOVO_shortName in x:
                referenceNOVO = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T DepthOfCoverage -R "+params.reference+referenceNOVO+" -o "+params.NOVOAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov.txt -I "+params.NOVOAligned_aln+x) #+" -nt "+str(coreSplit[0]))
        f.write('\n')
        f2.write("rm "+params.NOVOAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov.txt\n")
    #print "genome coverage bed tools sh file created as GenomeCoverage.sh"
    f.close()
    f2.close()
    
    ########################3
    #This creates the genome coverage bed tools sh file
    
    os.chdir(params.scripts_NOVO)
    GenomeCoverage = "14_2_GenomeCoverage.sh"
    GenomeCoverage_cleanup = "14_2_GenomeCoverage_cleanup.sh"
     
    f = open(GenomeCoverage,'w')
    f2 = open(GenomeCoverage_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceNOVO_shortName = getFastaName(ref)
            if referenceNOVO_shortName in x:
                referenceNOVO = ref
                break
        f.write(params.bedtools+"genomeCoverageBed -bga -ibam "+params.NOVOAligned_aln+x+" -g "+params.reference+referenceNOVO+" > "+params.NOVOAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov_bed.txt")
        f.write('\n')
        f2.write("rm "+params.NOVOAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov_bed.txt\n")
    #print "genome coverage bed tools sh file created as GenomeCoverage.sh"
    f.close()
    f2.close()
    

 ########################3
    #This creates the regions with zero coverage sh file
    os.chdir(params.scripts_NOVO)
    ZeroCov = "15_ZeroCov.sh"
    ZeroCov_cleanup = "15_ZeroCov_cleanup.sh"
     
    f = open(ZeroCov,'w')
    f2 = open(ZeroCov_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        #f.write("grep -w 0$ "+params.NOVOAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov_bed.txt > "+params.NOVOAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov=0.txt") 
        f.write("awk 'NF && $4<2' "+params.NOVOAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov_bed.txt > "+params.NOVOAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov=0.txt")
        f.write('\n')
        f2.write("rm "+params.NOVOAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov=0.txt\n")
    #print "zero coverage sh file written as ZeroCov.sh"
    f.close()
    f2.close()
    return

#############################################-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
#scripts creation
def variantScriptsSMALT(trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE):
    '''
    cleanedUpFileData = partitionFastQList(params.trimmedFastQ)
    fileArraySE = cleanedUpFileData[0]
    fileArrayPE = cleanedUpFileData[1]

    IDSE, SMSE, LBSE, barcodesSE, trimArraySE = extractFileNameData(fileArraySE) #change dirX to params.trimmedFastQ
    IDPE, SMPE, LBPE, barcodesPE, trimArrayPE = extractFileNameData(fileArrayPE) 
    '''
    fileArraySE = trimmedFilesSE
    fileArrayPE = trimmedFilesPE
    if params.reads == "singleEnd":
        #fileArray , trimArray , ID, SM  = GenerateCleanFileNamesSEMode(params.trimmedFastQ)
        fileArray = trimmedFilesSE
##        trimArray = trimArraySE
        ID = IDSE
        SM = SMSE
    if params.reads == "pairedEnd":
        #fileArray , trimArray , ID, SM = GenerateCleanFileNames(params.trimmedFastQ)
        fileArray = trimmedFilesPE
##        trimArray = trimArrayPE
        ID = IDPE
        SM = SMPE
    if params.reads == "mixed":
        fileArray = trimmedFilesSE+trimmedFilesPE
##        trimArray = trimArraySE+trimArrayPE
        ID = IDSE + IDPE
        SM = SMSE + SMPE

    carryOverList_old = ID
    carryOverList_new = []
##########################################
    #this part creates the picard .sh file 

    os.chdir(params.SMALTAligned_aln)
    
    if params.reads == "singleEnd":
        for pos in range(len(fileArraySE)): #Add all
            carryOverList_new.append(carryOverList_old[pos]+"_"+"trim_smalt_sort_RG.sam")
        
    if params.reads == "pairedEnd":
        for pos in range(len(fileArrayPE)): # add every 2nd one
            if pos % 2 <> 0:
                #samFileList.append(carryOverList_old[pos]+"_"+"trim_bwa.sam")
                carryOverList_new.append(carryOverList_old[pos]+"_"+"trim_smalt_sort_RG.sam")
     
    if params.reads == "mixed": # add both
        for pos in range(len(fileArraySE)):
            carryOverList_new.append(carryOverList_old[pos]+"_"+"trim_smalt_sort_RG.sam")
        for pos in range(len(fileArrayPE)):
            if pos % 2 <> 0:
                carryOverList_new.append(carryOverList_old[pos+len(fileArraySE)]+"_"+"trim_smalt_sort_RG.sam")
                
    os.chdir(params.scripts_SMALT)
    picard = "4_picardValidate.sh"
##    picard_cleanup = "4_picardValidate_cleanup.sh"
    f = open(picard,'w')
##    f2 = open(picard_cleanup,'w')
    carryOverList_old = carryOverList_new
    carryOverList_new = []
    pos = 0
    for x in carryOverList_old:#while pos <= len(carryOverList_old)-1:
        #f.write("java -jar "+params.picardDirOnPc+" I="+params.SMALTAligned_aln+samFileList[pos]+" O="+params.SMALTAligned+picardReport+samFileList[pos][0:len(samFileList[pos])-4]+"_validateReport.txt")
        f.write("java -jar "+params.picardDirOnPc+" I="+params.SMALTAligned_aln+x+" O="+params.SMALTAligned+picardReport+x[0:len(x)-4]+"_validateReport.txt")
        #carryOverList_new.append(#Nothing because this is just a report which is not used again in pipeline)
        f.write('\n')
##        f2.write()
        pos +=1
    #print "picard validation sh file created as picardValidate.sh"
    f.close()
    ############################################3
    
    #This part creates the sam to bam .sh file
##    samArray = samFileList
         
    os.chdir(params.scripts_SMALT)
    samToBam = "5_createSamToBam.sh"
    samToBam_cleanup = "5_createSamToBam_cleanup.sh"
    f = open(samToBam,'w')
    f2 = open(samToBam_cleanup,'w')
    for x in carryOverList_old:   #samFileList:
        f.write(params.samtools+" view -Sb "+params.SMALTAligned_aln+x+" | "+params.samtools+" sort - "+params.SMALTAligned_aln+x[:(len(x)-4)]+"_sorted")
        f.write('\n')
        f2.write("rm "+params.SMALTAligned_aln+x[:(len(x)-4)]+"_sorted.bam\n")
        
    #print "sam to bam .sh file created as createSamToBam.sh"
    f.close()
    f2.close()

    #This part creates the indexing of the bam .sh file       
    os.chdir(params.scripts_SMALT)
    index = "6_indexBam.sh"
    index_cleanup = "6_indexBam_cleanup.sh"
    f = open(index,'w')
    f2 = open(index_cleanup,'w')
    for x in carryOverList_old:    #samFileList:
        f.write(params.samtools+" index "+params.SMALTAligned_aln+x[:(len(x)-4)]+"_sorted.bam")
        f.write('\n')
        f2.write("rm "+params.SMALTAligned_aln+x[:(len(x)-4)]+"_sorted.bam.bai\n")
        carryOverList_new.append(x[:(len(x)-4)]+"_sorted.bam")
    #print "bam indexing sh file created as indexBam.sh"
    f.close()
    f2.close()

    #getMappedReads
    #depends on samArray created above
    carryOverList_old = carryOverList_new
    #carryOverList_new = []
    os.chdir(params.scripts_SMALT)
    getMappedReads = "14_getMappedReads.sh"
    f = open(getMappedReads,'w')
##    getMappedReads_cleanup = "14_getMappedReads_cleanup .sh"
##    f2 = open(getMappedReads_cleanup ,'w')
    for x in carryOverList_old:    #samFileList:
        #f.write("samtools flagstat "+params.SMALTAligned_aln+x[:(len(x)-4)]+"_sorted.bam > "+params.SMALTAligned_aln+x[:(len(x)-4)]+"_samtools_stats.txt")
        f.write(params.samtools+" flagstat "+params.SMALTAligned_aln+x[:(len(x)-11)]+"_realigned_resorted_dedup.bam > "+params.SMALTAligned_aln+x[:(len(x)-11)]+"_realigned_resorted_dedup_samtools_stats.txt")
        f.write('\n')

        #Dont append to new list, only output is report file
    #print "samtools flagstat .sh file created as getMappedReads.sh "
    f.close()

 #########################
    #This creates the GATK sh files
    carryOverList_old = carryOverList_new
    carryOverList_new = []
    GATK = "7_1_GATK.sh"
    GATK2 = "7_2_GATK.sh"
    GATK_cleanup = "7_x_GATK_cleanup.sh"
##    referenceNOVO = loadReferenceFasta(params.smaltRef)
    os.chdir(params.scripts_SMALT)           
    f = open(GATK,'w')
    f2 = open(GATK2,'w')
    f3 = open(GATK_cleanup,'w')
    for x in carryOverList_old:   #samFileList:
        for ref in params.fastaList:
            referenceSMALT_shortName = getFastaName(ref)
            if referenceSMALT_shortName in x:
                referenceSMALT = ref
                break
            referenceSMALT = ref
##        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T RealignerTargetCreator -R "+params.smaltRef+referenceNOVO+" -I "+params.SMALTAligned_aln+x[:(len(x)-4)]+"_sorted"+".bam -o "+params.SMALTAligned_aln+x[:(len(x)-4)]+"_sorted.intervals")
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T RealignerTargetCreator -R "+params.reference+referenceSMALT+" -I "+params.SMALTAligned_aln+x+" -o "+params.SMALTAligned_aln+x[:(len(x)-4)]+".intervals")
        f.write('\n')
##        f2.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -fixMisencodedQuals -T RealignerTargetCreator -R "+params.reference+referenceSMALT+" -I "+params.SMALTAligned_aln+x+" -o "+params.SMALTAligned_aln+x[:(len(x)-4)]+".intervals")
##        f2.write('\n')
        f3.write("rm "+params.SMALTAligned_aln+x[:(len(x)-4)]+".intervals\n")
        carryOverList_new.append(x[:(len(x)-4)]+".intervals") #This is the interval file, not the main bam file, so keep both old and new lists
    #print "GATK intervals sh file created as GATK.sh"

    f.close()
    f2.close()
    f3.close()
    ########################3
    #This creates the realignment sh file
    os.chdir(params.SMALTAligned_aln) #~!~~~~~~~~~~~~~~~~~~~NOOOO use previous samFileList
    #samFileList = []

    '''
    carryOverList_new is the intervals file
    carryOverList_old is the bam file list.
    '''
    
##    for fileX in glob.glob('*_smaltSrtRG_sorted.bam'): 
##        samFileList.append(fileX)

    os.chdir(params.scripts_SMALT)
    Realignment = "8_1_Realignment.sh"
    Realignment2 = "8_2_Realignment.sh"
    Realignment_cleanup = "8_x_Realignment_cleanup.sh"
    temp = []     
    f = open(Realignment,'w')
    f2 = open(Realignment2,'w')
    f3 = open(Realignment_cleanup,'w')
    for pos in range(len(carryOverList_old)): #samFileList:
        for ref in params.fastaList:
            referenceSMALT_shortName = getFastaName(ref)
            if referenceSMALT_shortName in carryOverList_old[pos]:
                referenceSMALT = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T IndelRealigner -R "+params.reference+referenceSMALT+" -I "+params.SMALTAligned_aln+carryOverList_old[pos]+" -o "+params.SMALTAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bam"+" -targetIntervals "+params.SMALTAligned_aln+carryOverList_new[pos])
        f.write('\n')
##        f2.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -fixMisencodedQuals -T IndelRealigner -R "+params.reference+referenceSMALT+" -I "+params.SMALTAligned_aln+carryOverList_old[pos]+" -o "+params.SMALTAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bam"+" -targetIntervals "+params.SMALTAligned_aln+carryOverList_new[pos])
##        f2.write('\n')
        f3.write("rm "+params.SMALTAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bam\n")
        f3.write("rm "+params.SMALTAligned_aln+carryOverList_old[pos][:(len(carryOverList_old[pos])-4)]+"_realigned.bai\n")
        temp.append(carryOverList_old[pos][:len(carryOverList_old[pos])-4]+"_realigned.bam")
    #print "GATK realignment sh file created as Realignment.sh"
    f.close()
    f2.close()
    f3.close()
    carryOverList_new = temp

    ########################3
    #This creates the base quality recalibration sh file
    #creating temp bam array to save on space:
    if params.BQSRPossible:
        os.chdir(params.scripts_SMALT)
        carryOverList_old = carryOverList_new
        carryOverList_new = []
        f1 = open("9.1_baseQualRecalSMALT.sh",'w')
        f2 = open("9.2_baseQualRecalSMALT.sh",'w')
        f3 = open("9.1_baseQualRecalSMALT_cleanup.sh",'w')
        f4 = open("9.2_baseQualRecalSMALT_cleanup.sh",'w')
        for x in carryOverList_old: 
            for ref in params.fastaList:
                referenceSMALT_shortName = getFastaName(ref)
                if referenceSMALT_shortName in x:
                    referenceSMALT = ref
                    break
                referenceSMALT = ref
            f1.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T BaseRecalibrator -R "+params.reference+referenceSMALT+" -I "+params.SMALTAligned_aln+x+" -knownSites "+params.dbSNP+"dbSNP.vcf -o "+params.SMALTAligned_aln+x[:len(x)-20]+"recal_data.table\n")
            f2.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T PrintReads -R "+params.reference+referenceSMALT+" -I "+params.SMALTAligned_aln+x+" -BQSR "+params.SMALTAligned_aln+x[:len(x)-20]+"recal_data.table -o "+params.SMALTAligned_aln+x[:len(x)-20]+"realigned_recal.bam\n")
            carryOverList_new.append(x[:(len(x)-20)]+"realigned_recal.bam")
            f3.write("rm "+params.SMALTAligned_aln+x[:len(x)-20]+"recal_data.table\n")
            f4.write("rm "+params.SMALTAligned_aln+x[:len(x)-20]+"realigned_recal.bam\n")
            f4.write("rm "+params.SMALTAligned_aln+x[:len(x)-20]+"realigned_recal.bai\n")
            #print "picard sort sh file created as picardSort.sh"
        f1.close()
        f2.close()
        f3.close()
        f4.close()
    else:
        print "Skipping Base quality score recalibration due to missing dbSNP file"

        

    ########################3
    #This creates the picard sort sh file
    #depends on sam array
    # creating temp bam array to save on space:

    #os.chdir(params.SMALTAligned_aln)
    os.chdir(params.scripts_SMALT)
    if BQSRPossible:
        amount = 20
    else:
        amount = 21
    
    carryOverList_old = carryOverList_new
    carryOverList_new = []
    picardSort = "10_picardSort.sh"
    f = open(picardSort,'w')
    picardSort_cleanup = "10_picardSort_cleanup.sh"
    f2 = open(picardSort_cleanup,'w')
    for x in carryOverList_old: #samFileList:
##        f.write("java -jar "+params.SortSamDir+" I="+params.SMALTAligned_aln+x[:-4]+"_sorted_realigned.bam"+" O="
##        +params.SMALTAligned_aln+x[:(len(x)-20)]+"realigned_resorted.bam SORT_ORDER=coordinate VALIDATION_STRINGENCY=LENIENT")       
        f.write("java -Xmx"+params.mem+"g -jar "+params.SortSamDir+" I="+params.SMALTAligned_aln+x+" O="
        +params.SMALTAligned_aln+x[:(len(x)-amount)]+"_realigned_resorted.bam SORT_ORDER=coordinate VALIDATION_STRINGENCY=LENIENT")
        f.write('\n')
        f2.write("rm "+params.SMALTAligned_aln+x[:(len(x)-amount)]+"_realigned_resorted.bam\n")
        f2.write("rm "+params.SMALTAligned_aln+x[:(len(x)-amount)]+"_realigned_resorted.bam.bai\n")
        carryOverList_new.append(x[:(len(x)-amount)]+"_realigned_resorted.bam")
    #print "picard sort sh file created as picardSort.sh"
    f.close()
    f2.close()

    
    ########################3
    #This creates the RE-indexing of the bam files sh file
    os.chdir(params.scripts_SMALT)
    carryOverList_old = carryOverList_new
    carryOverList_new = []
    reIndexBamFiles = "11_reIndexBamFiles.sh"
    f = open(reIndexBamFiles,'w')
    reIndexBamFiles_cleanup = "11_reIndexBamFiles_cleanup.sh"
    f2 = open(reIndexBamFiles_cleanup,'w')
    
    for x in carryOverList_old: #bamFileList:
##        print [carryOverList_old]
##        raw_input()
##        f.write("samtools index "+params.SMALTAligned_aln+x[:(len(x)-20)]+"realigned_resorted.bam")               
        f.write(params.samtools+" index "+params.SMALTAligned_aln+x)               
        f.write('\n')
        f2.write("rm "+params.SMALTAligned_aln+x+".bai\n")
    #print "re-Index of bam files sh writen to reIndexBamFiles.sh"
    f.close()
    f2.close()


    ########################3
    #This creates the remove PCR duplicates sh file
    os.chdir(params.scripts_SMALT)
    removePCRDuplicates = "12_removePCRDuplicates.sh"
    f = open(removePCRDuplicates,'w')
    removePCRDuplicates_cleanup = "12_removePCRDuplicates_cleanup.sh"
    f2 = open(removePCRDuplicates_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
##        f.write(params.java7+" -jar "+params.markDuplicates+" I="+params.SMALTAligned_aln+x[:(len(x)-20)]+"realigned_resorted.bam"+" O="+params.SMALTAligned_aln+x[:(len(x)-20)]+"realigned_resorted_dedup.bam VALIDATION_STRINGENCY=LENIENT REMOVE_DUPLICATES=TRUE M=duplicate_metrics TMP_DIR=tmp ASSUME_SORTED=true > "+params.SMALTAligned_aln+x[:(len(x)-20)]+"realigned_resorted_rmdup.log")
        f.write(params.java7+" -jar "+params.markDuplicates+" I="+params.SMALTAligned_aln+x+" O="+params.SMALTAligned_aln+x[:(len(x)-4)]+"_dedup.bam VALIDATION_STRINGENCY=LENIENT REMOVE_DUPLICATES=TRUE M=duplicate_metrics TMP_DIR=tmp ASSUME_SORTED=true > "+params.SMALTAligned_aln+x[:(len(x)-4)]+"_rmdup.log")
        f.write('\n')
##        f2.write(params.SMALTAligned_aln+x[:(len(x)-4)]+"_dedup.bam")
        f2.write(params.SMALTAligned_aln+x[:(len(x)-4)]+"_rmdup.log")
    #print "picard remove pcr duplicates sh writen to removePCRDuplicates.sh"
    f.close()
    f2.close()
    

    ########################3
    #This creates the RE-indexing of the bam files sh file a 1nd time after the removal of pcr duplicates as in previous step
    os.chdir(params.scripts_SMALT)
    carryOverList_new = []
    reIndexBamFiles2 = "13_reIndexBamFiles2.sh"
    f = open(reIndexBamFiles2,'w')
    for x in carryOverList_old: #bamFileList:
##        f.write("samtools index "+params.SMALTAligned_aln+x[:(len(x)-20)]+"realigned_resorted_dedup.bam")              
        
        f.write(params.samtools+" index "+params.SMALTAligned_aln+x[:(len(x)-4)]+"_dedup.bam")              
        f.write('\n')
        carryOverList_new.append(x[:(len(x)-4)]+"_dedup.bam")
    #print "re-Index (FOR SECOND TIME) of bam files sh writen to reIndexBamFiles2.sh"
    f.close()

    ########################3
    #This creates the SNP calling sh file using GATK

    os.chdir(params.scripts_SMALT)
    snpCalling = "15_SNPCallingGATK.sh"
    carryOverList_old = carryOverList_new
##    carryOverList_new = []
    f = open(snpCalling,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceSMALT_shortName = getFastaName(ref)
            if referenceSMALT_shortName in carryOverList_old[pos]:
                referenceSMALT = ref
                break
##        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T UnifiedGenotyper -l INFO -R "+params.smaltRef+referenceNOVO+" -I "+params.SMALTAligned_aln+x[:(len(x)-20)]+"realigned_resorted_dedup.bam -o "+params.SMALTAligned+snpDir+x[:-32]+"_gatk_snps.vcf -stand_call_conf 50 -stand_emit_conf 10.0 -dcov 2000 > "+params.SMALTAligned+snpDir+x[:-32]+"_SNP_Genotype.log")  
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T UnifiedGenotyper -l INFO -R "+params.reference+referenceSMALT+" -I "+params.SMALTAligned_aln+x+" -o "+params.SMALTAligned+snpDir+x[:-46]+"_gatk_snps.vcf -stand_call_conf 50 -stand_emit_conf 10.0 -dcov 2000 > "+params.SMALTAligned+snpDir+x[:-46]+"_Genotype.log")  
        f.write('\n')
    #print "SNP calling using GATK sh writen to SNPCallingGATK.sh"
    f.close()

    ########################3
    #This creates the INDEL calling sh file using GATK
    '''
    os.chdir(params.scripts_SMALT)
    indelCalling = "15_INDELCallingGATK.sh"
     
    f = open(indelCalling,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceSMALT_shortName = getFastaName(ref)
            if referenceSMALT_shortName in x:
                referenceSMALT = ref
                break
        #f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T UnifiedGenotyper -l INFO --genotype_likelihoods_model INDEL -R "+params.smaltRef+referenceNOVO+" -I "+params.SMALTAligned_aln+x[:(len(x)-20)]+"realigned_resorted_dedup.bam -o "+params.SMALTAligned+indelDir+x[:-32]+"_gatk_indels.vcf -stand_call_conf 50 -stand_emit_conf 10.0 -dcov 2000 > "+params.SMALTAligned+indelDir+x[:-32]+"_Genotype.log")  
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T UnifiedGenotyper -l INFO --genotype_likelihoods_model INDEL -R "+params.reference+referenceSMALT+" -I "+params.SMALTAligned_aln+x+" -o "+params.SMALTAligned+indelDir+x[:-46]+"_gatk_indels.vcf -stand_call_conf 50 -stand_emit_conf 10.0 -dcov 2000 > "+params.SMALTAligned+indelDir+x[:-46]+"_Indel_Genotype.log")  
        f.write('\n')
    #print "SNP calling using GATK sh writen to INDELCallingGATK.sh"
    f.close()
    '''
    
    ########################3
    #This creates the SAMTOOLS SNP CALLER sh file 
    os.chdir(params.scripts_SMALT)
    snpCalling = "15_2_VARIANT_CALLING_SAMTOOLS.sh"
     
    f = open(snpCalling,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceSMALT_shortName = getFastaName(ref)
            if referenceSMALT_shortName in x:
                referenceSMALT = ref
                break
        f.write(params.samtools+" mpileup -ug -f "+params.reference+referenceSMALT+" "+params.SMALTAligned_aln+x+" | "+params.bcftools+" view -bcvg | "+params.bcftools+" view > "+params.SMALTAligned+snpDir+x[:-4]+"_VARIANTS_SAMTOOLS.vcf"+"\n")
    #print "SNP calling using GATK sh writen to SNPCallingGATK.sh"
    f.close()
    
    
    ########################3
    #This creates the SNP HAPLOTYPE CALLER sh file using GATK
    os.chdir(params.scripts_SMALT)
    snpCalling = "15_SNP_INDEL_Calling_GATK_HAPLOTYPECALLER.sh"
     
    f = open(snpCalling,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceSMALT_shortName = getFastaName(ref)
            if referenceSMALT_shortName in x:
                referenceSMALT = ref
                break
##        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T HaplotypeCaller -nct "+str(coreSplit[1])+" -R "+params.smaltRef+referenceNOVO+" -I "+params.SMALTAligned_aln+x[:(len(x)-20)]+"realigned_resorted_dedup.bam -o "+params.SMALTAligned+snpDir+x[:-32]+"_gatk_HC_snps.vcf -stand_call_conf 30 -stand_emit_conf 10.0 > "+params.SMALTAligned+snpDir+x[:-32]+"_Genotype.log")  
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T HaplotypeCaller -nct "+str(params.coreSplit[2])+" -R "+params.reference+referenceSMALT+" -I "+params.SMALTAligned_aln+x+" -o "+params.SMALTAligned+snpDir+x[:-4]+"_gatk_HC_snps.vcf -stand_call_conf 30 -stand_emit_conf 10.0 > "+params.SMALTAligned+snpDir+x[:-4]+"_SNP_HC_Genotype.log")  
        f.write('\n')
    #print "SNP calling using GATK sh writen to SNPCallingGATK.sh"
    f.close()

    ###############################
    # GATK GEN COVERAGE
    ###############################
    os.chdir(params.scripts_SMALT)
    GenomeCoverage = "16_GenomeCoverage.sh"
    f = open(GenomeCoverage,'w')
    GenomeCoverage_cleanup = "16_GenomeCoverage_cleanup.sh"
    f2 = open(GenomeCoverage_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceSMALT_shortName = getFastaName(ref)
            if referenceSMALT_shortName in x:
                referenceSMALT = ref
                break
        f.write(params.java7+" -Xmx"+params.mem+"g -jar "+params.gatkHomeDir+" -T DepthOfCoverage -R "+params.reference+referenceSMALT+" -o "+params.SMALTAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov.txt -I "+params.SMALTAligned_aln+x) #+" -nt"+str(coreSplit[0]))
        f.write('\n')
        f2.write("rm "+params.SMALTAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov.txt\n")
    #print "genome coverage bed tools sh file created as GenomeCoverage.sh"
    f.close()
    f2.close()
    
    ###############################
    # BED TOOL GEN COVERAGE
    ###############################
    os.chdir(params.scripts_SMALT)
    GenomeCoverage = "16_2_GenomeCoverage.sh"
    f = open(GenomeCoverage,'w')
    GenomeCoverage_cleanup = "16_2_GenomeCoverage_cleanup.sh"
    f2 = open(GenomeCoverage_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        for ref in params.fastaList:
            referenceSMALT_shortName = getFastaName(ref)
            if referenceSMALT_shortName in x:
                referenceSMALT = ref
                break

        f.write(params.bedtools+"genomeCoverageBed -bga -ibam "+params.SMALTAligned_aln+x+" -g "+params.reference+referenceSMALT+" > "+params.SMALTAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov_bed.txt")
        f.write('\n')
        f2.write("rm "+params.SMALTAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov_bed.txt\n")
    #print "genome coverage bed tools sh file created as GenomeCoverage.sh"
    f.close()
    f2.close()
    
 ########################3
    #This creates the regions with zero coverage sh file
    os.chdir(params.scripts_SMALT)
    ZeroCov = "17_ZeroCov.sh"
    f = open(ZeroCov,'w')
    ZeroCov_cleanup = "17_ZeroCov_cleanup.sh"
    f2 = open(ZeroCov_cleanup,'w')
    for x in carryOverList_old: #bamFileList:
        #f.write("grep -w 0$ "+params.SMALTAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov_bed.txt > "+params.SMALTAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov=0.txt") 
        f.write("awk 'NF && $4<2' "+params.SMALTAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov_bed.txt > "+params.SMALTAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov=0.txt") 
        f.write('\n')
        f2.write("rm "+params.SMALTAligned+genomeCovDir+x[:(len(x)-4)]+"_genomecov=0.txt\n")

    #print "zero coverage sh file written as ZeroCov.sh"
    f.close()
    f2.close()
    return


def referenceSH():
    os.chdir(params.scripts_SMALT)
    
    f = open("indexSMALTRef.sh",'w')
    f.write(params.smaltBinary+" index -k 20 -s 1 "+params.smaltRef+"h37smaltrefk20s1 "+params.smaltRef+"H37Rv_4411532_fixed.fasta")
    f.close()

def spolpred(params,trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE, fileArraySE, fileArrayPE):
    os.chdir(params.scripts_StrainIdentification)
    f = open("spolpred.sh",'w')
    pos = 0
    count = 0
    while pos <= len(fileArrayPE)-2:
        if fileArrayPE[pos].endswith(".gz"):
            f.write(params.spolpred+" "+params.fastQ+fileArrayPE[pos]+" -l 100 -o "+params.mapperOut+spolPredOut+str(fileArrayPE[pos][:-9])+"_spolPredOut.txt -d off &\n")                
        else:
            f.write(params.spolpred+" "+params.mapperOut+spolPredOut+str(fileArrayPE[pos])+" -l 100 -o "+params.mapperOut+spolPredOut+str(fileArrayPE[pos][:-8])+"_spolPredOut.txt -d off &\n")
        count += 1
        if count >= 2: #int(params.cores):
            count = 0
            f.write("wait\n")
        pos += 2       
    pos = 0
    threadCount = 0
    flag = False
    for pos in range(len(fileArraySE)):
        flag = False
        if fileArraySE[pos].endswith(".gz"):
            f.write(params.spolpred+" "+params.fastQ+fileArraySE[pos]+" -d off -l 100 -o "+params.mapperOut+spolPredOut+str(fileArraySE[pos])[:-9]+"_spolPredOut.txt -d off&\n")
        else:
            f.write(params.spolpred+" "+params.fastQ+fileArraySE[pos]+" -d off -l 100 -o "+params.mapperOut+spolPredOut+str(fileArraySE[pos])[:-6]+"_spolPredOut.txt -d off&\n")
        if threadCount >= 2: #int(params.cores):
            f.write("wait\n")
            flag= True
            threadCount = 0
        threadCount +=1
    if not flag:
        f.write("wait\n")

    f.close()
    return

def spolpred_custom(inputDir,outputDir):

    cleanedUpFileData = partitionFastQList(inputDir)
    fileArraySE = cleanedUpFileData[0]
    fileArrayPE = cleanedUpFileData[1]

    fileArraySE , trimArraySE , IDSE, SMSE = GenerateCleanFileNamesX(fileArraySE) #change dirX to params.trimmedFastQ
    fileArrayPE , trimArrayPE , IDPE, SMPE = GenerateCleanFileNamesX(fileArrayPE) #change dirX to params.trimmedFastQ

    fileArray = fileArraySE+fileArrayPE
    trimArray = trimArraySE+trimArrayPE
    ID = IDSE + IDPE
    SM = SMSE + SMPE
   
    os.chdir(params.scripts_StrainIdentification)
    f = open("spolpred1.sh",'w')

    pos = 0
    count = 0
    while pos <= len(fileArrayPE)-2:
        f.write("perl "+params.shuffle+" "+inputDir+str(fileArrayPE[pos])+" "+inputDir+str(fileArrayPE[pos+1])+" "+outputDir+str(fileArrayPE[pos+1])[:-6]+"_combined.fastq &\n")    
        count += 1
        if count == params.cores:
            count = 0
            f.write("wait\n")
        pos += 2    
    f.close()
    #return

    f = open("spolpred2.sh",'w')
    pos = 0
    threadCount = 0

    #NOTE these come from trimmed fastq, and are all single ends, did not perform combine reads on these so doing spolpred on SE
    flag = False
    for pos in range(len(fileArraySE)):
        flag = False
        f.write(params.spolpred+" "+inputDir+str(fileArraySE[pos])+" -l 100 -o "+outputDir+str(fileArraySE[pos])[:-6]+"_spolPredOut.txt -d off&\n")
        if threadCount == params.cores:
            f.write("wait\n")
            flag= True
            threadCount = 0
        threadCount +=1
    if not flag:
        f.write("wait\n")

    #NOTE these come from RESULTS/SpolPredOut/*combined files, these are PE files condenced into one file, this gives spolpred more reads
    #to work with and thus more accuracy hopefully. 
    pos = 0
    flag = False
    while pos <= len(fileArrayPE)-2:
        flag = False
        if IDPE[pos] <> IDPE[pos+1]:
            print "error - the two files are not matching - cannot proceed - please manually quit and fix the error"
            print ID[pos],'and',ID[pos+1]
            raw_input("error - the two files are not matching - cannot proceed - please manually quit and fix the error")
            exit
        if outputDir[-1] == "/" and outputDir[-2] == "/":
            params.mapperOut = params.mapperOut[:-1]
        f.write(params.spolpred+" "+outputDir+str(fileArrayPE[pos+1])[:-6]+"_combined.fastq"+" -l 100 -o "+outputDir+spolPredOut+str(fileArrayPE[pos+1])[:-6]+"_spolPredOut.txt -d off& > spolpred_log.txt\n")
        pos += 2

        if threadCount == params.cores:
            flag = True
            f.write("wait\n")
            threadCount = 0
        threadCount +=1
    if not flag:
        f.write("wait\n")
    f.close()

    print "spolpred sh written"
    f.close()
    return



###############################################################################
#END mapper scripts
###############################################################################






###############################################################################
#### TRIMMING
###############################################################################

def partitionFastQList(fastQDir):
    def sampleName(fileX):
        return fileX.split("_")[0]
    data = []
    os.chdir(fastQDir)
    for fileX in os.listdir(fastQDir):
        if fileX.endswith(".fastq") or fileX.endswith("fastq.gz"):
            data.append(fileX)
    data.sort()
    singleData = []
    pairedData = []
    previous = ''
    firstEntry = True
    pos = 0
    flag = True
    while pos <= len(data)-1:
        if pos == 0 or flag:
            previous = data[pos]
            if pos == len(data)-1:
                singleData.append(previous.replace("\n",""))
            pos+=1
            flag = False
            continue

        current = data[pos]
        if sampleName(current) == sampleName(previous):
            pairedData.append(previous.replace("\n",''))
            pairedData.append(current.replace("\n",''))
            pos+=1
            flag=True
            continue
        else:
            if pos == len(data) -1:
                singleData.append(previous.replace("\n",''))
                singleData.append(current.replace("\n",''))
                pos+=2
                continue
            else:
                singleData.append(previous.replace("\n",''))
                flag=True
                continue
    return [singleData, pairedData]

def noTrimmingFileNamePartition(): 

    outputTrimmedFilesSE = []
    outputTrimmedFilesPE = []
    cleanedUpFileData = partitionFastQList(params.fastQ)

    fileArraySE = cleanedUpFileData[0]
    fileArrayPE = cleanedUpFileData[1]
    
    IDSE, SMSE, LBSE, barcodesSE, trimArraySE = extractFileNameData(fileArraySE) #ID,SM,LB,barcodes,trimArray 
    IDPE, SMPE, LBPE, barcodesPE, trimArrayPE = extractFileNameData(fileArrayPE)  #ID,SM,LB,barcodes,trimArray
    if fileArraySE <> []:
        print "files detected as SE "
        print fileArraySE
    if fileArrayPE <> []:
        print "files detected as PE:"
        print fileArrayPE
    
    if params.reads == "singleEnd": 
        fileArray = fileArraySE
        trimArray = trimArraySE
        ID = IDSE
        SM = SMSE
    elif params.reads == "pairedEnd":
        fileArray = fileArrayPE
        trimArray = trimArrayPE
        ID = IDPE
        SM = SMPE
    elif params.reads == "mixed":
        fileArray = fileArraySE+fileArrayPE
        trimArray = trimArraySE+trimArrayPE
        ID = IDSE + IDPE
        SM = SMSE + SMPE
    pos = 0
    for fileNameX in fileArraySE:
        if fileNameX.lower().endswith(".fastq.gz"):
            outputName = fileNameX #fileNameX[:-9]+"_trimSE.fastq"
        elif fileNameX.lower().endswith(".fastq"):
            outputName = fileNameX #fileNameX[:-6]+"_trimSE.fastq"
        outputTrimmedFilesSE.append(outputName) #+".gz")
    fileArray = fileArrayPE
    ID = IDPE
    while pos <= len(fileArray)-2:
        if ID[pos] <> ID[pos+1]:
            print "filename matching error - the two files are not matching - cannot proceed - please manually quit and fix the error"
            raw_input()
            exit
        if fileArray[pos].lower().endswith("fastq.gz"):
            outputName1 = fileArray[pos] #fileArray[pos][:-9]+"_trimPE.fastq"
        elif fileArray[pos].lower().endswith("fastq"):
            outputName1 = fileArray[pos] #fileArray[pos][:-6]+"_trimPE.fastq"
        if fileArray[pos+1].lower().endswith("fastq.gz"):
            outputName2 = fileArray[pos] #fileArray[pos+1][:-9]+"_trimPE.fastq"
        elif fileArray[pos+1].lower().endswith("fastq"):
            outputName2 = fileArray[pos] #fileArray[pos+1][:-6]+"_trimPE.fastq"
        outputTrimmedFilesPE.append(outputName1) #+".gz")
        outputTrimmedFilesPE.append(outputName2) #+".gz")
        pos +=2
    print "no-trimming filename partition complete"
    return outputTrimmedFilesSE, outputTrimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE, fileArraySE, fileArrayPE


def trimmomaticMulti(): 
    outputTrimmedFilesSE = []
    outputTrimmedFilesPE = []
    cleanedUpFileData = partitionFastQList(params.fastQ)

    fileArraySE = cleanedUpFileData[0]
    fileArrayPE = cleanedUpFileData[1]
    
    IDSE, SMSE, LBSE, barcodesSE, trimArraySE = extractFileNameData(fileArraySE) #ID,SM,LB,barcodes,trimArray 
    IDPE, SMPE, LBPE, barcodesPE, trimArrayPE = extractFileNameData(fileArrayPE)  #ID,SM,LB,barcodes,trimArray
    if fileArraySE <> []:
        print "files detected as SE "
        print fileArraySE
    if fileArrayPE <> []:
        print "files detected as PE:"
        print fileArrayPE
    
    if params.reads == "singleEnd": 
        fileArray = fileArraySE
        trimArray = trimArraySE
        ID = IDSE
        SM = SMSE
    elif params.reads == "pairedEnd":
        fileArray = fileArrayPE
        trimArray = trimArrayPE
        ID = IDPE
        SM = SMPE
    elif params.reads == "mixed":
        fileArray = fileArraySE+fileArrayPE
        trimArray = trimArraySE+trimArrayPE
        ID = IDSE + IDPE
        SM = SMSE + SMPE

    os.chdir(params.main)

    try:
        os.makedirs(params.scripts_trimming)
    except:
        print "Using previously existing temporary trimming script directory"
    try:
        os.makedirs(params.trimmedFastQ)
    except:
        print "Using previously existing output folder for trimmed FASTQ files" 
        
    os.chdir(params.trimmedFastQ)
    try:
        os.makedirs("log/")
        os.makedirs("unpaired/")
    except:
        print "The log and unpaired trimming directories already exists, proceeding"       
    trimmomaticSH = "autoTrim.sh"
    os.chdir(params.scripts_trimming)
    f = open(trimmomaticSH,'w') 
    pos = 0

    for fileNameX in fileArraySE:
        if fileNameX.lower().endswith("fastq.gz"):
            outputName = fileNameX[:-9]+"_trimSE.fastq"
        elif fileNameX.lower().endswith("fastq"):
            outputName = fileNameX[:-6]+"_trimSE.fastq"
        outputTrimmedFilesSE.append(outputName+".gz")
        f.write("java -jar "+params.trimOMatic+" SE -threads "+str(params.cores)+" -phred33 "+params.fastQ+str(fileNameX)+" "+params.trimmedFastQ+outputName+" ILLUMINACLIP:"+params.tools+"illumina_adapters.fna.fasta:"+params.trimOMaticParams+"\n")
        f.write("gzip "+params.trimmedFastQ+outputName+"\n")    
        f.write("rm "+params.trimmedFastQ+outputName+"\n")                                                                                                                                                                                                       
    fileArray = fileArrayPE
    ID = IDPE
   
    while pos <= len(fileArray)-2:
        if ID[pos] <> ID[pos+1]:
            print "trimmomaticSH error - the two files are not matching - cannot proceed - please manually quit and fix the error"
            raw_input()
            exit
        if fileArray[pos].lower().endswith("fastq.gz"):
            outputName1 = fileArray[pos][:-9]+"_trimPE.fastq"
        elif fileArray[pos].lower().endswith("fastq"):
            outputName1 = fileArray[pos][:-6]+"_trimPE.fastq"
        if fileArray[pos+1].lower().endswith("fastq.gz"):
            outputName2 = fileArray[pos+1][:-9]+"_trimPE.fastq"
        elif fileArray[pos+1].lower().endswith("fastq"):
            outputName2 = fileArray[pos+1][:-6]+"_trimPE.fastq"
        outputTrimmedFilesPE.append(outputName1+".gz")
        outputTrimmedFilesPE.append(outputName2+".gz")
        f.write("java -jar "+params.trimOMatic+" PE -threads "+str(params.cores)+" -phred33 "+params.fastQ+str(fileArray[pos])+" "+params.fastQ+str(fileArray[pos+1])+" "+params.trimmedFastQ+outputName1+" "+params.trimmedFastQ+"unpaired/"+outputName1+" "+params.trimmedFastQ+outputName2+" "+params.trimmedFastQ+"unpaired/"+outputName2+" ILLUMINACLIP:"+params.tools+"illumina_adapters.fna.fasta:"+params.trimOMaticParams+"\n") #params are usually: 2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:20 MINLEN:36
        f.write("gzip -f "+params.trimmedFastQ+outputName1+"\n")
        f.write("gzip -f "+params.trimmedFastQ+outputName2+"\n")   
        f.write("rm "+params.trimmedFastQ+outputName1+ " &\n")
        f.write("rm "+params.trimmedFastQ+outputName2+" &\n")                
        pos +=2
    f.write("wait\n")
    f.close()

    #print "trimmomatic mixed-mode list created as: ",trimmomaticSH
    f.close()
    return outputTrimmedFilesSE, outputTrimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE, fileArraySE, fileArrayPE
    
############################################    

def fastXToolKitTrim():
    outputTrimmedFilesSE = []
    outputTrimmedFilesPE = []
    cleanedUpFileData = partitionFastQList(params.fastQ)

    fileArraySE = cleanedUpFileData[0]
    fileArrayPE = cleanedUpFileData[1]
    
    IDSE, SMSE, LBSE, barcodesSE, trimArraySE = extractFileNameData(fileArraySE) #ID,SM,LB,barcodes,trimArray 
    IDPE, SMPE, LBPE, barcodesPE, trimArrayPE= extractFileNameData(fileArrayPE)  #ID,SM,LB,barcodes,trimArray

    print "files detected as SE "
    print fileArraySE
    print "files detected as PE:"
    print fileArrayPE
    
    def calculateTrim(strList,cutOffList):
        trimValueList = []
        pos = 0      
        valueList = []
        for stringX in strList:
            valueList.append(stringX)

        pos = 0
        for x in valueList:
            print int(cutOffList[pos]), "and ", int(x), " trim ", (int(cutOffList[pos]) - int(x))
            trimValueList.append(int(cutOffList[pos]) - int(x))
            pos+=1
        return trimValueList
    ################################################################

    fileArray = []
    values = []
    totalLoaded = 0
    roots = []
    readLength =0
    readLenArray = []

    lookFor = "fastqc_data.txt"
    print "Working directory: ",params.fastQCStatsDir

    for root, directories,files in os.walk(params.fastQCStatsDir):
        for f in files:     
            if f == lookFor:
                totalLoaded +=1
                with open(os.path.join(root, f), 'r') as tempFile:
                    cutOff = None
                    lineNum = 0
                    
                    for line in tempFile:
                        lineNum += 1
                    
                        if lineNum>15 and ">>END_MODULE" in line:
                            break                   
                        wordList = line.strip()
                        if lineNum == 9:
                            s = wordList.split()
                            readLenArray.append(int(s[2]))                                        
                    
                        if lineNum == 4:
                            fileName = wordList[9:]
                            fileArray.append(fileName)
                        if lineNum >= 14 and wordList <> ">>END_MODULE":
                            s = wordList.split()
   
##                            if trimmingMethod == "Strict":
##                                if float(s[3]) >= 20.0 and float(s[2]) >=  20.0 and float(s[1]) >=  20.0: #1 is the mean, 2 is the median and 3 is my previous yellow box cutoff
##                                    cutOff = (s[0])
##                                else:
##                                    break                               
                            if params.trimMethod == "Fixed_Amount_Trim": 
                                if float(s[2]) >=  20.0 and float(s[1]) >=  20.0: #1 is the mean, 2 is the median and 3 is my previous yellow box cutoff
                                    cutOff = (s[0])
                                else:
                                    break
                    values.append(cutOff)
                tempFile.close()

    print "Loading FastQC summary statistics, a total of ",totalLoaded,"files were loaded"
    trimList = calculateTrim(values,readLenArray) 
    #print trimList

    autoTrim = "autoTrim.sh"
    os.chdir(params.scripts_trimming)
    f = open(autoTrim,'w')
    pos = 0

    for x in trimList:
        outputName = ""
        if fileArray[pos] in fileArraySE:
            if fileArray[pos].lower().endswith("fastq.gz"):
                outputName = fileArray[pos][:-9]+"_trimSE"+str(x)+".fastq"
                outputTrimmedFilesSE.append(outputName+".gz")
            elif fileArray[pos].lower().endswith("fastq"):
                outputName = fileArray[pos][:-6]+"_trimSE"+str(x)+".fastq"
                outputTrimmedFilesSE.append(outputName+".gz")

        elif fileArray[pos] in fileArrayPE:
            if fileArray[pos].lower().endswith("fastq.gz"):
                outputName = fileArray[pos][:-9]+"_trimPE"+str(x)+".fastq"
                outputTrimmedFilesPE.append(outputName+".gz")
                    
            elif fileArray[pos].lower().endswith("fastq"):
                outputName = fileArray[pos][:-6]+"_trimPE"+str(x)+".fastq"
                outputTrimmedFilesPE.append(outputName+".gz")
        else:
            raw_input("File could not be matched to either PE or SE: "+fileArray[pos]+" please rename file according to suggested naming convention listed in manual")
            exit()

        if int(x) == 0: #if you dont have to trim, then just copy file over 
            writeThis = "cp "+params.fastQ+str(fileArray[pos])+' '+params.trimmedFastQ+outputName
        else:
            writeThis = params.fastXTrimmer+ ' -t '+ str(x)+' -Q 33 -i '+ params.fastQ+str(fileArray[pos])+' -o '+params.trimmedFastQ+outputName
        f.write(writeThis+"\n")
        f.write("gzip "+params.trimmedFastQ+outputName+" &\n")
                    
        f.write() 
        pos +=1
        
    print "Trimming script file created as",autoTrim
    f.write("wait\n")
    f.close()
    return outputTrimmedFilesSE, outputTrimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE, fileArraySE, fileArraySE

##########################################################
#END TRIMMING
##########################################################


##########################################################
#Check if EMBL and FASTA files match to support annotation
##########################################################
def validateRefFiles(refDir,userRef):
    '''
    each fasta file requires a matching embl file
    Assumption using only one reference fasta file
    May use multiple embl files for annotation 
    '''
    BQSRPossible = False
    os.chdir(refDir)
    fastaList = []
    emblList = []
    try:
        for fileX in os.listdir(refDir+"/"+userRef+"/dbSNP"):
            fileName = fileX.lower()
            if fileName.lower() == "dbsnp.vcf":
                BQSRPossible = True
    except:
        BQSRPossible = False
        

            
    for fileX in os.listdir(refDir+"/"+userRef+"/FASTA"):
        fileName = fileX.lower()
        if fileName.endswith(".gz"):
            print "Error, the input fasta file is in compressed format:",fileX
            print "Please ensure the reference fasta and embl files are uncompressed"
            raw_input("Press enter to exit the program")
            exit()
        if fileName.endswith(".fasta") or fileName.endswith(".fa"):
            print "Evaluating", fileX
            fileExt = fileX.split(".")[-1]
##            print fileExt
            tempFasta = fileX[:len(fileX)-len(fileExt)-1]
##            print "adding", [tempFasta]
            fastaList.append(tempFasta)
    fastaList.sort()
    for fileX in os.listdir(refDir+"/"+userRef+"/EMBL"):
        print "Evaluating", fileX
        fileName = fileX.lower()
        if fileName.endswith(".gz"):
            print "Error, the input fasta file is in compressed format:",fileX
            print "Please ensure the reference fasta and embl files are uncompressed"
            raw_input("Press enter to exit the program")
            exit()
        if fileName.endswith(".embl") or fileName.endswith(".dat"):
            fileExt = fileX.split(".")[-1]
##            print fileExt
            tempEmbl = fileX[:len(fileX)-len(fileExt)-1]
##            print "adding", [tempEmbl]
            emblList.append(tempEmbl)
    emblList.sort()
            
    if len(fastaList) == 0:
        print "Error, no fasta files found in", refDir+"/"+userRef+"/FASTA"
        raw_input("Press enter to exit the program")
        exit()
        return False, False
    if len(emblList) == 0:
        print "Error, no embl files found in", refDir+"/"+userRef+"/EMBL"
        print "This will not allow annotation of variants..."
        raw_input("Press enter to contiune without annotation ability.")
        #exit()
        return False , False
    flag = True
    for fileX in fastaList:
        if fileX not in emblList:
            print "Error matching FASTA file to EMBL file"
            print "The fasta file '",fileX,"' does not have a matching EMBL file"
            #raw_input("Press enter to exit the program")
            flag = False, False

    for fileX in emblList:
        if fileX not in fastaList:
            print "Error matching EMBL file to FASTA file"
            print "The EMBL file '",fileX,"' does not have a matching FASTA file"
            #raw_input("Press enter to exit the program")
            flag = False, False
    if flag:
        print "All FASTA files matched to corresponding EMBL files"
    return flag, BQSRPossible, emblList[0]

def available_cpu_count():
    """ Number of available virtual or physical CPUs on this system, i.e.
    user/real as output by time(1) when called with an optimally scaling
    userspace-only program"""

    # cpuset
    # cpuset may restrict the number of *available* processors
    

    # Python 2.6+
    try:
        import multiprocessing
        return multiprocessing.cpu_count()
    except (ImportError, NotImplementedError):
        pass

    #http://code.google.com/p/psutil/
    try:
        import psutil
        return psutil.cpu_count()   # psutil.NUM_CPUS on old versions
    except (ImportError, AttributeError):
        pass
    print "Error auto-detecting system CPU thread count."
    return "2"
    
    try:
        m = re.search(r'(?m)^Cpus_allowed:\s*(.*)$',
                      open('/proc/self/status').read())
        if m:
            res = bin(int(m.group(1).replace(',', ''), 16)).count('1')
            if res > 0:
                return res
    except:
        pass


def available_memory():
    temp =""
    try:
        p = Popen(['free', '-m'], stdin=PIPE, stdout=PIPE, stderr=PIPE)
        output, err = p.communicate("input data that is passed to subprocess' stdin")
        
        temp = ""
        prev = " "
        for char in output:
            if char == prev and prev == " ":
                continue
            else:
                temp += char
                prev = char

        ##print temp
        temp = temp.split("\n")[1]
        temp = temp.split(" ")
        temp = temp[1]
    except:
        print "error detecting free memory, re-attempting"
    if temp == "":
        try:
            p = Popen(['free', '-m'], stdin=PIPE, stdout=PIPE, stderr=PIPE)
            output, err = p.communicate("input data that is passed to subprocess' stdin")

##            raw_input()
            rc = p.returncode
            temp = output.split("\n")
##            print temp
            temp = temp[2]
##            print temp
            temp=temp.replace(" ","*")
##            print temp
            temp=temp.split("*")
            print temp
            temp = temp[-1]
##            raw_input("error is here")
        except:
            print "Error auto-detecting system memory."
            return "4000"
    if temp == "":
        print "Error auto-detecting system memory"
        return "4000" #default
    return temp





#################################################################################################################

def checkFileNames(fastQDir,readsType):
    print "------------------------------------"
    print "checking validity of file names in these given directories:"
    print "INPUT DIR:", fastQDir
    print "reads type", readsType
##    print "OUTPUTDIR:",settingsList[3]
##    print "SETTINGS:", settingsList
    print "------------------------------------"
    count = 0
    #settingsList=['4', '2954', '/home/pagit/MASTER', 'Illumina', 'pairedEnd']
    fileList = []
    for tempFile in os.listdir(fastQDir):
        if tempFile.endswith(".fastq") or tempFile.endswith("fastq.gz"):
            fileList.append(tempFile)
            count += 1
    if count == 0:
        return False
    print "a total of ",len(fileList),"files were found in ",fastQDir
    try:
        for x in fileList:
            f = open(x,'r')
            print "checking read access to:",f.name
            f.close()
    except:
        print "error opening fastQ files in",fastQDir,"ensure file permissions allow read access"
        return False

    if readsType == 'pairedEnd':
        print "now matching paired-end file mathcing..."
        fileList.sort()
        pos = -2
        while True: 
            pos += 2
            if ("R1" in fileList[pos] or "read1" in str.lower(fileList[pos])) and ("R2" in fileList[pos+1] or "read2" in str.lower(fileList[pos+1])):
                print fileList[pos],"matched to ",fileList[pos+1]
            else:
                print "error matching paired end files, please rename files to use unique identifier followed by '_' and R1 or R2 in case of "
                print "paired-end files or simply unique identifier in case of single end files."
                print "example: Single end filename --> R1234.fastq or R1234.fastq.gz"
                print "example: Paired end files --> R1234_R1.fastq.gz and R1234_R2.fastq.gz" 
                print "The problematic files were:",fileList[pos],"and",fileList[pos+1]
                print "make sure to select mixed mode when running paired end and single end files together."
                return False
            if pos+2 == len(fileList):
                break
        print "If the fastQ files do not appear to correctly match, you should rename the files as described in the manual"
    return True
   
#####################################################################################################
def setup(cpu_count,memory,skipSetup):
    '''
    create a usersettings file or read an existing one.
    Store info for input fastq, hardware, output folder, selected reference
    '''
    annotationAllowed = True
    globalDir = os.getcwd()
    if "/BIN" in globalDir:
        globalDir = globalDir[:-4]
    refDir = globalDir+"/Reference"
    outputDir = globalDir+"/Output" #"\home\user\NGS-DATA\Output"
    binDir = globalDir+"/BIN"
    os.chdir(globalDir)
    skip = "N"
    try:
        f = open('userSettings.txt','r') 
    except:
        print "No user settings file found, creating new config file..."
        f=open('userSettings.txt','w')
        f.write("CPU_CORES\t"+str(cpu_count)+"\n")
        f.write("SYSTEM_MEMORY\t"+str(memory)+"\n") 
        f.write("FASTQ_DIRECTORY\t"+outputDir[:-6]+"FASTQ\n")
        f.write("OUTPUT_DIRECTORY\t"+outputDir+"\n")
##        f.write("PLATFORM\tIllumina\n")
        f.write("READS_TYPE\tmixedReads\n")
        f.write("reference\tMycobacteriumTuberculosis_H37Rv\n")
        f.write("trimmingMode\tQuality_Trim\n")
        f.close() #Aleady have user settings stored, check integrity

    f = open('userSettings.txt','r')
    print "-"*70
    print "Previous run settings:"
    print "-"*70
    count = 0
    tempDir1 = ""
    tempDir2 = ""
    for x in f: #load settings from userSettings.txt
        count+=1
##        print count
        print x
        if count == 1:
            temp = x.split("\t")[1]
            temp = temp.replace("\n","")
            prev_cpu_count = temp    
        if count == 2:
            temp = x.split("\t")[1]
            temp = temp.replace("\n","")
            prev_memory = int(temp)      
        if count == 3:
            tempDir1 = x.split("\t")[-1].strip()
            if tempDir1[-1] <> "/":
                tempDir1+="/"
            inputDir = tempDir1
        if count == 4:
            tempDir2 = x.split("\t")[-1].strip()
            if tempDir2[-1] <> "/":
                tempDir2+="/"
            outputDir = tempDir2
        if count == 5:
            temp = x.split("\t")[1]
            temp = temp.replace("\n","")
            reads_type = temp
            
        if count == 6:
            temp = x.split("\t")[1]
            temp = temp.replace("\n","")
            user_ref = temp
            
        if count == 7:
            temp = x.split("\t")[1]
            temp = temp.replace("\n","")
            trimmingMode = temp
    f.close()
    print "Input Folder containing FASTQ files: ",[tempDir1]
    print
    print "Output Folder: ",[tempDir2]
    print "-"*70
    skip = "?"
    try:
        os.chdir(tempDir1)
    except:
        print "could not access FASTQ directory:",tempDir1,"user will be prompted to provide valid path..."
        skip = "N"  
    try:
        os.chdir(tempDir2)
    except:
        print "could not access valid output directory",tempDir2,"user will be prompted to provide valid path..."
        skip = "N"
             
    if skip == "?": #if the user has a choice since the input and outpur paths exist
        while True and not skipSetup:
            skip = raw_input("are these the correct settings to use?  Y/N,  Q = quit:")
            if skip not in ["Y","y","N","n","Q","q"]:
                continue
            else:
                break
    if skip == "Q" or skip == "q":
        print "halting program"
        exit()
    if skip == "Y" or skip =="y" or skipSetup:
        print "Not altering current settings, exiting settings"
        annotationAllowed, BQSRPossible, emblFile = validateRefFiles(refDir,user_ref)
        if not annotationAllowed:
            print "Please refer to the manual for providing a suitable reference folder."
            print "Variant annotation will NOT be supported if you deceide to continue..."
            while True:
                ans = raw_input("press Q to quit or C to continue without annotation support")
                if ans == "Q" or ans == "q":
                    exit()
                elif ans =="C" or ans == "c":
                    break  
        return binDir, globalDir, prev_cpu_count , prev_memory,  inputDir, outputDir, reads_type, user_ref, trimmingMode, annotationAllowed, BQSRPossible, emblFile #dont need to change the usersettings.txt file 

    #User wants to modify settings / enter new settings
    print "Settings require update, entering setup"
    print cpu_count,'CPU cores and',
    print memory,'mb free memory detected.'
  
    flag = False
    ans = False

    refOptions = [name for name in os.listdir(refDir) if os.path.isdir(os.path.join(refDir, name))]
    refOptions.sort()
    
    while not flag:
        counter = 0
        print "--------------------------------------------"
        print "Please select a reference from the selection below:"
        for ref in refOptions:
            counter += 1
            print str(counter)+":\t",ref
        print "Q: quit\n"
        
        selectedRef = raw_input("Enter the corresponding number for the reference (1,2,3 etc): ")
        if selectedRef =="q" or selectedRef =="Q":
            quit()
        try:
            selectedRef = int(selectedRef)
        except:
            selectedRef = "N/A"
        if selectedRef not in range(1,len(refOptions)+1): 
            continue
        else:
            print "Reference selected:", refOptions[selectedRef-1]
            userRef = refOptions[selectedRef-1]
            break
    annotationAllowed, BQSRPossible, emblFile = validateRefFiles(refDir,user_ref)
    
    if not annotationAllowed:
        print "Please refer to the manual to create a reference folder"
        print "Annotation will NOT be supported if you deceide to continue"
        while True:
            ans = raw_input("press enter Q to quit or C to continue without annotation support")
            if ans == "Q" or ans == "q":
                exit()
            elif ans =="C" or ans == "c":
                annotationAllowed = False
                break
  
    while not flag:
        print 
        userPrefcpu = raw_input("How many CPU cores would you like to use (recommended amount: "+str(cpu_count)+" cores):")
        try:
            userPrefcpu = int(userPrefcpu)
        except:
            print "not a valid input"
            continue
        
        if userPrefcpu > int(cpu_count):
            ans = raw_input("Your system does not appear to have",userPrefcpu,"cores, continue anyway? Y/N:")
        elif int(userPrefcpu) <= 0:
            print "not a valid input"
            continue
        if ans == "N" or ans == "n":
            continue
        userPrefmem = raw_input("How much memory (in Mb) would you like to allocate to this process? (recommended "+str(int(memory))+"):")
        try:
            userPrefmem = int(userPrefmem)
        except:
            print "not a valid input"
            continue
        
        if int(userPrefmem) > int(memory):
            ans = raw_input("Your system does not appear to have "+userPrefmem+" memory, continue anyway? Y/N:")
        if ans == "N" or ans == "n":
            continue
        if int(userPrefmem <= 100):
            print "Invalid amount, try again"
            continue
        flag = True

    flag = False
    while not flag:
        print ("Is your data Paired-end, Single-end reads or both (mixed)? ")
        reads = raw_input("1 = paired-end 2 = single end, 3 = both (mixed): Q = quit:")
        if reads == 'Q' or reads == 'q':
            print "exiting program"
            exit()
        elif reads == "1" or reads == "2" or reads == "3":
            flag = True
    if reads =="1":
        reads = "pairedEnd"
    elif reads == "2":
        reads = "singleEnd"
    elif reads == "3":
        reads = "mixed"
    #params.reads = reads
    validDir = False
    workingDir = ""
    while not validDir:
        print 
        workingDir = raw_input("Enter directory containg your FASTQ files to be processed, for example /home/user/USAP/FASTQ, enter Q to quit:")
        if workingDir == 'Q' or workingDir == 'q':
            print "halting program"
            exit()
##        print workingDir    
        if workingDir[0] <> "/":
            workingDir = "/"+workingDir
        if workingDir[-1] <> "/":
            workingDir += "/"
##        print workingDir
##        raw_input("matching?")
            
        try:  #Check if working dir exists
            os.chdir(workingDir)
            print "directory accepted"
            if not checkFileNames(workingDir,reads): #check if there are suitable fastQ files and read access
                print "Will now prompt user to re-enter input folder containing suitable input files"
                valifDir = False
                continue
            else:
                validDir = True
        except:         
            validDir = False
            print "there is no such directory found: ",workingDir
            print 
            if "\\" in workingDir:
                print "make sure to use '/' and not '\\'"
                continue

    validDir = False
    while not validDir:
        print
        outputDir = raw_input("Enter your output directory to which you have write access (example: /home/user/NGS-DATA-OUT) or enter Q to quit:")
        if outputDir == 'Q' or outputDir == 'q':
            print "halting program"
            exit()

        if outputDir[0] <> "/":
            outputDir ="/"+outputDir 
        if outputDir[-1] <> "/":
            outputDir += "/"
        try:                        #Check if working dir exists
            os.chdir(outputDir)
            validDir=True
            print "directory accepted"
            break
        except:       
            if "\\" in outputDir:
                print "make sure to use '/' and not '\\'"           
                continue
            print "no such directory exists, would you like to create it? Y/N, Quit= Q"
            ans = raw_input()
            if ans =="q" or ans =="Q":
                exit()
        
            if ans =="n" or ans =="N":
                continue #retry entering the dir
                
            if ans == "Y" or ans == "y":
                try:
                    if outputDir[-1] <> "/":
                        outputDir+="/"
                    os.mkdir(outputDir)
                    os.chdir(outputDir)
                    validDir = True
                    print outputDir,"directory created"
                except:
                    print "could not create ",outputDir,"make sure file permissions are set correct to allow write access."
                    continue

    flag = False
    while not flag:
        print "Select read trimming method:"
        
##        print "1 = Quality based trimming using Trimmomatic"
##        print "2 = Trim a fixed amount using fastX trimmer"
##        print "3 = No trimming (faster but not recommended)" 
  
        trimMethod = raw_input("1 = Quality Trimming (recommended method), 2 = fixed amount, 3 = no trimming (not recommended), Q = quit:")
        if trimMethod == 'Q' or trimMethod == 'q':
            print "halting program"
            exit()

        elif trimMethod == "1" or trimMethod == "2" or trimMethod == "3":
            flag = True

    if trimMethod =="1":
        trimMethod = "Quality_Trim"
    elif trimMethod == "2":
        trimMethod ="Fixed_Amount_Trim"
    elif trimMethod == "3":
        raw_input("Please note that using this setting requires all fastq files to be Gzipped and thus to end with .gz, press enter to continue")
        trimMethod ="No_Trim"
        #print "Warning: To use this option all input fastq files must be in compressed format."
        #raw_input("Please ensure all input fastQ files are uncompressed before continuing, press enter to continue")
        
    print "-"*40
    print "Updated Settings:"
    print "Detected CPU cores: ",str(cpu_count)+", cores selected:",userPrefcpu
    print "Detected memory (mb):",str(memory)+", memory allocated:",userPrefmem
    print "FastQ directory", workingDir
    print "Output directory", outputDir
##    print "Platform type", platform
    print "reads type:", reads
    print "Refrence Strain selection:", userRef
##    print "Reference Srain:",strain
    print "Trimming method:",trimMethod
    print "-"*40
   
    os.chdir(globalDir)
    
    f=open('userSettings.txt','w')
    f.write("CPU_CORES\t"+str(userPrefcpu)+"\n")
    f.write("SYSTEM_MEMORY\t"+str(userPrefmem)+"\n")
    tempWorkingDir = workingDir
    if tempWorkingDir[-1] == "/":
        tempWorkingDir = tempWorkingDir[:-1]
    f.write("FASTQ_DIRECTORY\t"+tempWorkingDir+"\n")
    outputDirTemp = outputDir
    if outputDirTemp[-1] == "/":
        outputDirTemp = outputDirTemp[:-1]
    f.write("OUTPUT_DIRECTORY\t"+outputDirTemp+"\n")
##    f.write("PLATFORM\t"+platform+"\n")
    f.write("READS_TYPE\t"+reads+"\n")
##    f.write("mode\t"+mode+"\n")
    f.write("reference\t"+userRef+"\n")
    f.write("trimmingMode\t"+trimMethod+"\n")
    f.close()
    print "setup completed."

    return binDir, globalDir, userPrefcpu , userPrefmem,  workingDir, outputDir, reads, userRef, trimMethod, annotationAllowed, BQSRPossible, emblFile
    
#################################################################

##########################33
def indexReferences():
    try:
        os.makedirs(params.scripts_BWA)
    except:
        print "overriding existing temporary BWA pipeline scripts"
            
    allRefFiles = []
    fastaList = []
    nessisaryExtBWA = ["dict","amb","ann","bwt","fai","pac","sa"]
    nessisaryExtNOVO = ["ndx"]
    nessisaryExtSMALT =["sma","smi"]
    already_indexed = True
    bwa_indexed = True
    novo_indexed = True
    smalt_indexed = True
    allRefFiles = []
    
    os.chdir(params.reference)     
    for ref_fasta_file in os.listdir(params.reference):  # fasta each fasta file in ref fasta dir
        allRefFiles.append(ref_fasta_file)
    for fastaFile in allRefFiles:
        if not fastaFile.endswith(".fasta") or fastaFile.endswith(".fa"):
            continue
        fastaList.append(fastaFile)
        already_indexed = True

        print "determining if index nessisary for:",fastaFile
        for ext in nessisaryExtBWA:
            #print ext
            if not fastaFile+"."+ext in allRefFiles:
                
                if ext == "dict":
                    fileExt = fastaFile.split(".")[-1]
                    fastaDictName = fastaFile[:len(fastaFile)-len(fileExt)-1]+".dict"
                    if fastaDictName in allRefFiles:
                        continue
                already_indexed = False
                print fastaFile+" has not been previously indexed -> required file extention is:"+"."+ext,
                print "indexing for BWA:", fastaFile
##                raw_input()
                #bwa index step
                print "Command:",params.bwa,"index",params.reference+fastaFile
                subprocess.call([params.bwa,"index",params.reference+fastaFile])
                os.chdir(params.scripts_BWA)
##                f = open("indexBWAREF.sh",'w')
                #--> subprocess.call("samtools faidx "+params.reference+fastaFile) #replaced with method below due to samtools path issues
                print "creating .fai file:", fastaFile
                #raw_input("ready?")
                path = params.reference
                fileX = fastaFile
                cmd = [params.samtools,'faidx',path+fileX]
                pipe = subprocess.Popen(cmd, shell = False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                out,err = pipe.communicate()
                result = out.decode()
                
                print out
                print err
                #raw_input("press enter, checking ref")
                #if debugMode:
                    
##                subprocess.call("sh "+params.scripts_BWA+"indexBWAREF.sh",shell=True)
                fileExt = fastaFile.split(".")[-1]
                fastaDictName = fastaFile[:len(fastaFile)-len(fileExt)-1]+".dict"            
                subprocess.call("java -jar "+params.picardCreateSequenceDictionary+" REFERENCE="+params.reference+fastaFile+" OUTPUT="+params.reference+fastaDictName, shell=True)
                for ref_fasta_file in os.listdir(params.reference):  # fasta each fasta file in ref fasta dir
                    if ref_fasta_file not in allRefFiles:
                        allRefFiles.append(ref_fasta_file)
##            break #--> continue to index with next mapper


        ###Indexing for novoalign --> ."ndx"
        fileExt = fastaFile.split(".")[-1]
        NOVORefName = fastaFile[:len(fastaFile)-len(fileExt)-1]+".ndx"
        if not NOVORefName in allRefFiles:
            already_indexed = False
            print "indexing for NOVOAlign:", fastaFile
##                print "debugging12345"
##                print "1",params.novoIndex
##                print "2",params.reference+NOVORefName
##                print "3",params.reference+fastaFile
##                print "and"
##                print [params.novoIndex,params.reference+NOVORefName,params.reference+fastaFile]
##                raw_input("ok?")
            os.chdir(params.reference)
            ftemp = open(params.reference+fastaFile,'r')
            ftemp.close()
            
            os.chdir(params.reference)
##                subprocess.call([params.novoIndex,params.reference+NOVORefName,params.reference+fastaFile])
            
            cmd = [params.novoIndex,params.reference+NOVORefName,params.reference+fastaFile]
            pipe = subprocess.Popen(cmd, shell = False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            out,err = pipe.communicate()
            result = out.decode()
            print "Result : ", [result]
            print "novoindex complete"

        ###Indexing for smalt --> .sma and .smi"
        fileExt = fastaFile.split(".")[-1]
        SMALTRefName = fastaFile[:len(fastaFile)-len(fileExt)-1]
        for ext in nessisaryExtSMALT:
            if not SMALTRefName+"."+ext in allRefFiles:
                already_indexed = False
                print "indexing for SMALT:", fastaFile
                os.chdir(params.reference)
                statinfo = os.stat(fastaFile) #(33188, 422511L, 769L, 1, 1032, 100, 926L, 1105022698,1105022732, 1105022732)
                byteSize = statinfo.st_size
                #  4411540 bacteria 
                #100000000 large
                if byteSize < 100000000:
                    print "using stepsize 1 for smalt index"
                    subprocess.call([params.smaltBinary,"index","-k","13","-s","1",params.reference+SMALTRefName,params.reference+fastaFile])
                else:
                    print "using stepsize 6 for smalt index"
                    subprocess.call([params.smaltBinary,"index","-k","13","-s","6",params.reference+SMALTRefName,params.reference+fastaFile])
            break

    if already_indexed:
        print "All reference files previously indexed"
    return fastaList
##################################################################################
def calculate_coverage_bed(inputFolder):      
    os.chdir(inputFolder)
    resultFile = open("Genome_Coverage_Results_Summary.txt",'w')
    resultFile.write("Sample\tCoverage\n")

    for fileX in os.listdir(inputFolder):
        if not fileX.endswith("genomecov_bed.txt"):
            continue
        f = open(fileX,"r")
        #f.readline()
        cov = 0
        genome = 0
        #prevLine = "begin"
        for line in f:
            temp = line.split()
            if not line or temp == [] or temp == "":
                continue
            start = temp[1]
            end = temp[2]
            count = 0
            for x in range(int(start),int(end)):
                try:
                    genome += 1 #int(temp[3])
                    cov += float(int(temp[3]))
                except:
                    print "error calculating genome coverage"
                    continue
        f.close()

        if "_" in f.name:
            tempName = f.name.split("_")[0]
            try:
                resultFile.write(tempName+"\t"+str(int(round(float(cov)/genome,2)))+"\n")
            except:
                print "Error calculating genome coverage"
                resultFile.write(tempName+"\tERROR\n")
                
        else:
            try:
                resultFile.write(f.name+"\t"+str(int(round(float(cov)/genome,2)))+"\n")
            except:
                print "Error calculating genome coverage"
                resultFile.write(tempName+"\tERROR\n")
                
##        print "coverage calculation"
##        print "total reads", str(cov)
##        print "total bp", str(genome)
##        print "ave coverage", float(cov) / genome
    resultFile.close()
    return
######################################################################

##################################################################################
def calculate_coverage(inputFolder):      
    os.chdir(inputFolder)
    resultFile = open("Genome_Coverage_Results_Summary.txt",'w')
    resultFile.write("Sample\tCoverage\n")

    for fileX in os.listdir(inputFolder):
        if not fileX.endswith("genomecov.txt"):
            continue
        f = open(fileX,"r")
        f.readline()
        cov = 0
        genome = 0
        #prevLine = "begin"
        for line in f:
            temp = line.split()
            if not line or temp == [] or temp == "":
                continue
            genome += 1
            try:
                cov += float(temp[-1])
            except:
                print "error processing genome coverage line:", line
        f.close()

        if "_" in f.name:
            tempName = f.name.split("_")[0]
            try:
                resultFile.write(tempName+"\t"+str(int(round(float(cov)/genome,2)))+"\n")
            except:
                print "Error calculating genome coverage"
                resultFile.write(tempName+"\tERROR\n")
                
        else:
            try:
                resultFile.write(f.name+"\t"+str(int(round(float(cov)/genome,2)))+"\n")
            except:
                print "Error calculating genome coverage"
                resultFile.write(tempName+"\tERROR\n")
                
##        print "coverage calculation"
##        print "total reads", str(cov)
##        print "total bp", str(genome)
##        print "ave coverage", float(cov) / genome
    resultFile.close()
    return
######################################################################

def RemoveJunkFromVCF5(vcfDir,outputDir,junkTerms,mapperCount,qualityCutOFF,minCoverage,readFreqCutoff,DRMarkersToRemove,filterSpecificPositions):
    '''
    This is the main filter step to remove:
    snps not called by at least n mappers
    snps which dont have qual above cutoff 
    snps with annotation data containing key words for filtering :pe ppe pgrs repeat etc
    snps with coverage below min cov cutoff
    snps which are present in less than X% of reads, so only those above X% are kept, the rest is heterogenous / seq error.

    It looks for the x, x, x rows for the bwa/novo/smalt quality values and requires that the amount of mappers that you specifify...
    ...be above the quality value that you specificy
    '''
    ##########################################################
    def loadDRMarkers(DRMarkersToRemove):
        positions = []
        f = open(DRMarkersToRemove,'r')
        for line in f:
            temp = line.split("\t")
            if temp[0] == "Drug":
                continue
            pos = temp[2]
            if pos == "-":
                continue
            if "/" in pos:
                temp2 = pos.split("/")
                for x in temp2:
                    if x not in positions:
                        positions.append(x)
            elif pos not in positions:
                positions.append(pos)
        f.close()
        return positions
    ##########################################################
    def containJunkRegion(stringX,junkTerms):
        for x in junkTerms:
            if x in stringX:
                #print "junk ",x,"in ", stringX
                #raw_input()
                return True #it contains JUUUNNKK!
        return False
    def determineCoverage(stringX):
        #"1/1:2,140:142:99:5086,373,0"
        temp = stringX.split(":")[1]
        temp = temp.split(",")
        try:
            wt = temp[0]
        except:
            wt =0
        try:
            mut = temp[1]
        except:
            mut = 0
        return int(wt),int(mut)
    ###########################################################
    if filterSpecificPositions:
        print "Loading list of posistions to remove"
        DRMarkerList = loadDRMarkers(DRMarkersToRemove)
    totalLoaded = 0
    os.chdir(vcfDir)
    for fileX in os.listdir(vcfDir):
        if fileX.endswith(".vcf"):
            totalLoaded +=1
            print "filtering variants for file: ",fileX
            os.chdir(vcfDir)
            inFile = open(fileX, 'r')
            fileData = []
            line = inFile.readline()
            fileData.append(line)
            for x in inFile:                   
                lineX = x.split("\t")
                SNPPosition = lineX[0]
                if filterSpecificPositions:
                    if SNPPosition in DRMarkerList:
                        print "removing known DR marker:", SNPPosition
                        continue
                #print lineX
                #raw_input()
                if lineX[4] == "":
                    mapper1 = 0
                else:
                    mapper1 = float(lineX[4])

                if lineX[9] == "":
                    mapper2 = 0
                else:
                    mapper2 = float(lineX[10])

                if lineX[14] == "":
                    mapper3 = 0
                else:
                    mapper3 = float(lineX[16])

                count = 0
                if mapper1 > qualityCutOFF:
                    count += 1
                if mapper2 > qualityCutOFF:
                    count += 1
                if mapper3 > qualityCutOFF:
                    count += 1
                if count < mapperCount or containJunkRegion(str.lower(x),junkTerms):
                    continue
                #Test for coverage cutoff
                bwaCov = lineX[5].strip()
                novoCov = lineX[11].strip()
                smaltCov = lineX[17].strip()
                
                bwaMutFreq = lineX[6].strip()
                if bwaMutFreq == "\t" or bwaMutFreq == " \t" or bwaMutFreq == "error" or bwaMutFreq =="N/A":
                    bwaMutFreq = 0
                else:
                    #print [bwaMutFreq]
                    bwaMutFreq = float(bwaMutFreq)
                    
                novoMutFreq = lineX[12].strip()
                if novoMutFreq == "\t" or novoMutFreq == " \t" or novoMutFreq == "error" or novoMutFreq ==  "N/A":
                    novoMutFreq = 0
                else:
                    novoMutFreq = float(novoMutFreq)
                smaltMutFreq = lineX[18].strip()
                if smaltMutFreq == "\t" or smaltMutFreq == " \t" or smaltMutFreq == "error" or smaltMutFreq == "N/A":
                    smaltMutFreq = 0
                else:
                    smaltMutFreq = float(smaltMutFreq)
                
                if bwaCov == "":
                    bwaMutCov = 0
                else:
                    bwaWT,bwaMut = determineCoverage(bwaCov)
                    bwaMutCov = int(bwaMut)

                if novoCov == "":
                    novoMutCov = 0
                else:
                    novoWT,novoMut = determineCoverage(novoCov)
                    novoMutCov = int(novoMut)
                        
                if smaltCov == "":
                    smaltMutCov = 0
                else:
                    smaltWT,smaltMut = determineCoverage(smaltCov)
                    smaltMutCov = int(smaltMut)
                    
                #print "thesre are coverages for each"
                #print fileX, bwaMutCov, novoMutCov, smaltMutCov 
                #raw_input()

                minCovNum = 0
                if bwaMutCov > minCoverage:
                    minCovNum += 1
                if novoMutCov > minCoverage:
                    minCovNum += 1
                if smaltMutCov > minCoverage:
                    minCovNum += 1
                if minCovNum < mapperCount: #at least mapperCount amount of mapper must pass the coverage cutoff
                    continue 
                
                mutFreqNum = 0
                if bwaMutFreq > readFreqCutoff: 
                    mutFreqNum += 1
                if novoMutFreq > readFreqCutoff: 
                    mutFreqNum += 1
                if smaltMutFreq > readFreqCutoff: 
                    mutFreqNum += 1
                if mutFreqNum < mapperCount: #at least mapperCount amount of mapper must pass the readFreqCutoff cutoff
                    continue
                    
                fileData.append(x)
            inFile.close()

            try:
                os.chdir(outputDir)
            except:
                os.mkdir(outputDir)
                os.chdir(outputDir)
            newVCFFile = open(fileX.replace("ANNO","FILTERED"),'w')
            for x in fileData:            
                newVCFFile.write(x)
            newVCFFile.close()
    print totalLoaded,"files Filtered"
    return
###############################################################################################
def getMapperOverlap(dir1,dir2,dir3,outputDir0):
    #this takes 3 dirs, finds the overlapping snps and writes as vcf file to outputdir0
    #The original files are kept and their quality values can be used to make downstream filtering decisions       
    def getName(name):
        returnName = ""
        for x in name:
            if x == "_":
                return returnName
            else:
                returnName += x
        return returnName
    
    def getMatchingVCF(name1,dir2):
        os.chdir(dir2)
        for fileX in os.listdir(dir2):
            if not fileX.endswith(".vcf"):
                continue
            if name1 == getName(fileX):
                return fileX
        print "error, no match found for file :", name1
        print "program will attempt to continue using files with matches for each mapper"
        print "press enter"
        raw_input()
        return None
    print dir1
    for file1 in os.listdir(dir1):   
        if not file1.endswith(".vcf"):
            continue
        print "Obtaining SNP overlap from:", file1
        snps1 = {}
        snps2 = {}
        snps3 = {}
        allSNPS = []
        file1Data = []
        file2Data = []
        file3Data = []
        overlapData = []

        name1 = getName(file1)
        file2 = getMatchingVCF(name1,dir2)
        file3 = getMatchingVCF(name1,dir3)
        if file2 == None or file3 == None:
            continue
        os.chdir(dir1)
        inFile = open(file1, 'r')
        line = ""
        while "#CHROM" not in line:
            line = inFile.readline()
            if "#CHROM" not in line:
                overlapData.append(line) #only load actual snp data not header
        #inFile.readline()
        overlapData.append("#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tEXTRAINF\tBWA_QUAL\tBWA_INFO\tBWA_EXTRA_INFO\tNOVO_QUAL\tNOVO_INFO\tNOVO_EXTRA_INFO\tSMALT_QUAL\tSMALT_INFO\tSMALT_EXTRA_INFO\tMUTATIONS\tMAPPER_COUNT\n")
        linePos = -1
        for x in inFile:
            linePos += 1
            file1Data.append(x) #only load actual snp data not header
            tempX = x.split("\t")
            snps1[tempX[1]] = linePos
            if int(tempX[1]) not in allSNPS:
                allSNPS.append(int(tempX[1])) #now all of file1 is in fileData as a list
        inFile.close()

        os.chdir(dir2)
        inFile = open(file2, 'r')
        for x in range(31):
            line = inFile.readline()

        linePos = -1
        for x in inFile:
            linePos += 1
            file2Data.append(x) #only load actual snp data not header
            tempX = x.split("\t")
            snps2[tempX[1]] = linePos
            try:
                if int(tempX[1]) not in allSNPS:
                    allSNPS.append(int(tempX[1]))
            except:
                print "error empty data found, indicates possible incorrect file spacing, press enter to continue", [tempX[1]]
                raw_input()
                continue
        #now all of file1 is in fileData as a list
        inFile.close()
        
        os.chdir(dir3)
        inFile = open(file3, 'r')
##        print "preparing to run on", str(file3)
        for x in range(31):
            line = inFile.readline()

        linePos = -1
        for x in inFile:
            linePos += 1
            file3Data.append(x) #only load actual snp data not header
            tempX = x.split("\t")
            snps3[tempX[1]] = linePos
            if int(tempX[1]) not in allSNPS:
                allSNPS.append(int(tempX[1])) #now all of file1 is in fileData as a list
        inFile.close()        
        print "matching :", name1, file2, file3
        allSNPS.sort()
        os.chdir(outputDir0)
        overlapFile = open(str(file1),'w')
        for x in overlapData:
            overlapFile.write(x)
        
        for x in allSNPS:
            x = str(x)
            mapperCount = 0
            mutations = ""
            alreadyHaveBodyData = False
            dataToWrite1 = ""
            dataToWrite2 = ""
            dataToWrite3 = ""
            if (x not in snps1) and (x not in snps2) and (x not in snps3):
                print "fatal error x not in all 3 dictionaries", x
                raw_input()
            #BWA DATA
            if x in snps1: #snps1[x] is the line position to seek to in order to get the corresponding element from file1Data

                if not alreadyHaveBodyData:
                    bodyData = file1Data[snps1[x]].replace("\n","\t")
                    alreadyHaveBodyData = True

                dataToWrite = file1Data[snps1[x]].split("\t")
                dataToWrite1 += dataToWrite[5]+"\t"+dataToWrite[7]+"\t"+dataToWrite[9].replace("\n","\t")
                mutations += dataToWrite[4]+"/" # the alt data from the 4th coloumb
                mapperCount +=1
            else:
                dataToWrite1 += "\t\t\t"
                mutations    += "*/"

            #NOVO DATA  
            if x in snps2: #snps1[x] is the line position to seek to in order to get the corresponding element from file1Data
                if not alreadyHaveBodyData:
                    bodyData = file2Data[snps2[x]].replace("\n","\t")
                    alreadyHaveBodyData = True
                dataToWrite = file2Data[snps2[x]].split("\t")
                dataToWrite2 += dataToWrite[5]+"\t"+dataToWrite[7]+"\t"+dataToWrite[9].replace("\n","\t")

                mutations += dataToWrite[4]+"/" # the alt data from the 4th coloumb
                mapperCount +=1
            else:
                dataToWrite2 += "\t\t\t"
                mutations    += "*/"
            #SMALT DATA    
            if x in snps3: #snps1[x] is the line position to seek to in order to get the corresponding element from file1Data
                if not alreadyHaveBodyData:
                    bodyData = file3Data[snps3[x]].replace("\n","\t")
                    alreadyHaveBodyData = True
                dataToWrite = file3Data[snps3[x]].split("\t")
                dataToWrite3 += dataToWrite[5]+"\t"+dataToWrite[7]+"\t"+dataToWrite[9].replace("\n","\t")
##                        dataToWrite1.replace("\n","\t")
                mutations += dataToWrite[4]+"/" # the alt data from the 4th coloumb
                mapperCount +=1
            else:
                dataToWrite3 += "\t\t\t"
                mutations    += "*/"
            toWrite = bodyData+dataToWrite1+dataToWrite2+dataToWrite3 +mutations[:-1]+"\tMSum="+str(mapperCount)
            overlapFile.write(toWrite+"\n")
            
        overlapFile.close()
##        print "done with,",str(file1)
    print "exiting"
    return    
###############################################################################################
def test_SNP_INDEL_Path(path,variantType):
    try:
        os.chdir(path)
        os.chdir(path+"BWA/"+variantType)
        os.chdir(path+"NOVO/"+variantType)
        os.chdir(path+"SMALT/"+variantType)
        return True
    except:
        print "could not find variant directory for", variantType
        return False
    return None
#######################################################################
def compressDeletionData_bedtools(genCovDir):    
    os.chdir(genCovDir)
    for fileX in os.listdir(genCovDir):
        if "_genomecov=0.txt" not in fileX:
            continue
        f = open(fileX,'r')
        print "extracting deletion data from file:",fileX
        outputData = []
        line = f.readline()

        temp = line.split()
        if temp == []:
            continue
        start = int(float(temp[1])) 
        end = int(float(temp[2]))
        outputData.append((start,end))
        for line in f:
            temp = line.split()
            if temp == []:
                continue
            prevStart = outputData[-1][0]
            prevEnd = outputData[-1][1]
            start = int(float(temp[1])) 
            end = int(float(temp[2]))
            if prevEnd == start: #join data and update
                outputData[-1] = (prevStart,end)
                continue
            else: # add new data range
                outputData.append((start,end))
        f.close()
        f=open(fileX[:-4]+"_collapsed.txt",'w')
        for x in outputData:
            f.write(str(x[0]+1)+"\t"+str(x[1]+1)+"\n")
        f.close()
###############################################        
def compressDeletionData(genCovDir):
    def process_line(line):
        dataCurr = line.split()
        dataCurr = int(dataCurr[0].split(":")[1])
        return dataCurr

    def add_element(element,dataList):
        dataList.append(element)
        return dataList
        
    def update_element(outputData,element):
        previous_element = outputData[-1]
        if type(previous_element) == int: #then its just one element, add one more
    ##        print "adding1"
            previous_element = (previous_element,element)
        elif type(previous_element) == tuple:
    ##        print "adding2"
            previous_element = (previous_element[0],element)
        else:
            print "type error"
            print type(previous_element)
            print previous_element
            raw_input()
        outputData[-1] = previous_element
        return outputData
    
    os.chdir(genCovDir)
    print genCovDir
    for fileX in os.listdir(genCovDir):
        if "_genomecov=0.txt" not in fileX:
            continue
        f = open(fileX,'r')
        print fileX
        outputData = []
        dataCurr = -1
        dataPrev = -1

        #add first element
        line = f.readline()
        dataCurr = line.split()
        if dataCurr == []:
            #print "fatal error, no data found for:",fileX
            #print "in DIR:",genCovDir
            #print [line]
            continue
        dataCurr = int(dataCurr[0].split(":")[1])
        outputData.append(dataCurr)
         
        #check if remaining elements must be collapesed or added individually
        pos = 0
        for line in f:
            pos += 1
            dataPrev = outputData[-1]
            if type(dataPrev) == tuple: #its already a range
                dataPrev = dataPrev[1] #just concider last element (1,1003)
            dataCurr = process_line(line)
            if dataPrev + 1 == dataCurr:
                #update previous to include this number in the number range
                outputData = update_element(outputData,dataCurr)
            else: # add previsous to list
                outputData = add_element(dataCurr,outputData)
        f.close()
        f=open(fileX[:-4]+"_collapsed.txt",'w')
        for x in outputData:
            if type(x) == tuple:
                f.write(str(x[0])+"\t"+str(x[1])+"\n")
            else:
                f.write(str(x)+"\t\n")
        f.close()
#######################################################################################
def loadAnnotationData(dirX):
    #LOADS THE ENTIRE Annotation DATA file into memory
    os.chdir(dirX)
    f = open("ANNO_TABLE_UPDATE_2014.txt","r")
    f = open("AnnotationTable.txt","r") 
    header = f.readline() #skip header
    header = f.readline()
    data = []
    for line in f:
        data.append(line)
    f.close()
    return header,data
#######################################################################################
def lookup(del_startPos,del_endPos,data):
    '''
    This function simply checks if the deletion is inside the start and end
    or spanning the start and end sites
    '''
    matches = []
    del_startPos = int(del_startPos)
    del_endPos = int(del_endPos)

    for line in data:
        tempLine = line.split()
        rvStart = int(tempLine[3])
        rvEnd = int(tempLine[4])
        if (rvStart >= del_startPos) and (rvStart <= del_endPos):
            #The the gene start location falls in the deletion
            matches.append(line)
        elif (rvEnd >= del_startPos) and (rvEnd <= del_endPos):
            #The the gene end location falls in the deletion
            matches.append(line)
        elif (del_startPos > rvStart) and (del_endPos < rvEnd): 
            #The deletion falls inside the gene
            matches.append(line)
        if rvStart > del_endPos:
            return matches
    return matches
######################################################################
def loadDeletionData(inputDir,fileX):
    os.chdir(inputDir)
    data = []
    for tempFile in os.listdir(inputDir):
        if tempFile.split("_")[0] == fileX and "0_collapsed.txt" in tempFile:
            try:
                print "loading", tempFile, "from", inputDir
                f=open(tempFile,'r')
            except:
                print "error, no deletion data found for ,",fileX
                return []
            data=[]
            for x in f:
                temp = x.replace("\n",'')
                data.append(temp.split("\t"))
            f.close()
    return data
########################################################################
def getDeletionStatus(rvStart,rvEnd,deletionData):
    #Screen the entire list of deletions to see if any fall inside
    #the rv start and rv end range
    #rvStart 100
    #rvEnd   150
    #deletionData [[142, ""],[1244,1245], 1500,1600]]
    '''
    takes as input the start and end of each rv number in the annotation data table
    for each it looks in the deletion data to see of the rv number is affeceted by any deletions
    appends deletion status and range, this allows for multiple deletions in a gene
    returns a list: [[1bp del, 100,200],[partial_deletion(right flank), 200,300]]
    '''
    rvStart = int(rvStart)
    rvEnd = int(rvEnd)
    deletion = [] #"none"
    pos = -1
    for x in deletionData: #For each deleion in the list, see if it affects this rvNumber (affects area from rvstart to rvend)
        if rvEnd < int(x[0]): #then all further deletions would also be way to far to the right, no need to continue
            return deletion    #     ====Start=============End===Del=====
        if x[1] == "":
            if int(x[0]) >= rvStart and int(x[0]) <= rvEnd:
                deletion.append(["1bp_deletion",x[0],"-"]) #====Start=====Del========End=====
                continue
            else:
                continue
        #The code below handles deletions which are of a range X to Y (not 1 bp) 
        delStart = int(x[0])
        delEnd = int(x[1])
          
        #case 1 rvnumber falls inside deletion thus rvnumber fully deleted   #=del_S======Start=====End===del_end==
        if delStart <= rvStart and delEnd >= rvEnd: #entire rvnumber deleted
##            print "full deletion found"
##            raw_input()
            deletion.append(["entire_gene_deleted",x[0],x[1]])  
        #case 2 rvnumber spans the left start site of rvnum  #=del_S======Start===del_end======End=====
        elif delStart <= rvStart and delEnd < rvEnd and delEnd > rvStart: #deletion on left part of rvnumber
##            print "partial del found1"
##            raw_input()
            deletion.append(["gene_left_flank_deleted",x[0],x[1]])
        #case 3 rvnumber spans the right start site of rvnum  #===Start===del_start======End==del_end======
        elif delStart > rvStart and delStart < rvEnd and delEnd > rvEnd: 
##            print "partial del found1"
##            raw_input()
            deletion.append(["gene_right_flank_deleted",x[0],x[1]])
        #case 4 deletion falls in central part of rvnum  #===Start===del_start==DEL_ENd====End==== 
        elif delStart > rvStart and delEnd < rvEnd: 
##            print "CENTRAL DELETION found1"
##            raw_input()
            deletion.append(["internal_deletion",x[0],x[1]])

##    print "this is the deletion"
##    print deletion
##    raw_input()
    return deletion

######################################################################
def addAnnoDataToDeletions(outputDir,pos_to_feature_num_dict, featureNum_to_Anno_DataDict, known_feature_properties,mapperOrderList):
    allZeroCovFiles = []
    print "Pooling deletion data..."
    anno_del_dir = outputDir+"ANNOTATED_DELETIONS/"
    try:
        os.mkdir(anno_del_dir)
    except:
        print "could not create ",anno_del_dir,"directory might already exist" 
    os.chdir(anno_del_dir) #The output directory

    #os.chdir(params.globalDir)
    #header, annoData = loadAnnotationData(params.globalDir)
    
    #inputDirBWA = mainPath+"BWA/GenomeCoverage/"
    #inputDirNOVO = mainPath+"NOVO/GenomeCoverage/"
    #inputDirSMALT = mainPath+"SMALT/GenomeCoverage/"
    
    #first load a comprehensive list of 0-cov files, may not have run all mappers
    
    for mapperName in mapperOrderList:
        for fileX in os.listdir(outputDir+mapperName+"/GenomeCoverage/"):
            if fileX.split("_")[0] not in allZeroCovFiles and "genomecov=0_collapsed.txt" in fileX:
                allZeroCovFiles.append(fileX.split("_")[0])
        
    for fileX in allZeroCovFiles: #This is the file abbpreviation
        tempZeroCovFileData = []
        
        #if "genomecov=0_collapsed.txt" not in fileX or "genomecov=0_collapsed.txt~" in fileX:
        #    continue
        print "loading deletion data from:",fileX
        zeroCovDataLists = []
        for mapperName in mapperOrderList:
            print mapperName
            zeroCovDataLists.append(loadDeletionData(outputDir+mapperName+"/GenomeCoverage/",fileX))
        #bwaData = loadDeletionData(inputDirBWA,fileX)
        #novoData = loadDeletionData(inputDirNOVO,fileX)
        #smaltData = loadDeletionData(inputDirSMALT,fileX)
        #Now have 3 lists which contain all the deletions from bwa novo and smalt for this file
        
        os.chdir(anno_del_dir)
        '''
        for each feature number:
            check if affected in each mappper
            if affected store data
            if not skip
        store in list
        sort list
        write data
        '''
        #for line in annoData: changing to make use of the new annotation data format 
        for featureNum in featureNum_to_Anno_DataDict:  
            temp = featureNum_to_Anno_DataDict[featureNum]        #[ranges,feature_type,orientation,temp_features]
            ranges = temp[0]
            for rangeX in ranges:
                tempString = ""
                start = rangeX[0]
                end = rangeX[1]
                deletionSum = 0
                deletionDataNOVO = getDeletionStatus(start,end,zeroCovDataLists[1])
                if deletionDataNOVO <> []:
                    deletionSum = 1
                deletionDataBWA = getDeletionStatus(start,end,zeroCovDataLists[0])
                if deletionDataBWA <> []:
                    deletionSum += 1
                deletionDataSMALT = getDeletionStatus(start,end,zeroCovDataLists[2])
                if deletionDataSMALT <> []:
                    deletionSum += 1
                if deletionSum == 0:
                    continue

                numDels = len(deletionDataBWA) # 1 or 2 typically
                pos = 0
                for x in deletionDataBWA:
                    pos+=1
                    tempString+=(str(x))
##                    delFile.write(str(x))
                    if pos < numDels:
##                        delFile.write(",")
                        tempString+=(",")
                        
##                delFile.write("\t")
                tempString+=("\t")

                numDels = len(deletionDataNOVO) # 1 or 2 typically
                pos = 0
                for x in deletionDataNOVO:
                    pos+=1
##                    delFile.write(str(x))
                    tempString += str(x)
                    if pos < numDels:
##                        delFile.write(",")
                        tempString += ","
##                delFile.write("\t")
                tempString += "\t"

                numDels = len(deletionDataSMALT) # 1 or 2 typically
                pos = 0
                for x in deletionDataSMALT:
                    pos+=1
##                    delFile.write(str(x))
                    tempString += str(x)
                    if pos < numDels:
##                        delFile.write(",")
                        tempString += ","
##                delFile.write("\t"+"del_Sum="+str(deletionSum))
                tempString += "\t"+"del_Sum="+str(deletionSum)+"\t"
                #delFile.write(ranges[0]+"\t"+temp[1]+"\t"+temp[2]+"\t")
##                print temp
##                print "-----------------------------------"
                tempString+=temp[0][0][0]+"\t"
                tempString+=temp[0][0][1]+"\t"
                tempString+=temp[1]+"\t"
                tempString+="'"+temp[2]+"'\t"
                tempDict = {}
##                print temp[3]
                for element in temp[3]:
                    tempDict[element[0]] = element[1]
                for known_feature in known_feature_properties:
##                    print known_feature, known_feature in tempDict
##                    raw_input("ok1!1")
                    if known_feature in tempDict:
                        tempString += tempDict[known_feature]+"\t"
                    else:
                        tempString += " \t"
##                tempString+= "\n"
##                print "this should be written"
##                print tempString
##                raw_input()
                tempData = tempString.split("\t")
                tempZeroCovFileData.append(tempData)
##                print tempZeroCovFileData
##                raw_input("ok which element to sort on?")
        tempZeroCovFileData = sorted(tempZeroCovFileData,key=lambda x: int(x[4]))
        tempString = ""
        delFile = open(str(fileX)+".txt",'w')
        for mapperName in mapperOrderList:
            tempString+=mapperName+"_del_status\t"
        tempString+="Del_sum\t"
        tempString+="GeneStart\tGeneEnd\t"
        tempString+="Feature_type\tOrientation\t"
        header = ""
        for x in known_feature_properties:
            header += x+"\t" 
        header += "\n"
        delFile.write(tempString+header)
        for line_element in tempZeroCovFileData:
##            print [line_element]
##            raw_input()
            for part in line_element:
                #print [part]
                #raw_input()
                delFile.write(part.replace("\n",'').replace("\r","")+"\t")
            delFile.write("\n")
        delFile.close()
        #delFile2.close()
    return
######################################################################
def getFilterSettings(MTB,autoMode,params): 
    junkTerms = []
    try:
        f = open(params.reference+"filter_settings.txt",'r')
        for line in f:
            temp = line.strip().replace("\n","")
            if temp not in junkTerms:
                junkTerms.append(temp)
        f.close()
    except:
        if MTB:
            f = open(params.reference+"filter_settings.txt",'w')
            junkTerms = ["ppe ","PE/PPE","repeat","PE_PGRS","PE-PGRS","pe family","PGRS family","insertion seqs and phages","Possible transposase"] #,"LowQual","lowqual",'not a real mutation']
            for term in junkTerms:
                f.write(term+"\n")
            f.close()
        else:
            f = open(params.reference+"filter_settings.txt",'w')
            f.close()
    if autoMode and MTB:
        mapperCount = 3
        qualityCutOff = 10
        minCoverage = 10
        readFreqCutoff = 0.8
        return junkTerms,mapperCount,qualityCutOff,minCoverage,readFreqCutoff


    print "Would you like to customise your variant filtering?"
    filterFlag = None
    while filterFlag not in ["y","Y","n","N"]:
        filterFlag = raw_input("Y/N")

    if filterFlag.upper() == "Y":        
        print "These are the default filter terms:"
        print junkTerms 
        
        print "Variants containing these keywords in their annotation data will be filtered"
        print "press Y to agree to filter these from all your samples, press N to customize your filter keywords"
        ans = raw_input("Y/N")
        while ans.upper() not in ["Y","N"]:
            ans = raw_input("Y/N")
        if ans.upper() == "N":
            print "You can now manually enter keywords you would like to filter for"
            print "if this keyword appears in any of your annotated variants, then these variants will be exclused from your downstream analysis"
            print "enter each term followed by the return key, enter 'quit' terminate the process"
            print "suggested terms are", junkTerms
            ans2 = []
            count = 0
            while "quit" not in ans2:
                count +=1
                temp = raw_input("filter Term"+str(count)+":")
                if temp.lower() =="quit":
                    break
                else:
                    ans2.append(temp)
            junkTerms = ans2
            print "your new filter terms are:",junkTerms
                            
        ##vcfDir = annotatedFixedDir
        mapperCount = None
        while mapperCount not in ["1","2","3"]:    
            mapperCount = raw_input("mapperCount ? (defualt = 3)")
        mapperCount= int(mapperCount)
    
        flag = False
        while not flag:
            qualityCutOff = raw_input("qualityCutOFF ? (default = 0)")
            try:
                qualityCutOff = int(qualityCutOff)
                flag = True
            except:
                flag = False
                
        flag = False
        while not flag:
            print "Read frequency: The proportion of mutant:Wild-Type reads to use for filtering cutoff."
            readFreqCutoff = raw_input("Read frequency cutoff: enter a value between 0 and 1 (default = 0.75) :")
            try:
                readFreqCutoff = float(readFreqCutoff)
                flag = True
            except:
                flag = False
        
        flag = False
        while not flag:
            print ""
            minCoverage = raw_input("Please enter the minimum coverage cutoff: ")
            try:
                minCoverage = int(minCoverage)
                flag = True
            except:
                flag = False
    else:
        #Default settings:
        #junkTerms = [] # ["ppe ","PE/PPE","repeat","pe-pgrs","pe_pgrs","pe family","insertion seqs and phages","Possible transposase"]#,"LowQual","lowqual",'not a real mutation']   
        mapperCount = 3
        qualityCutOff = 10
        minCoverage = 10
        readFreqCutoff = 0.8
    return junkTerms,mapperCount,qualityCutOff,minCoverage,readFreqCutoff

def filterDeletions(inputDir,outputDir,mapperCount,junkTerms):
    for fileX in os.listdir(inputDir):
        print "Filtering deletions from file:", fileX
        if not fileX.endswith(".txt"):
            continue
        os.chdir(inputDir)
        f = open(fileX,'r')
        data = []
        data.append(f.readline())
        
        for line in f:
            filterLine = False
            temp = line.split("\t")
            delSum = int(temp[3].split("=")[1])
            if delSum < mapperCount:
                continue
            for term in junkTerms:
                if term in line:
##                    print "filtering this line:", line
                    filterLine = True
                    break
            if not filterLine:
                data.append(line)
        f.close()
        os.chdir(outputDir)
        f=open(fileX[:-4]+"_FILTERED_DEL.txt",'w')
        for x in data:
            f.write(x)
        f.close()
    return    
    

############# START DETERMINE LINEAGES ################
def loadDataLineageDatabase(fileX):

##    os.chdir("C:/Users/rvdm/Dropbox/Ruben Custom Tools/")
    f= open(fileX,'r')
    headerInfo = f.readline().split("\t")
    #load the data into a dictionary
    data = {}
    knownLineages = []
    knownLineagesCounts = {}
    for line in f:
        temp = line.split("\t")
        pos = temp[1]
        lineage = temp[0]
        if lineage not in knownLineagesCounts:
            knownLineagesCounts[lineage] = 1
        else:
            knownLineagesCounts[lineage] += 1
        if lineage not in knownLineages:
            knownLineages.append(lineage)
        change = temp[3]
        if pos not in data:
            data[pos] = [[change,lineage]]
        else:
            data[pos].append([change,lineage])
    f.close()
    knownLineages.sort()
    return data, headerInfo,knownLineages,knownLineagesCounts

def matchVCF_to_Lineage_DB(dirX,fileX,DB,lineageOrder):
    os.chdir(dirX)
    f = open(fileX,'r')
    f.readline()
    lineage = "NONE"
    lineage_scores = {}
    for line in f:
        temp = line.split("\t")
        pos = temp[0]
        if pos in DB: #Then it is a lineage marker
            ref = temp[8]
            if ref == "":
                ref = temp[2]
            if ref == "":
                ref = temp[14]
            mut = temp[9]
            if mut == "":
                mut = temp[3]
            if mut == "":
                mut = temp[15]

            if len(ref) > 1 or len(mut) >1: # skip indels
                continue
                
            for DB_element in DB[pos]: #could be marker for more than one lineage, these are ["A/T","lineage2.2"],[],[]
                #unpack data
                temp_lineage = DB_element[1]
                temp_ref = DB_element[0][0]
                temp_mut = DB_element[0][2]
                if temp_ref not in ["A","T","G","C"] or temp_mut not in ["A","T","G","C"]:
                    print DB_element
                    raw_input("database format error")
                if ref <> temp_ref:
                    print "error, references do not match"
                    print line
                    print DB_element
                    raw_input()
                if ref == temp_ref and mut == temp_mut:
                    #print line
                    #print DB_element
                    #print "found a lineage specific marker"
                    #raw_input()
                    #NOW STORE THE SCORE TOWARDS EACH LINEAGE IN THE SCORE DICTIONARY
                    if temp_lineage not in lineage_scores:
                        lineage_scores[temp_lineage] = 1
                    else:
                        lineage_scores[temp_lineage] += 1
    #print "done loading data for", fileX
    best_lineage_score = 0
    best_lineage_name = "No matches, likely same as reference."
    for lineageName in lineage_scores:
        if int(lineage_scores[lineageName]) > int(best_lineage_score):
            best_lineage_score = lineage_scores[lineageName]
            best_lineage_name = lineageName
        #print best_lineage_name, best_lineage_score
        #raw_input()
    allDataPerLineage = []
    
    for lineage in lineageOrder:
        if lineage not in lineage_scores: #then this lineage had no matches
            allDataPerLineage.append(0)
        else:
            allDataPerLineage.append(lineage_scores[lineage])
##    print best_lineage_name
##    print best_lineage_score
##    print allDataPerLineage
##    raw_input()
    return best_lineage_name, best_lineage_score, allDataPerLineage

def determine_lineages(filteredVariantsDir,lineageFile):
    DB, headerInfo, knownLineages, knownLineageCounts  = loadDataLineageDatabase(lineageFile)
    percentage_results = open(params.mapperOut+"/percentage_lineageData.txt",'w')
    fullResults = open(params.mapperOut+"/full_lineageData.txt",'w')
    fullResults.write("Sample\tClosest_Matching_Lineage\tBest_Score")
#here i want the lineage score for ALL known lineages, the summary file will deceide which to report
    percentage_results.write("Sample\tClosest_Matching_Lineage\tBest_Score")
    for lineage in knownLineages:
        fullResults.write("\t"+lineage)
        percentage_results.write("\t"+lineage)
    fullResults.write("\n")
    percentage_results.write("\n")
        
    for fileX in os.listdir(filteredVariantsDir):
        print "Detecting known lineage markers in file:", fileX
        if not fileX.endswith(".vcf"):
            continue
        lineageName, score, allData = matchVCF_to_Lineage_DB(filteredVariantsDir,fileX,DB,knownLineages)
##        print fileX, [lineageName], [score]
##        print allData
##        raw_input("check if lineage matches here...")
        fullResults.write(fileX+"\t"+lineageName+"\t"+str(score))
        if lineageName not in knownLineageCounts: #Then there was no mathces to lineage DB, could be this is very similar to the refenrece used.
            percentage_results.write(fileX+"\t"+lineageName+"\t"+"NO_MATCHES")
            
        else:
            percentage_results.write(fileX+"\t"+lineageName+"\t"+str(round( 100.0*score / knownLineageCounts[lineageName],2)))
        pos = -1
        for x in allData:
            pos += 1
            tempLineage = knownLineages[pos]
            fullResults.write("\t"+str(x))
            if lineageName not in knownLineageCounts: #Then there was no mathces to lineage DB, could be this is very similar to the refenrece used.
                percentage_results.write("\t"+"NO_MATCHES")
            else:
                percentage_results.write("\t"+str(round(100.0*x / knownLineageCounts[lineageName],2)))
        fullResults.write("\n")
        percentage_results.write("\n")
##    results.close()
    percentage_results.close()
    fullResults.close()
    return
#########   End determine lineages  ##################################
def hardWareInit(debugMode):
    if params.multiMode:
        params.coreSplit = [0,0,0]
        corePos = 0
        if debugMode:
            print params.cores
        for x in range(int(params.cores)):
            corePos = corePos % 3
            corePos +=1
            params.coreSplit[corePos-1] += 1

        for x in params.coreSplit: #Failsafe 
            if x == 0:
                params.coreSplit = [1,1,1]
                break
            
        params.mem = str(int(params.mem)/ 3000)
        if params.mem == "0":
            params.mem = str(int(params.mem) / 1000)
        if params.mem == "0":
            params.mem = "1"
    else: 
        params.mem = str(int(params.mem) / 1000)
        if params.mem == "0": #Failsafe
            params.mem = "1"
        params.coreSplit = [str(params.cores),str(params.cores),str(params.cores)]
        if params.cores == 0:
            params.coreSplit = ["1","1","1"]  #Failsafe
    return 

######################
def lookupSpoligotypes(spolPredOut,debugMode):
    dirX = params.mapperOut+spolPredOut
    os.chdir(dirX)

    count = 0
    #data = []
    names = {}
    for fileX in os.listdir(dirX):
        if not fileX.endswith("spolPredOut.txt"):
            continue
        f = open(fileX,'r')
        line = f.readline()

        temp = line.split()[0]
        octal = line.split()[1]
        while temp.find("/") <> -1:
            temp = temp[temp.find("/")+1:]
        temp = temp.split("_")[0]
        name = temp
        if name not in names:
            names[name] = octal
        else:
            print fileX, name, "already have from", names[name]
    ##        print name,octal
    ##        raw_input()
        f.close()
        count += 1
    f = open("concat_spoligo_all.txt",'w')
    for x in names:
        f.write(x+"\t"+names[x]+"\n")
    f.close()

    print count, "SpolPred files loaded"

    os.chdir(params.binDir)
    
    lookupDict = {}
    f = open("spolpred_lookup_table.txt",'r')
    for line in f:
        temp= line.replace("\n",'')
        temp= temp.split("\t")
        name = temp[2]
        octal = temp[1]
        #code = temp[0]
    ##    print name
    ##    print octal
    ##    raw_input()
        lookupDict[octal] = name
    f.close()
        
    os.chdir(dirX)
    results = []
    f = open("concat_spoligo_all.txt",'r')
    for line in f:
        temp= line.split()
        name = temp[0]
        octal = temp[1]
        if octal in lookupDict:
            spoligo = lookupDict[octal]
            print "found",spoligo
        else:
            spoligo = "no_match"
        results.append(name+"\t"+octal+"\t"+spoligo+"\n")
    f.close()

    os.chdir(params.mapperOut) #the main results folder
    
    f=open("SpolPred_Results_Summary.txt",'w')
    f.write("Sample\toctalCode\tStrain_Name\n")
    for x in results:
        if debugMode:
            print x
        f.write(x)
    f.close()
    return
    
###########################################################################################################################################################
def snpDistanceMatrix(dir1,outputDir, outputDir_for_indiv_files,write_all_results,output_File_Name):
    def write_individual_results(file1, file2 ,unique1, unique2,outputDir):
        os.chdir(outputDir)
        f=open(file1.split("_")[0]+"_"+file2.split("_")[0]+".vcf",'w')
        for x in unique1:
            f.write(x)
        f.close()
        
    def merge(left,right,compare):
        result = []
        i,j = 0, 0
        while i < len(left) and j < len(right):
            if compare(int(left[i][0]),int(right[j][0])):
                result.append(left[i])
                i+=1
            else:
                result.append(right[j])
                j+=1
        while (i < len(left)):
            result.append(left[i])
            i+=1
        while (j < len(right)):
            result.append(right[j])
            j+=1
        return result


    def mergeSort(L, compare = operator.lt):
        if len(L) < 2:
            return L[:]
        else:
            middle = int(len(L)/2)
            left = mergeSort(L[:middle], compare)
            right = mergeSort(L[middle:], compare)
            return merge(left, right, compare)

    def getOverlap(a,b):
        #takes two dictionaries, returns the common variants between B and A, (sorted)
        overlapList = []
        match = 0
        for x in a:
            if x in b:
                overlapList.append(a[x]) #[x,a[x]])
        mergeSortedList = mergeSort(overlapList)
        return mergeSortedList

    def subtract(a,b):
        '''
        Snps at same pos can have syn or non-syn- thuse NOT  the same
        ie this function should return 2 lists, one list of all the snps that are at the same position
        and a second list of snps that a exactly the same mutation
        '''
        
        #takes two dictionaries, subtracts all info in B from A,
        #ie only the unique elements in A remain and are returned (sorted).
        exactMatches = []
        positionMatches = [] # - these will be split into the 4 possibilities - A,T,G or C  =  [0,0,2,1]
        #total = the sum of these two above
        uniqueList = []
        match = 0
        for x in a:
            if x not in b: #then the bp pos match, but do the mutations match - fix this later
                uniqueList.append(a[x]) #[x, a[x]])
        return uniqueList


    def loadAllRefereceSNPS_singleFile(dirX,fileX):
        '''
        this creates a dictionary of all the bp position snp info...problem is what about diff mut at same bp pos - use poolData2List instead
        '''
        genome = {} # {100:"b",200:"c"}
        totalLoaded = 0
        totalSNPS = 0
        fileArray = []
        os.chdir(dirX)

        print "reading file: ",fileX
        inFile = open(fileX, 'r')
        for x in range(1): #there is only a one line header
            header = inFile.readline()
        for x in inFile:
            line = x.split('\t')
            if "INTERGENIC" in line:
                extra = '' #\t\t\t\t\t\t\t\t\t\t\t\t\t'
            else:
                extra = ""
            if line[0] not in genome: #this bp pos is already in genome
                genome[line[0]] = x.replace("\n","\t")+extra+str(file)+"\talt="+line[2]+"\n"  #the 1 means one file had this mutation
            else:
                fileCountPostition = len(x)+len(extra)
                genome[line[0]] = genome[line[0]].replace("\n","\t")+str(file)+"\talt="+line[2]+"\n"      
        inFile.close()
        totalSNPS += len(genome)
        #print "A total of ",totalLoaded,"files were loaded which contains a total of",totalSNPS,"SNPS."
        return genome,header

    def compareALL_VS_ALL(dir1,outputdir,outputDir_for_indiv_files,write_all_results,output_File_Name): #strict - must be in all of a all of b
        allFiles = [] # list of all files in the comparison folder
        os.chdir(outputdir)
        for fileX in os.listdir(dir1):
            if fileX.endswith(".vcf"):
                allFiles.append(fileX)
        allFiles.sort()
        
        inFile = open(output_File_Name,'w')
        for x in allFiles:
            inFile.write("\t"+x.split("_")[0])
        inFile.write("\n")
        
        for x in range(len(allFiles)): #x each file from beginning of all files:
            inFile.write(allFiles[x].split("_")[0])
            for y in range(len(allFiles)): # fileY in allFiles y each file from here to the right:
                #print [x,y],
                #load snps for x
                #load snps for y
                #subtract 
                #write output (unique to a, unique to b, overlap, total)
                pooledSNP1,header = loadAllRefereceSNPS_singleFile(dir1,allFiles[x]) 
                pooledSNP2,header = loadAllRefereceSNPS_singleFile(dir1,allFiles[y])         
                unique1 = subtract(pooledSNP1,pooledSNP2)
                unique2 = subtract(pooledSNP2,pooledSNP1)
                numberUniqueA = len(unique1)
                numberUniqueB = len(unique2)
                overlap = getOverlap(pooledSNP1,pooledSNP2) #a list
                lenOverlap = len(overlap)

                ALLUniqueSNPSFROMDIR1= unique1 # = subtract(pooledSNP1,{})
                ALLUniqueSNPSFROMDIR1 = sorted(ALLUniqueSNPSFROMDIR1, key=lambda x: int(x.split("\t")[0]))
                unique2 = sorted(unique2, key=lambda x: int(x.split("\t")[0]))
                overLapList = sorted(overlap, key=lambda x: int(x.split("\t")[0]))
        
                os.chdir(outputdir)
                inFile.write("\t"+str(len(unique1)+len(unique2)))
                if write_all_results:
                    write_individual_results(allFiles[x],allFiles[y],unique1, unique2,outputDir_for_indiv_files)
            inFile.write("\n")
        print "closing "
        inFile.close()
    compareALL_VS_ALL(dir1,outputDir,outputDir_for_indiv_files,write_all_results,output_File_Name)

############################################################################################################################################
##############################################################################################
def fixFastq(fileX):
    if fileX.endswith(".fastq.gz"): #gunzip first
        print "uncompressing",fileX
        cmd = ["gunzip",fileX]
        pipe = subprocess.Popen(cmd, shell = False, stdout = subprocess.PIPE, stderr=subprocess.PIPE)
        out,err = pipe.communicate()
        result = out.decode()
    fileX = fileX.replace(".gz",'')
    try:
        f = open(fileX,'r')
    except:
        print "error fixing broken fastq file:",fileX
        return False
    
    #print "fixing", fileX
    newFileName = fileX.replace(".fastq","_fixed.fastq")
    newFile = open(newFileName,'w')
    haveNewHeader = False
    nextHeader = ""
    while True:
        if haveNewHeader:
            header = nextHeader
        else:
            header = f.readline()
        line1 = f.readline()
        line2 = f.readline()
        line3 = f.readline()
        if not header:
            break
                  
        #if len(line3) == len(line1)+1: #crop last char from line3 probably = "!"
        #    haveNewHeader = False
        #    newFile.write(header)
        #    newFile.write(line1)
        #    newFile.write(line2)
        #    newFile.write(line3[:-2]+"\n")
        #    
        #elif len(line3) <> len(line1)+1 and "!@SL" in line3:
        #    nextHeader = line3[len(line1):]
        #    haveNewHeader = True
        #    newFile.write(header)
        #    newFile.write(line1)
        #    newFile.write(line2)
        #    newFile.write(line3[:len(line1)-2]+"\n")
        #    print "sit1"
        #    print [line1]
        #    print [(line3[:len(line1)-2]+"\n")]
        #    raw_input()
        elif len(line1) <> len(line3): #crop last char from line3 probably = "!"
            #if "@SL" in line3:
            #nextHeader = line3[len(line1):]
            #haveNewHeader = True
            newFile.write(header)
            newFile.write(line1)
            newFile.write(line2)
            newFile.write(line3[:len(line1)-1]+"\n")
            
            #print "sit3"
            #print [line1]
            #print [line3[:len(line1)-2]+"\n"]
            #raw_input()
                          
        else:
            haveNewHeader = False
            newFile.write(header)
            newFile.write(line1)
            newFile.write(line2)
            newFile.write(line3) 

    f.close()
    newFile.close()
    return newFileName
##############################################################################################    
    
def createDirStructure():
    if True:
        os.chdir(params.main)
        print params.main
        try:
            os.makedirs(params.scripts_BWA) #Scripts
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.scripts_NOVO)
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.scripts_SMALT)
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.scripts_StrainIdentification)
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.mapperOut) #Results
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.BWAAligned)
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.NOVOAligned)
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.SMALTAligned)
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.BWAAligned_aln) #Alignment_Files
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.NOVOAligned_aln)
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.SMALTAligned_aln)
        except: "The directory already exists, proceeding"
        picardReport = "picardReport/"   
        try:
            os.makedirs(params.BWAAligned+picardReport) 
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.NOVOAligned+picardReport) 
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.SMALTAligned+picardReport) 
        except: "The directory already exists, proceeding"
        genomeCovDir = "GenomeCoverage/"
        try:
            os.makedirs(params.BWAAligned+genomeCovDir) 
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.NOVOAligned+genomeCovDir) 
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.SMALTAligned+genomeCovDir) 
        except: "The directory already exists, proceeding"
        snpDir = os.path.join("VARIANTS/") 
        try:
            os.makedirs(params.BWAAligned+snpDir) 
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.NOVOAligned+snpDir) 
        except: "The directory already exists, proceeding"
        try:
            os.makedirs(params.SMALTAligned+snpDir) 
        except: "The directory already exists, proceeding"
    ##    indelDir = os.path.join("INDELS/") 
    ##    try:
    ##        os.makedirs(params.BWAAligned+indelDir) 
    ##    except: "The directory already exists, proceeding"
    ##    try:
    ##        os.makedirs(params.NOVOAligned+indelDir) 
    ##    except: "The directory already exists, proceeding"
    ##    try:
    ##        os.makedirs(params.SMALTAligned+indelDir) 
    ##    except: "The directory already exists, proceeding"
        spolPredOut = "SpolPredOut/"
        try:
            os.makedirs(params.mapperOut+spolPredOut)
        except:
            "The directory already exists, proceeding"
        return picardReport, genomeCovDir, snpDir, spolPredOut
#######################################################################
#def runScript(path,scriptName):
#    '''
#    Many of the steps in the pipelines do not support multi-threading
#    this does not optimally make use of multi-core CPUs.
#    Program running structure is thus to run all pipelines simultanously with equal split of resources
#    this presents a problem for large references - solultion is to taper alignment steps
#    For now will run all at same time 
#    '''
#    print "Running script:", "sh "+path+scriptName
#    startTime = time.time()
#    cmd = ['sh',path+scriptName]
#    pipe = subprocess.Popen(cmd, shell = False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
###    if wait_flag:
#    out,err = pipe.communicate()
#    result = out.decode()
#    print "Result : ", [result]
#    print "Error : ", [err]
#    print "COMPLETED:", scriptName
#    print time.time() - startTime
#######################################################################

#######################################################################
#######################################################################
#######################################################################
#######################################################################
#######################################################################
#######################################################################
#######################################################################
#######################################################################
#######################################################################
#######################################################################
#######################################################################
#######################################################################
#######################################################################
#######################################################################
#       USAP MAIN CONTROL   ###########################################
try:
    readline.set_completer_delims(' \t\n;')
    readline.parse_and_bind('tab: complete')
    readline.set_completer(complete)
except:
    print "Error loading completer module..."

if True:
    ans, skipSetup, controlPoint, mappers, variantTools, spaceSavingMode, debugMode, autoMode = interface()
    mapperOrderList = ["BWA","NOVO","SMALT"]
    
    if ans == "1":
        print "Running USAP..."        
        #obtain system settings to optimize performace
        cpu_count = available_cpu_count() #int or None
        memory = available_memory() #int or None
        print [cpu_count]
        print [memory]
        raw_input("These are detected")
        
        #Obtain user preferences
        binDir, globalDir, userPrefcpu , userPrefmem,  inputDir, outputDir, readsType, userRef, trimMethod, annotationAllowed, BQSRPossible, emblFile = setup(cpu_count,memory,skipSetup)
        params = paramaters(binDir, globalDir, userPrefcpu , userPrefmem,  inputDir, outputDir, readsType, userRef, trimMethod, BQSRPossible, emblFile)
        if debugMode:
            print "Selected user reference and output folder:"
            print userRef
            print params.reference
            print outputDir
            raw_input("ref and output settings are shown")

        if "MycobacteriumTuberculosis_H37Rv" in params.reference: 
            MTB = True
        else:
            MTB = False
        junkTerms,mapperCount,qualityCutOFF,minCoverage,readFreqCutoff = getFilterSettings(MTB,autoMode,params)   
        
        try:
            os.chdir(outputDir)
        except:
            try:
                os.mkdir(outputDir)
            except:
                print "output directory already exists"
        print "Setup and parameter initialization complete."
        #step 1
        #Determine running mode
        ################################################
        params.multiMode = True 
        if userPrefmem < 2000:
            params.multiMode = False
            print "Less than 2gig RAM allocated, this wil not allow simultanious execution due to resource requirements..."
        elif int(userPrefcpu) < 3:
            params.multiMode = False
            print "Detected 2 or less CPU cores, this wil not allow simultanious execution due to resource requirements..."
        hardWareInit(debugMode) # determine the coresplit and mem allocation
        ###############################################
        #Step 2: check if nessisary to index reference, if so index for each mapper
        params.fastaList = indexReferences()
        ####################################################################################################################
        #Step 3: FASTQC, Trim reads + adapter removal:
        print "preparing to run FastQC"
        try:
            os.makedirs(params.scripts_trimming)
        except:
            print "Using previously existing trimming script directory"
        try:
            os.chdir(params.scripts_trimming)
            f=open('FastQC.sh','w')
            f.close()
            f=open('autoTrim.sh','w')
            f.close()
        except:
            print "Error creating QC scripts."
            raw_input()
            exit()
        
        ###################################
        #Setup output directory structure:
        picardReport, genomeCovDir, snpDir, spolPredOut = createDirStructure()
        ###################################

        flag = fastQCSH() 
        if not flag:
            exit()
        ###########################################################################
        #note: trimMethod can be "Quality_Trim", "Fixed_Amount_Trim", or "No_Trim"
        if params.trimMethod == "No_Trim":
            print "No trimming selected."
            trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE, fileArraySE, fileArrayPE  = noTrimmingFileNamePartition()
            #print trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE, fileArraySE, fileArrayPE
            #raw_input("1111111111111111111111111")
        else:
            os.chdir(params.scripts_trimming)
            try:    
                os.makedirs(params.trimmedFastQ)
            except:
                print "Using previously existing trimmed-FastQ folder"
                
            #print "Trimming method selected:", params.trimMethod
            if params.trimMethod == "Quality_Trim":
                print "creating trimmomatic script..."
                trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE, fileArraySE, fileArrayPE  = trimmomaticMulti() 
            elif params.trimMethod == "Fixed_Amount_Trim":
                trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE, fileArraySE, fileArrayPE  = fastXToolKitTrim()
            else:
                print [params.trimMethod]
                raw_input("user settings file corrupt")
                exit()

        if controlPoint <= 2:
            if debugMode:
                raw_input("control point 1: FASTQC")
            print "Running FastQC..."
    ##        subprocess.call("sh "+params.scripts_trimming+"FastQC.sh &",shell = True)
            subprocess.call(["sh",params.scripts_trimming+"FastQC.sh"])
            print "FastQC complete."
    
        if controlPoint <= 3:
            if debugMode:
                raw_input("control point reached: Trimming")
            print "Trimming reads..."
            ###################################NEW FEB 2016####################
            ##############################
            '''os.chdir(params.scripts_trimming)
            path = params.scripts_trimming
            scriptName = "autoTrim.sh"
            print "Running Trimming script:", "sh "+params.scripts_trimming+scriptName
            cmd = ['sh',path+scriptName]
            trimLog1 = open(params.scripts_trimming+"Trim_LOG1.log",'a')
            trimLog2= open(params.scripts_trimming+"Trim_LOG2.log",'a')
            pipe1 = subprocess.Popen(cmd, shell = False, stdout=trimLog1, stderr=trimLog2)
            '''
            ###############################
            ''''
            os.chdir(params.scripts_trimming)
            f = open("autoTrim.sh",'r')
            
            for line in f:
                temp = line.split()
                if temp == []:
                    continue
                cmd = temp
                if debugMode:
                    print "running", cmd
                pipe = subprocess.Popen(cmd, shell = False, stdout = subprocess.PIPE, stderr=subprocess.PIPE)
                out,err = pipe.communicate()
                result = out.decode()
                if "Sequence and quality length don't match" in err:
                    #get the filename of the broken file / files
                    #case 1 - it is SE
                    if cmd[3] == "SE":
                        print "SE FASTQ found with mismatching sequence and quality lengths, attempting to fix broken file...", cmd[7]
                        newFileName = fixFastq(cmd[7])
                        cmd[7] = newFileName
                    elif cmd[3] == "PE":
                        print "PE FASTQ found with mismatching sequence and quality lengths, attempting to fix broken file...", cmd[7],"and",cmd[8]
                        newFileNameR1 = fixFastq(cmd[7])
                        cmd[7] = newFileNameR1
                        newFileNameR2 = fixFastq(cmd[8])
                        cmd[8] = newFileNameR2
                
                    print "Attempting to use fixed fastq file(s) for trimming..." 
                    if debugMode:
                        print "running", cmd
                    pipe = subprocess.Popen(cmd, shell = False, stdout = subprocess.PIPE, stderr=subprocess.PIPE)
                    out,err = pipe.communicate()
                    result = out.decode()
                    if "Sequence and quality length don't match" in err:
                        print "Fatal error, attempt to fix FASTQ file failed for 2nd time, is this file corruput? Check the file format matches input types:",
            '''
            ################################### END NEW FEB 2016####################
    
            subprocess.call("sh "+params.scripts_trimming+"autoTrim.sh",shell = True)
            print "Trimming complete."
    
        if controlPoint <= 4:
            ####################################################################################################################
            #Step4 Alignment using 3 mappers 
            #Here the program maximize performance by divising the resources between the 3 mappers
            #so if there is not enough resources, the programs must run one at a time, two at a time or all 3 at the same time
            ####################################################################################################################
            #CREATE TRI-PARTATE-SCRIPTS
            print "creating variant detection scripts"
            if params.BQSRPossible:
                print "BASE QUALITY SCORE RECALIBRATION IS ENABLED"
            else:
                print "BASE QUALITY SCORE RECALIBRATION IS NOT POSSIBLE DUE TO MISSING DATABASE FILE"
            #if debugMode:
                #raw_input("is this correct BQSR info?")
            if debugMode:
                print "Mappers are:", mappers
                print "selected variant callers are:",variantTools
                raw_input("press enter to continue")
            #BWA:
            BWAAlign_combine(trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE) # NO LONGER USING THIS --> BWACombineMulti()
            variantScriptsBWA(trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE)
            #NOVO:
            NOVOAlign(trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE)
            NOVOAlignMulti()
            variantScriptsNOVO(trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE)
            #SMALT:
            SMALTAlign(trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE) 
            variantScriptsSMALT(trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE)
            #######################################################################################################################
            #Run serially
            commandListBWA = []
            commandListNOVO = []
            commandListSMALT = []
            ##BWA
            for mapper in mappers:
                if mapper[0] == "BWA" and mapper[1] == True:
                    commandListBWA.append("1_BWAAlign.sh") #"Combining reads from sai files..." Step 1
                    commandListBWA.append("2_combineReads.sh") #Step 2
                    commandListBWA.append("3_picardValidate.sh") #echo "Validating reads..." #Step 3
                    commandListBWA.append("4_createSamToBam.sh") #echo "Converting Sam file to BAM file..." #Step 4
                    commandListBWA.append("1_BWAAlign_cleanup.sh") #for step 3 CLEANUP
                    commandListBWA.append("2_combineReads_Cleanup.sh") #for step 5 CLEANUP
                    commandListBWA.append("5_indexBam.sh") #echo "Indexing BAM file..." #Step 5
                    commandListBWA.append("6_1_GATK.sh") #echo "Creating intervals file to allow realignment..." #Step 6    
                    commandListBWA.append("python "+params.binDir+"/fixMisEncodedQuals.py "+params.BWAAligned_aln+" "+params.scripts_BWA+" "+"6_2_GATK.sh") #This script updates 6_2_GATK.sh if any files did not complete
                    commandListBWA.append("6_2_GATK.sh") #echo "Creating intervals file to allow realignment..." --> fix misencoded_quality_scores #Step 7
                    commandListBWA.append("7_1_Realignment.sh") #echo "Realigning reads..." #Step 8
                    commandListBWA.append("python "+params.binDir+"/fixMisEncodedQuals.py "+params.BWAAligned_aln+" "+params.scripts_BWA+" "+"7_2_Realignment.sh") #This script updates 7_1_GATK.sh if any files did not complete
                    commandListBWA.append("7_2_Realignment.sh") #echo "Realigning reads..." #Step 9
                    commandListBWA.append("4_createSamToBam_cleanup.sh") #for step 12 CLEANUP
                    commandListBWA.append("5_indexBam_cleanup.sh") #for step 12
                    if params.BQSRPossible: 
                        commandListBWA.append("8.1_baseQualRecalBWA.sh") # Base quality recalibration #Step 10
                        commandListBWA.append("8.2_baseQualRecalBWA.sh") # Base quality recalibration #Step 11
                    commandListBWA.append("6_1_GATK_cleanup.sh") #for step 12
                    commandListBWA.append("6_2_GATK_cleanup.sh") #for step 12
                    commandListBWA.append("9_picardSort.sh") #echo "Sorting reads..." 12
                    commandListBWA.append("7_1_Realignment_cleanup.sh") #for step 12
                    commandListBWA.append("7_2_Realignment_cleanup.sh")
                    commandListBWA.append("10_reIndexBamFiles.sh") #echo "Re-indexing..."
                    commandListBWA.append("8.1_baseQualRecalBWA_cleanup.sh") #for step 12 CLEANUP"
                    commandListBWA.append("8.2_baseQualRecalBWA_cleanup.sh") #for step 12 CLEANUP"
                    commandListBWA.append("11_removePCRDuplicates.sh") #echo "Removing PCR duplicates..." 13
                    commandListBWA.append("12_reIndexBamFiles2.sh") #echo "Re-indexing..." 14
                    commandListBWA.append("9_picardSort_cleanup.sh") #for step 15 CLEANUP"
                    commandListBWA.append("13_getMappedReads.sh") #echo "Calculating mapped reads..." 15
                    commandListBWA.append("10_reIndexBamFiles_cleanup.sh") #for step 15 CLEANUP"
                    #here make use of the list variantTools which is [callername:True/False,callername:True/False] to toggle between gatk,samtools or gatk+samtools
                    for caller in variantTools:
                        if caller[0] == "GATK" and caller[1] == True:
                            commandListBWA.append("14_SNP_INDEL_Calling_GATK_HAPLOTYPECALLER.sh") #echo "SNP calling using GATK..." 16
                        elif caller[0] == "GATK" and caller[1] == False:
                                commandListBWA.append('echo "Skipping GATK based variant calling due to user customization"')   #Default is True
                    #commandListBWA.append("15_GenomeCoverage.sh") #echo "Calculating genome coverage..." 18
                    commandListBWA.append("15_2_GenomeCoverage.sh") #echo "Calculating genome coverage..." 18
                    commandListBWA.append("16_ZeroCov.sh")  #echo "Calculating areas with no coverage..." 19
                    for caller in variantTools:
                        if caller[0] == "SAMTOOLS" and caller[1] == True:
                            commandListBWA.append("14_2_VARIANT_CALLING_SAMTOOLS.sh") #echo running samtools based variant calling
                        elif caller[0] == "SAMTOOLS" and caller[1] == False:
                            commandListBWA.append('echo "Skipping Samtools based variant calling due to user customization"')   #Default is True
        ##            commandListBWA.append("15_GenomeCoverage_cleanup.sh")
        ##            commandListBWA.append("16_ZeroCov_cleanup.sh")
                ########################
                #NOVO
                if mapper[0] == "NOVOAlign" and mapper[1] == True:
                ##    commandListNOVO.append("1_1_NOVOAlign.sh")
                    commandListNOVO.append("1_2_NOVOAlign_multi.sh")
                ##    commandListNOVO.append("1_2_NOVOAlign_multi.sh")# > ./Scripts/NOVO/NovoAlign.out")  #echo "Aligning Reads to reference using NOVOAlign..." 1
                    commandListNOVO.append("2_picardValidate.sh") #echo "Validating reads..." 2
                    commandListNOVO.append("3_createSamToBam.sh") # echo "Converting Sam file to BAM file..." 3
                    commandListNOVO.append("1_1_NOVOAlign_cleanup.sh") 
                    commandListNOVO.append("4_indexBam.sh") #echo "Indexing BAM file..." 4
                    commandListNOVO.append("5_1_GATK.sh") #echo "Creating intervals file to allow realignment..." 5
                    commandListNOVO.append("python "+params.binDir+"/fixMisEncodedQuals.py "+params.NOVOAligned_aln+" "+params.scripts_NOVO+" "+"5_2_GATK.sh") #This script updates 6_2_GATK.sh if any files did not complete
                    commandListNOVO.append("5_2_GATK.sh") #echo "Creating intervals file to allow realignment..." 6
        
                    commandListNOVO.append("6_1_Realignment.sh") #echo "Realigning reads..." 7
                    commandListNOVO.append("python "+params.binDir+"/fixMisEncodedQuals.py "+params.NOVOAligned_aln+" "+params.scripts_NOVO+" "+"6_2_Realignment.sh") #This script updates 6_2_GATK.sh if any files did not complete
                    commandListNOVO.append("6_2_Realignment.sh") #echo "Realigning reads..." 8
                    commandListNOVO.append("3_createSamToBam_cleanup.sh")
                    commandListNOVO.append("4_indexBam_cleanup.sh")
                    if params.BQSRPossible:
                        commandListNOVO.append("7.1_baseQualRecalNOVO.sh") # Base quality recalibration 9
                        commandListNOVO.append("7.2_baseQualRecalNOVO.sh") # Base quality recalibration 10
                    commandListNOVO.append("5_1_GATK_cleanup.sh")
                    commandListNOVO.append("8_picardSort.sh")  #echo "Sorting reads..." 11   
                    commandListNOVO.append("6_x_Realignment_cleanup.sh")     
                    commandListNOVO.append("7.1_baseQualRecalNOVO_cleanup.sh")
                    commandListNOVO.append("7.2_baseQualRecalNOVO_cleanup.sh") 
                    
                    commandListNOVO.append("9_reIndexBamFiles.sh") #echo "Re-indexing..." 12
                    commandListNOVO.append("10_removePCRDuplicates.sh") #echo "Removing PCR duplicates..." 13
                    commandListNOVO.append("11_reIndexBamFiles2.sh") #echo "Re-indexing..." 14        
                    commandListNOVO.append("8_picardSort_cleanup.sh")
                    commandListNOVO.append("12_getMappedReads.sh") #echo "Calculating mapped reads..." 15
                    commandListNOVO.append("9_reIndexBamFiles_cleanup.sh")
                ##    commandListNOVO.append("13_SNPCallingGATK.sh") #echo "SNP calling using GATK..." 
                ##    commandListNOVO.append("13_INDELCallingGATK.sh") # echo "INDEL calling using GATK..."
                    for caller in variantTools:
                        if caller[0] == "GATK" and caller[1] == True:
                            commandListNOVO.append("13_SNP_INDEL_Calling_GATK_HAPLOTYPECALLER.sh") #echo "SNP calling using GATK..." 16
                        elif caller[0] == "GATK" and caller[1] == False:
                            commandListNOVO.append('echo "Skipping GATK based variant calling due to user customization"')   #Default is True
                    
                    #commandListNOVO.append("14_GenomeCoverage.sh") #echo "Calculating genome coverage..." 17
                    commandListNOVO.append("14_2_GenomeCoverage.sh") #echo "Calculating genome coverage..." 17
                    commandListNOVO.append("15_ZeroCov.sh") #echo "Calculating areas with no coverage..." 18
                    for caller in variantTools:
                        if caller[0] == "SAMTOOLS" and caller[1] == True:
                            commandListNOVO.append("13_2_VARIANT_CALLING_SAMTOOLS.sh") #echo "Calculating areas with no coverage..." 18
                        elif caller[0] == "SAMTOOLS" and caller[1] == False:
                            commandListNOVO.append('echo "Skipping Samtools based variant calling due to user customization"')   #Default is True 
        ##            commandListNOVO.append("14_GenomeCoverage_cleanup.sh")
                    #commandListNOVO.append("15_ZeroCov_cleanup.sh")

                ########################
                #SMALT
                if mapper[0] == "SMALT" and mapper[1] == True:
                    commandListSMALT.append("1_SMALTAlign.sh") #echo "Aligning Reads to reference using SMALT...Output is <SampleName>_trim_smalt.sam" 1
                    commandListSMALT.append("2_sortSmaltSam.sh") #echo "Sorting reads...Output is <SampleName>_trim_smalt_sort.sam" 2
                    commandListSMALT.append("1_SMALTAlign_cleanup.sh")
                    commandListSMALT.append("3_addReadGroupsToSortedSam.sh")#echo "Adding readgroups...Output is <SampleName>_trim_smalt_SrtRG.sam" 3
                    commandListSMALT.append("2_sortSmaltSam_cleanup.sh")
                    commandListSMALT.append("4_picardValidate.sh")  #echo "Validating reads...Output is <SampleName>_trim_smalt_SrtRG_validateReport" 4
                    commandListSMALT.append("5_createSamToBam.sh") #echo "Converting Sam file to BAM file and sorting...Output is <SampleName>_trim_smalt_SrtRG.sam" 5
                    commandListSMALT.append("6_indexBam.sh") #echo "Indexing BAM file...Output is <SampleName>_trim_smalt_SrtRG_sorted.bam.bai" 6
                    commandListSMALT.append("3_addReadGroupsToSortedSam_cleanup.sh")
                    commandListSMALT.append("7_1_GATK.sh")#echo "Creating intervals file to allow realignment...Output is <SampleName>_trim_smalt_SrtRG_sorted.intervals" 7
        
                    commandListSMALT.append("python "+params.binDir+"/fixMisEncodedQuals.py "+params.SMALTAligned_aln+" "+params.scripts_SMALT+" "+"7_2_GATK.sh") #This script updates 6_2_GATK.sh if any files did not complete
                    commandListSMALT.append("7_2_GATK.sh")#echo "Creating intervals file to allow realignment...Output is <SampleName>_trim_smalt_SrtRG_sorted.intervals" 8
                    commandListSMALT.append("8_1_Realignment.sh") #echo "Realigning reads...Output is <SampleName>_trim_smalt_SrtRG_sorted_realigned.bam" 9
                    commandListSMALT.append("python "+params.binDir+"/fixMisEncodedQuals.py "+params.SMALTAligned_aln+" "+params.scripts_SMALT+" "+"8_2_Realignment.sh") #This script updates 6_2_GATK.sh if any files did not complete
                    commandListSMALT.append("8_2_Realignment.sh") #echo "Realigning reads...Output is <SampleName>_trim_smalt_SrtRG_sorted_realigned.bam" 10
                    commandListSMALT.append("5_createSamToBam_cleanup.sh")
                    commandListSMALT.append("6_indexBam_cleanup.sh")
                    if params.BQSRPossible: 
                        commandListSMALT.append("9.1_baseQualRecalSMALT.sh") # Base quality recalibration 11
                        commandListSMALT.append("9.2_baseQualRecalSMALT.sh") # Base quality recalibration 12
                    commandListSMALT.append("7_x_GATK_cleanup.sh")
                    
                    commandListSMALT.append("10_picardSort.sh")#echo "Sorting reads...Output is <SampleName>_trim_smalt_SrtRG_realigned_resorted.bam" 13
                    commandListSMALT.append("8_x_Realignment_cleanup.sh")
                    commandListSMALT.append("9.1_baseQualRecalSMALT_cleanup.sh")
                    commandListSMALT.append("9.2_baseQualRecalSMALT_cleanup.sh")
                    commandListSMALT.append("11_reIndexBamFiles.sh") #echo "Re-indexing...Output is <SampleName>_trim_smalt_SrtRG_realigned_resorted.bam.bai" 14
                    commandListSMALT.append("12_removePCRDuplicates.sh")  #echo "Removing PCR duplicates...Output is <SampleName>_trim_smalt_SrtRG_realigned_resorted_dedup.bam" 15
                    commandListSMALT.append("13_reIndexBamFiles2.sh")  #echo "Re-indexing...Output is <SampleName>_trim_smalt_SrtRG_realigned_resorted_dedup.bam.bai" 16
                    commandListSMALT.append("10_picardSort_cleanup.sh")
                    #commandListSMALT.append("11_reIndexBamFiles_cleanup.sh")
                    commandListSMALT.append("14_getMappedReads.sh")  #echo "Calculating mapped reads...Output is <SampleName>_trim_smalt_SrtRG_sorted_samtools_stats.txt" 17
                    commandListSMALT.append("12_removePCRDuplicates_cleanup.sh")
                    commandListSMALT.append("11_reIndexBamFiles_cleanup.sh") 
                    #commandListSMALT.append("15_SNPCallingGATK.sh")   #echo "SNP calling using GATK...Output is in /SNP directory, <SampleName>_gatk_snps.vcf and <SampleName>_Genotype.log" 18
                ##    commandListSMALT.append("15_INDELCallingGATK.sh")  #echo "INDEL calling using GATK...Output is in INDEL directory, <SampleName>_gatk_indels.vcf" 
            
                    for caller in variantTools:
                        if caller[0] == "GATK" and caller[1] == True:
                            commandListSMALT.append("15_SNP_INDEL_Calling_GATK_HAPLOTYPECALLER.sh") #echo "SNP calling using GATK..." 19
                        elif caller[0] == "GATK" and caller[1] == False:
                            commandListSMALT.append('echo "Skipping GATK based variant calling due to user customization"')   #Default is True
                    
                    #commandListSMALT.append("16_GenomeCoverage.sh") #echo "Calculating genome coverage...Output is in GenomeCoverage directory ,<sampleName>_genomecov.txt" 20 
                    commandListSMALT.append("16_2_GenomeCoverage.sh") #echo "Calculating genome coverage...Output is in GenomeCoverage directory ,<sampleName>_genomecov.txt" 20 
                    commandListSMALT.append("17_ZeroCov.sh") #echo "Calculating areas with no coverage...Output is in GenomeCoverage directory, <sampleName>_genomecov=0.txt" 21
                    
                    for caller in variantTools:
                        if caller[0] == "SAMTOOLS" and caller[1] == True:
                            commandListSMALT.append("15_2_VARIANT_CALLING_SAMTOOLS.sh") #echo "Calculating areas with no coverage...Output is in GenomeCoverage directory, <sampleName>_genomecov=0.txt" 21
                        elif caller[0] == "SAMTOOLS" and caller[1] == False:
                            commandListSMALT.append('echo "Skipping Samtools based variant calling due to user customization"')   #Default is True
                    
                    ##commandListSMALT.append("16_GenomeCoverage_cleanup.sh")
                    #SMALT_cleanups["FINAL"] = ["18_ZeroCov_cleanup.sh"]
                #################################################################################################################    
            if debugMode:
                raw_input("control point reached: MapperScipts")
            print "Running BWA/NOVO/SMALT scripts"
            '''
            if not multimode:
                one at a time
            elif multimode:
                all 3 same time
            if one pipeline complete, increase the cpu and memory allocation to others
            wait till done, print output to screen
            print "done"
            '''
            print commandListBWA
            print
            print commandListNOVO
            print
            print commandListSMALT
            print
            
            '''
            here is the new idea - write the entire commandlistbwa to a .sh file, same for other mappers
            then run subprocessout on these and set to run in background
            then wait for all 3 to complete
            then continue to next step
            this does the same as the bash script
            it also makes use of intermediary script to check if certail steps completed
            '''
            
            os.chdir(params.scripts_BWA)
            f = open("BWA_pipeline.sh",'w')
            for command in commandListBWA:
                if "python" not in command and "echo" not in command:
                    f.write("sh "+command+"\n")
                else:
                    f.write(command+"\n")
            f.close()
            os.chdir(params.scripts_NOVO)
            f = open("NOVOAlign_pipeline.sh",'w')
            for command in commandListNOVO:
                if "python" not in command:
                    f.write("sh "+command+"\n")
                else:
                    f.write(command+"\n")
            f.close()
            os.chdir(params.scripts_SMALT)
            f = open("SMALT_pipeline.sh",'w')
            for command in commandListSMALT:
                if "python" not in command:
                    f.write("sh "+command+"\n")
                else:
                    f.write(command+"\n")
            f.close()
            if debugMode:
                raw_input("HALT! press enter to start running scripts!")
            #raw_input("Extra stop introduced")
            #if MTB and debugMode:
            #    raw_input("creating spolpred scripts here")
            picardReport, genomeCovDir, snpDir, spolPredOut = createDirStructure()
            if MTB:
                spolpred(params,trimmedFilesSE, trimmedFilesPE, IDSE, SMSE, LBSE, IDPE, SMPE, LBPE, fileArraySE, fileArrayPE)
                #if debugMode:
                    #raw_input("press enter to start running spolpred and mapping scripts...")
                os.chdir(params.scripts_StrainIdentification)
                path = params.scripts_StrainIdentification
                scriptName = "spolpred.sh"
                print "Running spolpred scrript:", "sh "+params.scripts_StrainIdentification+scriptName
                cmd = ['sh',path+scriptName]
                strainLogFile1 = open(params.scripts_StrainIdentification+"spolpred_LOG1.log",'a')
                strainLogFile2 = open(params.scripts_StrainIdentification+"spolpred_LOG2.log",'a')
                pipe_strainIdent = subprocess.Popen(cmd, shell = False, stdout=strainLogFile1, stderr=strainLogFile2)

            if not spaceSavingMode:
                scriptLocations = [params.scripts_BWA,params.scripts_NOVO,params.scripts_SMALT]
                for scriptLocation in scriptLocations:
                    for scriptName in os.listdir(scriptLocation):
                        if "cleanup" in scriptName:
                            os.chdir(scriptLocation)
                            os.remove(scriptName) 
                #raw_input("check if cleanup scripts are removed or kept")                     
            ###########################################################################################################
            #BWA
            ###########################################################################################################
            os.chdir(params.scripts_BWA)
            path = params.scripts_BWA
            scriptName = "BWA_pipeline.sh"
            print "Running BWA script:", "sh "+params.scripts_BWA+scriptName
            path = params.scripts_BWA
            cmd = ['sh',path+scriptName]
    ##        cmd = ["echo","hello"]
    ##        pipe1 = subprocess.Popen(cmd, shell = False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            bwaLogFile1 = open(params.scripts_BWA+"BWA_LOG1.log",'a')
            bwaLogFile2 = open(params.scripts_BWA+"BWA_LOG2.log",'a')
            pipe1 = subprocess.Popen(cmd, shell = False, stdout=bwaLogFile1, stderr=bwaLogFile2)
    
    ##        out,err = pipe1.communicate()
    ##        result = out.decode()
    ##        print "Result : ", [result]
    ##        if err <> "":
    ##            print "Error : ", err
    ##        print "BWA mapper scripts completed"
            ###########################################################################################################
            #BWA END
            ###########################################################################################################
            ###########################################################################################################
            #NOVO BEGIN
            ###########################################################################################################
            os.chdir(params.scripts_NOVO)
            path = params.scripts_NOVO
            scriptName = "NOVOAlign_pipeline.sh"
            print "Running NOVOAlign script:", "sh "+params.scripts_NOVO+scriptName
            path = params.scripts_NOVO
            cmd = ['sh',path+scriptName]
    ##        cmd = ["echo","hello"]
    ##        pipe2 = subprocess.Popen(cmd, shell = False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            novoLogFile1 = open(params.scripts_NOVO+"NOVO_LOG1.log",'a')
            novoLogFile2 = open(params.scripts_NOVO+"NOVO_LOG2.log",'a')
            pipe2 = subprocess.Popen(cmd, shell = False, stdout=novoLogFile1, stderr=novoLogFile2)
    
    ##        out,err = pipe2.communicate()
    ##        result = out.decode()
    ##        print "Result : ", [result]
    ##        if err <> "":
    ##            print "Error : ", err
    ##        print "NOVOAlign mapper scripts completed"
            ######################################################################
            #NOVO END
            ######################################################################
            ###########################################################################################################
            #SMALT BEGIN
            ###########################################################################################################
            os.chdir(params.scripts_SMALT)
            path = params.scripts_SMALT
            scriptName = "SMALT_pipeline.sh"
            print "Running SMALT script:", "sh "+params.scripts_SMALT+scriptName
            path = params.scripts_SMALT
            cmd = ['sh',path+scriptName]
    ##        pipe3 = subprocess.Popen(cmd, shell = False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            smaltLogFile1 = open(params.scripts_SMALT+"SMALT_LOG1.log",'a')
            smaltLogFile2 = open(params.scripts_SMALT+"SMALT_LOG2.log",'a')
            pipe3 = subprocess.Popen(cmd, shell = False, stdout=smaltLogFile1, stderr=smaltLogFile2)
    ##        out,err = pipe3.communicate()
    ##        result = out.decode()
    ##        print "Result : ", [result]
    ##        if err <> "":
    ##            print "Error : ", err
    ##        print "SMALT mapper scripts completed"
            ######################################################################
            #SMALT END
            ######################################################################
            if MTB:
                exit_codes = [p.wait() for p in pipe1,pipe2,pipe3,pipe_strainIdent]
                out4,err4 = pipe_strainIdent.communicate()
            else:
                exit_codes = [p.wait() for p in pipe1,pipe2,pipe3]
            out1,err1 = pipe1.communicate()
            out2,err2 = pipe2.communicate()
            out3,err3 = pipe3.communicate()
            
            try:
                result1 = out1.decode()
                result2 = out2.decode()
                result3 = out3.decode()
                if MTB:
                    result4 = out4.decode()
            except:
                print "variant detection complete"
            
    ##        print "Result : ", [result3]
    ##        if err3 <> "":
    ##            print "Error : ", err3
    ##        print "Mapper scripts completed"
    ##        print params.scripts_SMALT
    ##        raw_input("ok to proceed?")
            
            bwaLogFile1.close
            bwaLogFile2.close
            novoLogFile1.close
            novoLogFile2.close
            smaltLogFile1.close
            smaltLogFile2.close
            if MTB:
                strainLogFile1.close 
                strainLogFile1.close 
            
        if controlPoint <= 5:
            if debugMode:
                print "calculating genome coverage"
                print "control point ", [controlPoint]
                raw_input("press enter")
            BWAGenCov = params.BWAAligned+"GenomeCoverage/" #NOVO Results dir
            NOVOGenCov = params.NOVOAligned+"GenomeCoverage/" #NOVO Results dir
            SMALTGenCov = params.SMALTAligned+"GenomeCoverage/" #NOVO Results dir
            calculate_coverage_bed(BWAGenCov) #outputs results into inputfolder
            calculate_coverage_bed(NOVOGenCov) #outputs results into inputfolder
            calculate_coverage_bed(SMALTGenCov) #outputs results into inputfolder
            ####################
            '''
            if spaceSavingMode:
                for covDir in [BWAGenCov,NOVOGenCov,SMALTGenCov]:
                    os.chdir(covDir)
                    for fileX in os.listdir(covDir):
                        if "_dedup_genomecov.txt" in fileX:
                            #raw_input("will now remove file"+str(fileX))
                            os.remove(fileX)
            '''
            print "genome coverage calculation complete"
            ####################
        if controlPoint <= 6:
            if debugMode:
                print "control point 6"
                raw_input("gen cov calc, press enter")
            #Compress deletion data before annotation
    ##        mainDir = "C:/Ruben/[ALL_RESULTS]/[ANNO_TEST]/Results/"      
            #mainDir = params.mapperOut
            BWAGenCov = params.BWAAligned+"GenomeCoverage/" #NOVO Results dir
            NOVOGenCov = params.NOVOAligned+"GenomeCoverage/" #NOVO Results dir
            SMALTGenCov = params.SMALTAligned+"GenomeCoverage/" #NOVO Results dir
            for covDir in [BWAGenCov,NOVOGenCov,SMALTGenCov]:
                print "Compressing deletion data in :",covDir 
                compressDeletionData_bedtools(covDir)#---------------------------------------------------------control point--------------------------------------------------------------------------------------
                #compressDeletionData(covDir)#---------------------------------------------------------control point--------------------------------------------------------------------------------------
            
        if controlPoint <= 7:
            print "Annotating detected variants"
            if debugMode:
                raw_input("press enter")
            #**********************
    ##        mainDir = "C:/Ruben/[ALL_RESULTS]/[ANNO_TEST]/Results/"
    ##        VCF_DIR1 = "C:/Ruben/[ALL_RESULTS]/[ANNO_TEST]/Results/BWA/SNP/"
    ##        VCF_DIR2 = "C:/Ruben/[ALL_RESULTS]/[ANNO_TEST]/Results/NOVO/SNP/"
    ##        VCF_DIR3 = "C:/Ruben/[ALL_RESULTS]/[ANNO_TEST]/Results/SMALT/SNP/"
    ##        [params.BWAAligned+snpDir, params.NOVOAligned+snpDir,params.SMALTAligned+snpDir]
    ##        VCF_Location_List = [VCF_DIR1,VCF_DIR2,VCF_DIR3]
            VCF_Location_List =[params.BWAAligned+snpDir, params.NOVOAligned+snpDir,params.SMALTAligned+snpDir]
            
            emblDir = params.EMBL
    ##        emblDir = "E:/"  
    ##        annotationFile = "H37Rv_4411532_updated_13_jun_2013.EMBL"
            annotationFile = params.emblFile
            #dirX = "G:/[CH38_HG_REF_FILES]/EMBL/"      
            #annotationFile = "Homo_sapiens.GRCh38.79.chromosome.1.dat"
    ##        outputDir = mainDir+"ANNOTATED_VARIANTS/"
            outputDir = params.mapperOut+"ANNOTATED_VARIANTS/"
            try:
                os.mkdir(outputDir)
            except:
                "already exists"        
            pos_to_feature_num_dict, featureNum_to_Anno_DataDict, known_feature_properties = autoAnnotateEMBL(emblDir, annotationFile, outputDir, VCF_Location_List, mapperOrderList)
            print "Variant annotation completed"
            print "Annotating putative deletions"
            outputDir = params.mapperOut
            addAnnoDataToDeletions(outputDir,pos_to_feature_num_dict, featureNum_to_Anno_DataDict, known_feature_properties,mapperOrderList)
            print "Annotation complete"
            
        if controlPoint <= 8:
            print "Preparing to filter variants."
            if debugMode:
                raw_input("press enter")
            ##########################################################################
            #Variant filtering 
            ##########################################################################
            try:
                os.mkdir(outputDir) 
            except: "already exists"
    
            annotatedDir = params.mapperOut+"ANNOTATED_VARIANTS/"
            outputDir =  params.mapperOut+"FILTERED_VARIANTS/"
            

            print "Applying filtering settings:"
            print "Variants containing these keywords will be filtered:", junkTerms
            print "mapper count: ", mapperCount
            print "Quality cutoff: ", qualityCutOFF
            print "readFreqCutoff cutoff: ", readFreqCutoff
            print "minCoverage cutoff: ", minCoverage
            
            #junkTerms = ["ppe ","PE/PPE","repeat","pe-pgrs","pe_pgrs","pe family","insertion seqs and phages","Possible transposase"] 
            DRMarkersToRemove = ""
            RemoveJunkFromVCF5(annotatedDir,outputDir,junkTerms,mapperCount,qualityCutOFF,minCoverage,readFreqCutoff,DRMarkersToRemove,False)
            print "Variant filtering completed" 
            inputDir = params.mapperOut+"ANNOTATED_DELETIONS/"
            outputDir = params.mapperOut+"FILTERED_DELETIONS/"
            try:
                os.mkdir(outputDir)
            except:
                print "Filtered deletion directory already exists..."
            filterDeletions(inputDir,outputDir,mapperCount,junkTerms)
            print "Deletion filtering completed" 
            #######################################################################################
    
        if controlPoint <= 9: #phenotype prediction
            phenoAllowed = True
            print "Matching known variants to phenotype database"
            if debugMode:
                raw_input("press enter to continue")
            try:
                os.chdir(params.pheno)
            except:
                print "No database of known molecular markers found (For disease/phenotype/drug resistance determination)."
                phenoAllowed = False
            if phenoAllowed:
                phenoFileList = []
                phenoAllowed = True
                for fileX in os.listdir(params.pheno):
                    if fileX.endswith(".txt"):
                        phenoFileList.append(fileX)
                if len(phenoFileList) == 0:
                    print "No phenotype database was detected in: ", params.pheno
                    print "This wil not allow matching of detected variants to known phenotypes."
                    print "Refer to user manual on creating an appropriate phenotype database for your selected reference."
                    phenoAllowed = False 
    
                if len(phenoFileList) > 1:
                    print "Error, more than one .txt file found in:", params.pheno
                    print "Please ensure that only one phenotype databse file is present in this folder." 
                    phenoAllowed = False
                # Will now check the format of the phenotype DB
            if phenoAllowed:
                f = open(params.pheno+fileX)
                header = f.readline()
                f.close()
                temp = header.split("\t")
                assumeHeader = ["Chromosome","Chromosome coordinate","Locus","Locus Tag","Gene coordinate","Codon Number","BP_CHANGES","Codon_Change","Amino Acid Change","PhenoType/Resistance","Sources"]
                pos = -1
                for element in assumeHeader:
                    pos+=1
                    if element <> temp[pos].strip(): #replace("\n","").replace("\r",""):
                        print "Format error in phenotype database file,"
                        print "The col num:",pos,"does not match to ",element
                        print [temp[pos]],"vs",[element]
                        print "Please refer to the user manual for correctly formatting the phenotype database."
                        phenoAllowed = False
                        break
            if phenoAllowed:
                print "Matching detected variants to known phenotype"
                if "MycobacteriumTuberculosis_H37Rv" in params.reference: 
                    MTB = True
                    print "Matching to M. tuberculosis"
                resultsFolder = params.mapperOut #"C:/Ruben/VBOXSHARE/ANNO_TEST_NEW/" #"G:/[ALL_RESULTS]/[missing2]/"  #"G:/[ALL_RESULTS]/880/" 
                FILTERED_VARIANTS = params.mapperOut+"FILTERED_VARIANTS/"# resultsFolder+"FILTERED_VARIANTS/"  
                FILTERED_DELETIONS = params.mapperOut+"FILTERED_DELETIONS/" #resultsFolder+"FILTERED_DELETIONS/"
                outputFolder = params.mapperOut
    ##            variantListDir = params.binDir #"C:/Users/rvdm/Dropbox/Scripts/"
                variantListName = params.pheno+fileX #"PHENO_DB_MTB.txt"                
                MATCH_PHENO_TO_VARIANTS(variantListName,FILTERED_VARIANTS,FILTERED_DELETIONS,outputFolder,MTB,debugMode)
                print "Phenotype matching to detected variants complete."
                if debugMode:
                    print FILTERED_VARIANTS
                    print FILTERED_DELETIONS
                    print outputFolder
                    print MTB

        lineageAllowed = False        
        if controlPoint <= 10:
            print "Starting Lineage Detection"
            if debugMode:
                raw_input("press enter")
            #Determine sample lineage based on list of known snps
            try:
                os.chdir(params.lineage)
                lineageFileList = []
                lineageAllowed = True
                for fileX in os.listdir(params.lineage):
                    if fileX.endswith(".txt"):
                        lineageFileList.append(fileX)
                if len(lineageFileList) == 0:
                    print "No lineage was detected in: ", params.lineage
                    print "This wil not allow matching of detected variants to known lineages."
                    print "Refer to user manual on creating an appropriate lineage database for your selected reference."
                    lineageAllowed = False 
        
                if len(lineageFileList) > 1:
                    print "Error, more than one .txt file found in:", params.pheno
                    print "Please ensure that only one phenotype databse file is present in this folder." 
                    lineageAllowed = False
            except:
                print "No list of lineage markers found, skipping lineage matching"
                lineageAllowed =  False
            
            if lineageAllowed:
                f = open(params.lineage+fileX)
                header = f.readline()
                f.close()
                temp = header.split("\t")
                assumeHeader = ["lineage","coordinate","gene_coordinate","allele_change","codon_number","codon_change","amino_acid_change","locus_Id","gene_name","mutation_type","essential"]
                pos = -1
                for element in assumeHeader:
                    pos+=1
                    if element <> temp[pos].strip(): #replace("\n","").replace("\r",""):
                        print "Format error in lineage database file,"
                        print "The col num:",pos,"does not match to ",element
                        print [temp[pos]],"vs",[element]
                        print "Please refer to the user manual for correctly formatting the lineage database."
                        lineageAllowed = False
                        break
            if lineageAllowed:
                FILTERED_VARIANTS = params.mapperOut+"FILTERED_VARIANTS/"
                determine_lineages(params.mapperOut+"FILTERED_VARIANTS/",params.lineage+fileX)
                print "Lineage determination completed"
    
        #################################################################################################
        if controlPoint <= 11: #Create Per sample Summary File
            print "creating detailed report file." #- currintly missing spolpred data
            #Here the summary file needs to report all data available, does not matter if annotation was possible
            if debugMode:
                raw_input("press enter to create report file")
            orderedFiles = []
            fileData = {}
            for fileX in os.listdir(params.fastQ):
                if fileX.lower().endswith(".fastq.gz") or fileX.lower().endswith(".fastq"):
                    temp = fileX.split("_")[0]
                    if temp not in fileData:
                        fileData[temp] = {} #a dictionary of dictionaries
                        orderedFiles.append(temp)
            orderedFiles.sort()
            #print "Summarizing results for :",fileData
            #{File} : {coverage_BWA,coverage_NOVO,coverage_SMALT,DR,lineage,spolpred...etc}
    
            #loading genome coverage data
            #################################################
            mapperListFileNameToCovDict = []
            for mapper in mapperOrderList:
                tempDict = {}
                f = open(params.mapperOut+mapper+"/GenomeCoverage/Genome_Coverage_Results_Summary.txt",'r')
                tempData = []
                f.readline()
                for line in f:
                    temp = line.split("\t")
                    name = temp[0]
                    cov = temp[1].replace("\n","")
                    tempDict[name] = cov 
                f.close()
                mapperListFileNameToCovDict.append(tempDict) #a list of 3 dictionaries
            #now have [{bwa_file:cov},{novo_file:cov},{smalt_file:cov{]
            #STORE the data in the main summary dictionary
            pos = -1
            for mapper in mapperOrderList:
                pos += 1
                covData = mapperListFileNameToCovDict[pos]
                for fileName in orderedFiles:
                    if fileName in covData:
                        if "AVR_COVERAGE" in fileData[fileName]:
                            fileData[fileName]["AVR_COVERAGE"].append(covData[fileName])
                        else:
                            fileData[fileName]["AVR_COVERAGE"] = [covData[fileName]] #slightly complicated...

                            #this ends up as fileName : [10,10,4] for bwa cov novo cov and smalt cov
            ###########################################
            #Loading lineage data
            if debugMode:
                raw_input("Forcing lineage allowed here")
                lineageAllowed = True
            if lineageAllowed:
                try:
                    f = open(params.mapperOut+"full_lineageData.txt",'r')
                    f.readline()
                except:
                    raw_input("no lineage data detected")
                for line in f:
                    temp = line.split("\t")
                    name = temp[0].split("_")[0]
                    lineage = temp[1]
                    score = temp[2]
                    if name in fileData:
                        fileData[name]["LINEAGE"] = lineage+"\t"+score
                    else:
                        print "error #4454, file with no output results detected, possible error: Corrupt FASTQ. File affected is :",name
                        raw_input("Press enter to continue") 
            ############################################################    
            if MTB:
                lookupSpoligotypes(spolPredOut,debugMode)
                f = open(params.mapperOut+"SpolPred_Results_Summary.txt",'r')
                temp = f.readline()
                for line in f:
                    temp = line.split("\t")
                    name = temp[0]
                    code = temp[1]
                    commonName = temp[2].replace("\n","")
                    if name in fileData:
                        fileData[name]["SPOLPRED"] = code+"\t"+commonName
                    else:
                        print "error #4454, file with no output results detected, possible error: Corrupt FASTQ. File affected is :",name
                        raw_input("Press enter to continue") 
                f.close()

            #######################################
            #get % mapped reads
            mappedReadsList = []
            for folderName in [params.BWAAligned_aln,params.NOVOAligned_aln,params.SMALTAligned_aln]:
                os.chdir(folderName)
                mappedReadsDict = {}
                for fileX in os.listdir(folderName):
                    if "samtools_stats.txt" not in fileX or "samtools_stats.txt~" in fileX :
                        continue
                    tempFile = open(fileX,'r')
                    numReads = tempFile.readline()
                    if numReads == "":
                        print "no mapped reads data found for file", fileX
                        numReads = ""
                        percentageMapped = ""
                    else:
                        numReads=numReads.split()[0]
                        tempFile.readline()
                ##        percentageMapped = tempFile.readline().split("(")[1].split(":")[0]
                        percentageMapped = tempFile.readline()
                        if "duplicates" in percentageMapped:
                            percentageMapped = tempFile.readline()
                ##        print [percentageMapped]
                        percentageMapped = percentageMapped.split("(")[1]
                ##        print percentageMapped
                        percentageMapped = percentageMapped.split(":")[0]
                ##        print percentageMapped
                ##        raw_input()
                    if "_" in fileX:
                        mappedReadsDict[fileX.split("_")[0]] = [percentageMapped,numReads]
                    else:
                        mappedReadsDict[fileX] = [percentageMapped,numReads]
                    tempFile.close()
                mappedReadsList.append(mappedReadsDict)

            #now have [{bwa_file:[percentageMapped,numReads]},{novo_file:[percentageMapped,numReads]},{smalt_file:[percentageMapped,numReads]}]
            #STORE the data in the main summary dictionary
            pos = -1
            for mapper in mapperOrderList:
                pos += 1
                mappedReadsData = mappedReadsList[pos]
                for fileName in orderedFiles:
                    if fileName in mappedReadsData:
                        if "MAPPED_READS" in fileData[fileName]:
                            fileData[fileName]["MAPPED_READS"].append(mappedReadsData[fileName])
                        else:
                            fileData[fileName]["MAPPED_READS"] = [mappedReadsData[fileName]] #slightly complicated...
            #NOW have file : [% mappedreads, numreads] for bwa,  [% mappedreads, numreads] for novo,  [% mappedreads, numreads] for smalt

            ########################################
            #########LOAD DR DATA
            try:
                f = open(params.mapperOut+"PHENO_RESULTS.txt",'r')
                flag = True
            except:
                print "NO phenotype file found"
                flag = False
            if flag:    
                DR_header = f.readline()
                for line in f:
                    temp = line.split("\t")
                    name = temp[0]
                    nameLen = len(name)
                    if "_" in name:
                        name = name.split("_")[0]
                    DRdata = line[nameLen+1:] 
                    if name in fileData:
                        if "PHENO" in fileData[name]:
                            
                            print "have the same DR data element for more than one file:", name
                            raw_input("FATAL ERROR, fix file name error")
                        else:
                            fileData[name]["PHENO"] = DRdata
                    else:
                        raw_input("error have dr for a file not in main list! Press enter to continue")

            #################################################
            #write data to file
            if debugMode:
                raw_input("now writing to one big happy file")
            else:
                print "Writing summarized results to file"
            os.chdir(params.mapperOut)
            f = open("SUMMARY.txt",'w')
            if MTB:
                keys = ["AVR_COVERAGE","MAPPED_READS","LINEAGE","SPOLPRED","PHENO"]
            else:
                keys = ["AVR_COVERAGE","MAPPED_READS","LINEAGE","PHENO"]
            f.write("SAMPLE_NAME")
            for key in keys[:-1]:
                if MTB and key == "SPOLPRED":
                    f.write("\tSPOLPRED_OCTAL_CODE\tCOMMON_NAME")
                elif key == "LINEAGE":
                    f.write("\tLINEAGE\tNUM_LINEAGE_SNP_MATCHES")
                else:
                    f.write("\t"+key)
            flagX = True
            for x in DR_header.split("\t"):
                if flagX:
                    flagX = False # to skip sample_name
                    continue
                f.write("\t"+x)
            if not "\n" in x:
                f.write("\n")
            #header is done, no write the data for each sample
            for fileName in orderedFiles:
                f.write(fileName)
                for key in keys:
                    if key not in fileData[fileName]:
                        if MTB and key == "SPOLPRED":
                            f.write("\tN/A\t")
                        else:
                            f.write("\tN/A")
                        lastString = "A"
                    else:
                        f.write("\t"+str(fileData[fileName][key]))
                        lastString = str(fileData[fileName][key])[-1]
                if lastString <> "\n":
                    f.write("\n")
            f.close()

            print "USAP complete, proceed to additional tool menu for further built-in analysis tools"
    #######################################################################################################################################################  
    #######################################################################################################################################################
    #######################################################################################################################################################
    #######################################################################################################################################################          
    elif ans == "2":
        for x in range(5):
            print
        ans = "?"
        while ans not in ["1","2","3","4","Q","q"]:
            ###############################################################################################################################################
            print "USAP extra tools (suggested order):"
            print "1: Reference file mappability calculation"
            print "2: Custom variant filtering, allows removal of specific list of variant (such as to remove homoplasy)" 
            print "3: Generate whole genome SNP multi-Fasta for direct use in Phylogeny programs"
            print "4: Variant comparison between samples or groups of samples"
            print "5: Generate SNP distance matrix for several samples"
            print "Q: Quit"
            ###############################################################################################################################################
            ans = raw_input("Please select the tool you would like to run from the list above: ")
        if ans == "Q" or ans == "q":
            exit()
        if ans == 1:            
            print "Genome mappability calculation selected"
            print "WARNING: This step is very time consuming and should only by computed once for each reference."
            
            globalDir = os.getcwd()
            if "/BIN" in globalDir:
                globalDir = globalDir[:-4]
            refDir = globalDir+"/Reference"
            refOptions = [name for name in os.listdir(refDir) if os.path.isdir(os.path.join(refDir, name))]
            refOptions.sort()
            flag = False
            while not flag:
                counter = 0
                print "--------------------------------------------"
                print "Please select a reference from the selection below:"
                for ref in refOptions:
                    counter += 1
                    print str(counter)+":\t",ref
                print "Q: quit\n"
                
                selectedRef = raw_input("Enter the corresponding number for the reference (1,2,3 etc): ")
                if selectedRef =="q" or selectedRef =="Q":
                    quit()
                try:
                    selectedRef = int(selectedRef)
                except:
                    selectedRef = "N/A"
                if selectedRef not in range(1,len(refOptions)+1): 
                    continue
                else:
                    print "Reference selected:", refOptions[selectedRef-1]
                    userRef = refOptions[selectedRef-1]
                    break
            for fileX in os.listdir(refDir+"/"+userRef+"/FASTA"):
                if fileX.lower().endswith(".fasta"):
                    ref = fileX
                    break
            print "The detected fasta file for use in mappability determination is:",fileX
            ans = 'z'
            while ans.upper() not in ["Y","N"]:
                ans = raw_input("Press 'Y' to continue calculating mappability which generates a report file of poor mapping regions, pess 'N' to quit")
            if ans == "N":
                exit()
            raw_input("running mappability determination on file"+ fileX)

            
        if ans == 2:
            
            #################################################################
            annotatedDir = params.mapperOut+"ANNOTATED_VARIANTS/"
            outputDir =  params.mapperOut+"FILTERED_VARIANTS/"    
            filterSpecificPositions = False 
            DRMarkersToRemove = ""          
            ################################################################# 
             
            print "Previous filtering settings used were:"
            print "Variants containing these keywords will be filtered:", junkTerms
            print "mapper count: ", mapperCount
            print "Quality cutoff: ", qualityCutOFF
            print "readFreqCutoff cutoff: ", readFreqCutoff
            print "minCoverage cutoff: ", minCoverage
            temp_ans = False
            while temp_ans.lower() not in ["y","n"]:
                temp_ans = raw_input("Are you sure you want to enter new filter settings and re-filter detected variants? <Y/N>: ")
            if temp_ans == "y":
                junkTerms,mapperCount,qualityCutOFF,minCoverage,readFreqCutoff = getFilterSettings(MTB,autoMode,params)   
                temp_ans2 = False
                while temp_ans2.lower() not in ["y","n"]:
                    temp_ans2 = raw_input("Do you want to remove variants occuring at specific positions? (Note to use this feature you need to create a folder under your selected reference containing a text file with the list of posistions to remove) <Y/N>: ")
                if temp_ans2 == "y":
                    filterSpecificPositions = True
                    tempFlag = True
                    tempFlag2 = False
                    try:
                        os.chdir(params.exclusionList)
                    except:
                        print "Error, could not access folder: ", params.exclusionList
                        print "This will not allow removal of specific variant positions"
                        raw_input("press enter to continue")
                        tempFlag = False
                    if tempFlag:
                        for fileX in os.listdir(params.exclusionList):
                            if fileX.endswith(".txt"):
                                tempFlag2 = True
                                DRMarkersToRemove = params.exclusionList+fileX
                                break                        
                RemoveJunkFromVCF5(annotatedDir,outputDir,junkTerms,mapperCount,qualityCutOFF,minCoverage,readFreqCutoff,DRMarkersToRemove,filterSpecificPositions)
            
            if ans == 2:
                #############################################################################################################  
                print "Running Whole Genome SNP based phylogeny tool"
                #ans = "?"
                #while ans.lower() not in "y/n":
                #    ans = raw_input("Would you like to make use of genome coverage data to take deletions into account? (Y/N):")
                #useCovData = False
                genCovDir = ""
                #if ans.lower() == "y":
                    #useCovData = True
                print "to use this tool, you need the filtered VCF files as well as the USAP formatted deletion region files"
                print "Step 1 create a new folder to contain your files"
                print "Step 2 copy the data genome"
                BWAGenCov = params.BWAAligned+"GenomeCoverage/" #NOVO Results dir
                NOVOGenCov = params.NOVOAligned+"GenomeCoverage/" #NOVO Results dir
                SMALTGenCov = params.SMALTAligned+"GenomeCoverage/" #NOVO Results dir
                for genCovDir in [NOVOGenCov,BWAGenCov,SMALTGenCov]: 
                    try:
                        os.chdir(genCovDir)
                        break
                    except:
                        continue    
                filesForPhylo = params.mapperOut+"FILTERED_VARIANTS/" 
                outputDir = params.mapperOut
                phyloAll(filesForPhylo, genCovDir, outputDir,useCovData) 
                #############################################################################################################
    
            if ans == 3:
                print "Pairwise / Group variant comparison tool"
                '''
                1 Ask user to create two folders
                must copy the annotated or filtered vcfs into folders
                then enter path for each
                then run tool
                "SNP comparisons between groups " 
                print instructions
                2 variant comparison tool
                "BIN/Global/compareDirAtoDirB.py"
                '''
            if ans == 4:
                print "SNP distance matrix / individual file comparison tool"
                '''
                1 Ask user to create one folders containing all VCFs to be included
                must copy the annotated or filtered vcfs into one folder
                then enter path 
                then run tool
                '''
                ############################################################################################################################################
                '''
                    F1 F2 F3
                F1
                F2
                F3
                '''
                '''
                #Change to suit your needs
                dir1 = "E:/Laura_VCF/raw_vcf_anno/new_filtered_bad_mapping_removed/pks1" 
                outputDir = dir1
                output_File_Name = "myTest123.txt"
                myOutputDir = "myDir"
                try:
                    os.mkdir(myOutputDir)
                except:
                    print myOutputDir,"already exists"
                outputDir_for_indiv_files = outputDir+"/"+myOutputDir
                snpDistanceMatrix(dir1,outputDir,outputDir_for_indiv_files,True,output_File_Name)
                '''
                ############################################################################################################################################

        #########################################################################################################################################################

print "add samtools / SNVER / GNUMAP variant callers"
print "next: GUI version"
